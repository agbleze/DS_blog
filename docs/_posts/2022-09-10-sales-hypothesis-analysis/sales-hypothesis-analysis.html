<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">

<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1"/>
  <meta name="generator" content="distill" />

  <style type="text/css">
  /* Hide doc at startup (prevent jankiness while JS renders/transforms) */
  body {
    visibility: hidden;
  }
  </style>

 <!--radix_placeholder_import_source-->
 <!--/radix_placeholder_import_source-->



  <!--radix_placeholder_meta_tags-->
  <title>Online Marketing Ads click prediction: End-to-End workflow for machine learning solution</title>

  <meta property="description" itemprop="description" content="This post demonstrates the process for predicting number of clicks with focus on procedures and logic behind the modelling decision making process. While the ideal aim is to produce a model that accurately predicts number of clicks, a great model with the least error is compromise between cost of computational power and time on one hand, and the expected marginal benefit to the business. Thus, the approach is to develop an accurate model that is computationally least expensive. By this, a benchmark model was defined as a minimal standard against which the model developed is deem acceptable or not. An exploratory analysis was undertaken to decide which algorithmn to employ. The findings of the exploratory analysis which included predictor variables not having a linear relationship with the target variable, potential loss of about 28% of data attributable to missing data, absence of multicollinearity and presence of outliers among others, informed the decision to use a non-parametric model hence decision tree based model. In other to choose a model that handles missing data relatively well, HistGradientBoostingRegressor was used. Various approaches to improving model performance were also demonstrated within the limited time and computational power available. Hyperparameter tuning was undertaken with grid search cross validation using few combinations of parameters. With the improved model from hyperparameter tuning, bagging was employed to further reduce error and overfitting. An alternative ensembel model of decision tree was also implemented with Extreme Gradient boosting Random Forest (XGBRFRegressor). It is duly recognized that not all techniques, some of which are capable of further reducing error, where employed due to the constrain of computation time. Nonetheless, an unexhaustive highlight of how to improve the model has been provided."/>


  <!--  https://schema.org/Article -->
  <meta property="article:published" itemprop="datePublished" content="2022-09-10"/>
  <meta property="article:created" itemprop="dateCreated" content="2022-09-10"/>
  <meta name="article:author" content="Linus Agbleze"/>

  <!--  https://developers.facebook.com/docs/sharing/webmasters#markup -->
  <meta property="og:title" content="Online Marketing Ads click prediction: End-to-End workflow for machine learning solution"/>
  <meta property="og:type" content="article"/>
  <meta property="og:description" content="This post demonstrates the process for predicting number of clicks with focus on procedures and logic behind the modelling decision making process. While the ideal aim is to produce a model that accurately predicts number of clicks, a great model with the least error is compromise between cost of computational power and time on one hand, and the expected marginal benefit to the business. Thus, the approach is to develop an accurate model that is computationally least expensive. By this, a benchmark model was defined as a minimal standard against which the model developed is deem acceptable or not. An exploratory analysis was undertaken to decide which algorithmn to employ. The findings of the exploratory analysis which included predictor variables not having a linear relationship with the target variable, potential loss of about 28% of data attributable to missing data, absence of multicollinearity and presence of outliers among others, informed the decision to use a non-parametric model hence decision tree based model. In other to choose a model that handles missing data relatively well, HistGradientBoostingRegressor was used. Various approaches to improving model performance were also demonstrated within the limited time and computational power available. Hyperparameter tuning was undertaken with grid search cross validation using few combinations of parameters. With the improved model from hyperparameter tuning, bagging was employed to further reduce error and overfitting. An alternative ensembel model of decision tree was also implemented with Extreme Gradient boosting Random Forest (XGBRFRegressor). It is duly recognized that not all techniques, some of which are capable of further reducing error, where employed due to the constrain of computation time. Nonetheless, an unexhaustive highlight of how to improve the model has been provided."/>
  <meta property="og:locale" content="en_US"/>

  <!--  https://dev.twitter.com/cards/types/summary -->
  <meta property="twitter:card" content="summary"/>
  <meta property="twitter:title" content="Online Marketing Ads click prediction: End-to-End workflow for machine learning solution"/>
  <meta property="twitter:description" content="This post demonstrates the process for predicting number of clicks with focus on procedures and logic behind the modelling decision making process. While the ideal aim is to produce a model that accurately predicts number of clicks, a great model with the least error is compromise between cost of computational power and time on one hand, and the expected marginal benefit to the business. Thus, the approach is to develop an accurate model that is computationally least expensive. By this, a benchmark model was defined as a minimal standard against which the model developed is deem acceptable or not. An exploratory analysis was undertaken to decide which algorithmn to employ. The findings of the exploratory analysis which included predictor variables not having a linear relationship with the target variable, potential loss of about 28% of data attributable to missing data, absence of multicollinearity and presence of outliers among others, informed the decision to use a non-parametric model hence decision tree based model. In other to choose a model that handles missing data relatively well, HistGradientBoostingRegressor was used. Various approaches to improving model performance were also demonstrated within the limited time and computational power available. Hyperparameter tuning was undertaken with grid search cross validation using few combinations of parameters. With the improved model from hyperparameter tuning, bagging was employed to further reduce error and overfitting. An alternative ensembel model of decision tree was also implemented with Extreme Gradient boosting Random Forest (XGBRFRegressor). It is duly recognized that not all techniques, some of which are capable of further reducing error, where employed due to the constrain of computation time. Nonetheless, an unexhaustive highlight of how to improve the model has been provided."/>

  <!--/radix_placeholder_meta_tags-->
  <!--radix_placeholder_rmarkdown_metadata-->

  <script type="text/json" id="radix-rmarkdown-metadata">
  {"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["title","description","author","date","output","draft","categories"]}},"value":[{"type":"character","attributes":{},"value":["Online Marketing Ads click prediction: End-to-End workflow for machine learning solution"]},{"type":"character","attributes":{},"value":["This post demonstrates the process for predicting number of clicks with focus on procedures and logic behind the modelling decision making process. While the ideal aim is to produce a model that accurately predicts number of clicks, a great model with the least error is compromise between cost of computational power and time on one hand, and the expected marginal benefit to the business. Thus, the approach is to develop an accurate model that is computationally least expensive. By this, a benchmark model was defined as a minimal standard against which the model developed is deem acceptable or not. An exploratory analysis was undertaken to decide which algorithmn to employ. The findings of the exploratory analysis which included predictor variables not having a linear relationship with the target variable, potential loss of about 28% of data attributable to missing data, absence of multicollinearity and presence of outliers among others, informed the decision to use a non-parametric model hence decision tree based model. In other to choose a model that handles missing data relatively well, HistGradientBoostingRegressor was used. Various approaches to improving model performance were also demonstrated within the limited time and computational power available. Hyperparameter tuning was undertaken with grid search cross validation using few combinations of parameters. With the improved model from hyperparameter tuning, bagging was employed to further reduce error and overfitting. An alternative ensembel model of decision tree was also implemented with Extreme Gradient boosting Random Forest (XGBRFRegressor). It is duly recognized that not all techniques, some of which are capable of further reducing error, where employed due to the constrain of computation time. Nonetheless, an unexhaustive highlight of how to improve the model has been provided.\n"]},{"type":"list","attributes":{},"value":[{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","url"]}},"value":[{"type":"character","attributes":{},"value":["Linus Agbleze"]},{"type":"character","attributes":{},"value":["https://agbleze.github.io/Portfolio/"]}]}]},{"type":"character","attributes":{},"value":["2022-09-10"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["distill::distill_article"]}},"value":[{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["self_contained"]}},"value":[{"type":"logical","attributes":{},"value":[false]}]}]},{"type":"logical","attributes":{},"value":[false]},{"type":"character","attributes":{},"value":["Machine learning","Python"]}]}
  </script>
  <!--/radix_placeholder_rmarkdown_metadata-->
  
  <script type="text/json" id="radix-resource-manifest">
  {"type":"character","attributes":{},"value":["Data/CaseStudy_DS_2022.pdf","Data/sample_submission.csv","Data/test_set.csv","Data/train_set.csv","sales-hypothesis-analysis_files/anchor-4.2.2/anchor.min.js","sales-hypothesis-analysis_files/bowser-1.9.3/bowser.min.js","sales-hypothesis-analysis_files/distill-2.2.21/template.v2.js","sales-hypothesis-analysis_files/figure-html5/unnamed-chunk-2-1.png","sales-hypothesis-analysis_files/figure-html5/unnamed-chunk-2-10.png","sales-hypothesis-analysis_files/figure-html5/unnamed-chunk-2-2.png","sales-hypothesis-analysis_files/figure-html5/unnamed-chunk-2-21.png","sales-hypothesis-analysis_files/figure-html5/unnamed-chunk-2-22.png","sales-hypothesis-analysis_files/figure-html5/unnamed-chunk-2-23.png","sales-hypothesis-analysis_files/figure-html5/unnamed-chunk-2-24.png","sales-hypothesis-analysis_files/figure-html5/unnamed-chunk-2-25.png","sales-hypothesis-analysis_files/figure-html5/unnamed-chunk-2-26.png","sales-hypothesis-analysis_files/figure-html5/unnamed-chunk-2-27.png","sales-hypothesis-analysis_files/figure-html5/unnamed-chunk-2-28.png","sales-hypothesis-analysis_files/figure-html5/unnamed-chunk-2-29.png","sales-hypothesis-analysis_files/figure-html5/unnamed-chunk-2-3.png","sales-hypothesis-analysis_files/figure-html5/unnamed-chunk-2-30.png","sales-hypothesis-analysis_files/figure-html5/unnamed-chunk-2-4.png","sales-hypothesis-analysis_files/figure-html5/unnamed-chunk-2-41.png","sales-hypothesis-analysis_files/figure-html5/unnamed-chunk-2-5.png","sales-hypothesis-analysis_files/figure-html5/unnamed-chunk-2-6.png","sales-hypothesis-analysis_files/figure-html5/unnamed-chunk-2-7.png","sales-hypothesis-analysis_files/figure-html5/unnamed-chunk-2-8.png","sales-hypothesis-analysis_files/figure-html5/unnamed-chunk-2-9.png","sales-hypothesis-analysis_files/figure-html5/unnamed-chunk-3-1.png","sales-hypothesis-analysis_files/header-attrs-2.14/header-attrs.js","sales-hypothesis-analysis_files/jquery-3.6.0/jquery-3.6.0.js","sales-hypothesis-analysis_files/jquery-3.6.0/jquery-3.6.0.min.js","sales-hypothesis-analysis_files/jquery-3.6.0/jquery-3.6.0.min.map","sales-hypothesis-analysis_files/popper-2.6.0/popper.min.js","sales-hypothesis-analysis_files/tippy-6.2.7/tippy-bundle.umd.min.js","sales-hypothesis-analysis_files/tippy-6.2.7/tippy-light-border.css","sales-hypothesis-analysis_files/tippy-6.2.7/tippy.css","sales-hypothesis-analysis_files/tippy-6.2.7/tippy.umd.min.js","sales-hypothesis-analysis_files/webcomponents-2.0.0/webcomponents.js","SCQ.png"]}
  </script>
  <!--radix_placeholder_navigation_in_header-->
  <!--/radix_placeholder_navigation_in_header-->
  <!--radix_placeholder_distill-->

  <style type="text/css">

  body {
    background-color: white;
  }

  .pandoc-table {
    width: 100%;
  }

  .pandoc-table>caption {
    margin-bottom: 10px;
  }

  .pandoc-table th:not([align]) {
    text-align: left;
  }

  .pagedtable-footer {
    font-size: 15px;
  }

  d-byline .byline {
    grid-template-columns: 2fr 2fr;
  }

  d-byline .byline h3 {
    margin-block-start: 1.5em;
  }

  d-byline .byline .authors-affiliations h3 {
    margin-block-start: 0.5em;
  }

  .authors-affiliations .orcid-id {
    width: 16px;
    height:16px;
    margin-left: 4px;
    margin-right: 4px;
    vertical-align: middle;
    padding-bottom: 2px;
  }

  d-title .dt-tags {
    margin-top: 1em;
    grid-column: text;
  }

  .dt-tags .dt-tag {
    text-decoration: none;
    display: inline-block;
    color: rgba(0,0,0,0.6);
    padding: 0em 0.4em;
    margin-right: 0.5em;
    margin-bottom: 0.4em;
    font-size: 70%;
    border: 1px solid rgba(0,0,0,0.2);
    border-radius: 3px;
    text-transform: uppercase;
    font-weight: 500;
  }

  d-article table.gt_table td,
  d-article table.gt_table th {
    border-bottom: none;
  }

  .html-widget {
    margin-bottom: 2.0em;
  }

  .l-screen-inset {
    padding-right: 16px;
  }

  .l-screen .caption {
    margin-left: 10px;
  }

  .shaded {
    background: rgb(247, 247, 247);
    padding-top: 20px;
    padding-bottom: 20px;
    border-top: 1px solid rgba(0, 0, 0, 0.1);
    border-bottom: 1px solid rgba(0, 0, 0, 0.1);
  }

  .shaded .html-widget {
    margin-bottom: 0;
    border: 1px solid rgba(0, 0, 0, 0.1);
  }

  .shaded .shaded-content {
    background: white;
  }

  .text-output {
    margin-top: 0;
    line-height: 1.5em;
  }

  .hidden {
    display: none !important;
  }

  d-article {
    padding-top: 2.5rem;
    padding-bottom: 30px;
  }

  d-appendix {
    padding-top: 30px;
  }

  d-article>p>img {
    width: 100%;
  }

  d-article h2 {
    margin: 1rem 0 1.5rem 0;
  }

  d-article h3 {
    margin-top: 1.5rem;
  }

  d-article iframe {
    border: 1px solid rgba(0, 0, 0, 0.1);
    margin-bottom: 2.0em;
    width: 100%;
  }

  /* Tweak code blocks */

  d-article div.sourceCode code,
  d-article pre code {
    font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
  }

  d-article pre,
  d-article div.sourceCode,
  d-article div.sourceCode pre {
    overflow: auto;
  }

  d-article div.sourceCode {
    background-color: white;
  }

  d-article div.sourceCode pre {
    padding-left: 10px;
    font-size: 12px;
    border-left: 2px solid rgba(0,0,0,0.1);
  }

  d-article pre {
    font-size: 12px;
    color: black;
    background: none;
    margin-top: 0;
    text-align: left;
    white-space: pre;
    word-spacing: normal;
    word-break: normal;
    word-wrap: normal;
    line-height: 1.5;

    -moz-tab-size: 4;
    -o-tab-size: 4;
    tab-size: 4;

    -webkit-hyphens: none;
    -moz-hyphens: none;
    -ms-hyphens: none;
    hyphens: none;
  }

  d-article pre a {
    border-bottom: none;
  }

  d-article pre a:hover {
    border-bottom: none;
    text-decoration: underline;
  }

  d-article details {
    grid-column: text;
    margin-bottom: 0.8em;
  }

  @media(min-width: 768px) {

  d-article pre,
  d-article div.sourceCode,
  d-article div.sourceCode pre {
    overflow: visible !important;
  }

  d-article div.sourceCode pre {
    padding-left: 18px;
    font-size: 14px;
  }

  d-article pre {
    font-size: 14px;
  }

  }

  figure img.external {
    background: white;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 1px 8px rgba(0, 0, 0, 0.1);
    padding: 18px;
    box-sizing: border-box;
  }

  /* CSS for d-contents */

  .d-contents {
    grid-column: text;
    color: rgba(0,0,0,0.8);
    font-size: 0.9em;
    padding-bottom: 1em;
    margin-bottom: 1em;
    padding-bottom: 0.5em;
    margin-bottom: 1em;
    padding-left: 0.25em;
    justify-self: start;
  }

  @media(min-width: 1000px) {
    .d-contents.d-contents-float {
      height: 0;
      grid-column-start: 1;
      grid-column-end: 4;
      justify-self: center;
      padding-right: 3em;
      padding-left: 2em;
    }
  }

  .d-contents nav h3 {
    font-size: 18px;
    margin-top: 0;
    margin-bottom: 1em;
  }

  .d-contents li {
    list-style-type: none
  }

  .d-contents nav > ul {
    padding-left: 0;
  }

  .d-contents ul {
    padding-left: 1em
  }

  .d-contents nav ul li {
    margin-top: 0.6em;
    margin-bottom: 0.2em;
  }

  .d-contents nav a {
    font-size: 13px;
    border-bottom: none;
    text-decoration: none
    color: rgba(0, 0, 0, 0.8);
  }

  .d-contents nav a:hover {
    text-decoration: underline solid rgba(0, 0, 0, 0.6)
  }

  .d-contents nav > ul > li > a {
    font-weight: 600;
  }

  .d-contents nav > ul > li > ul {
    font-weight: inherit;
  }

  .d-contents nav > ul > li > ul > li {
    margin-top: 0.2em;
  }


  .d-contents nav ul {
    margin-top: 0;
    margin-bottom: 0.25em;
  }

  .d-article-with-toc h2:nth-child(2) {
    margin-top: 0;
  }


  /* Figure */

  .figure {
    position: relative;
    margin-bottom: 2.5em;
    margin-top: 1.5em;
  }

  .figure .caption {
    color: rgba(0, 0, 0, 0.6);
    font-size: 12px;
    line-height: 1.5em;
  }

  .figure img.external {
    background: white;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 1px 8px rgba(0, 0, 0, 0.1);
    padding: 18px;
    box-sizing: border-box;
  }

  .figure .caption a {
    color: rgba(0, 0, 0, 0.6);
  }

  .figure .caption b,
  .figure .caption strong, {
    font-weight: 600;
    color: rgba(0, 0, 0, 1.0);
  }

  /* Citations */

  d-article .citation {
    color: inherit;
    cursor: inherit;
  }

  div.hanging-indent{
    margin-left: 1em; text-indent: -1em;
  }

  /* Citation hover box */

  .tippy-box[data-theme~=light-border] {
    background-color: rgba(250, 250, 250, 0.95);
  }

  .tippy-content > p {
    margin-bottom: 0;
    padding: 2px;
  }


  /* Tweak 1000px media break to show more text */

  @media(min-width: 1000px) {
    .base-grid,
    distill-header,
    d-title,
    d-abstract,
    d-article,
    d-appendix,
    distill-appendix,
    d-byline,
    d-footnote-list,
    d-citation-list,
    distill-footer {
      grid-template-columns: [screen-start] 1fr [page-start kicker-start] 80px [middle-start] 50px [text-start kicker-end] 65px 65px 65px 65px 65px 65px 65px 65px [text-end gutter-start] 65px [middle-end] 65px [page-end gutter-end] 1fr [screen-end];
      grid-column-gap: 16px;
    }

    .grid {
      grid-column-gap: 16px;
    }

    d-article {
      font-size: 1.06rem;
      line-height: 1.7em;
    }
    figure .caption, .figure .caption, figure figcaption {
      font-size: 13px;
    }
  }

  @media(min-width: 1180px) {
    .base-grid,
    distill-header,
    d-title,
    d-abstract,
    d-article,
    d-appendix,
    distill-appendix,
    d-byline,
    d-footnote-list,
    d-citation-list,
    distill-footer {
      grid-template-columns: [screen-start] 1fr [page-start kicker-start] 60px [middle-start] 60px [text-start kicker-end] 60px 60px 60px 60px 60px 60px 60px 60px [text-end gutter-start] 60px [middle-end] 60px [page-end gutter-end] 1fr [screen-end];
      grid-column-gap: 32px;
    }

    .grid {
      grid-column-gap: 32px;
    }
  }


  /* Get the citation styles for the appendix (not auto-injected on render since
     we do our own rendering of the citation appendix) */

  d-appendix .citation-appendix,
  .d-appendix .citation-appendix {
    font-size: 11px;
    line-height: 15px;
    border-left: 1px solid rgba(0, 0, 0, 0.1);
    padding-left: 18px;
    border: 1px solid rgba(0,0,0,0.1);
    background: rgba(0, 0, 0, 0.02);
    padding: 10px 18px;
    border-radius: 3px;
    color: rgba(150, 150, 150, 1);
    overflow: hidden;
    margin-top: -12px;
    white-space: pre-wrap;
    word-wrap: break-word;
  }

  /* Include appendix styles here so they can be overridden */

  d-appendix {
    contain: layout style;
    font-size: 0.8em;
    line-height: 1.7em;
    margin-top: 60px;
    margin-bottom: 0;
    border-top: 1px solid rgba(0, 0, 0, 0.1);
    color: rgba(0,0,0,0.5);
    padding-top: 60px;
    padding-bottom: 48px;
  }

  d-appendix h3 {
    grid-column: page-start / text-start;
    font-size: 15px;
    font-weight: 500;
    margin-top: 1em;
    margin-bottom: 0;
    color: rgba(0,0,0,0.65);
  }

  d-appendix h3 + * {
    margin-top: 1em;
  }

  d-appendix ol {
    padding: 0 0 0 15px;
  }

  @media (min-width: 768px) {
    d-appendix ol {
      padding: 0 0 0 30px;
      margin-left: -30px;
    }
  }

  d-appendix li {
    margin-bottom: 1em;
  }

  d-appendix a {
    color: rgba(0, 0, 0, 0.6);
  }

  d-appendix > * {
    grid-column: text;
  }

  d-appendix > d-footnote-list,
  d-appendix > d-citation-list,
  d-appendix > distill-appendix {
    grid-column: screen;
  }

  /* Include footnote styles here so they can be overridden */

  d-footnote-list {
    contain: layout style;
  }

  d-footnote-list > * {
    grid-column: text;
  }

  d-footnote-list a.footnote-backlink {
    color: rgba(0,0,0,0.3);
    padding-left: 0.5em;
  }



  /* Anchor.js */

  .anchorjs-link {
    /*transition: all .25s linear; */
    text-decoration: none;
    border-bottom: none;
  }
  *:hover > .anchorjs-link {
    margin-left: -1.125em !important;
    text-decoration: none;
    border-bottom: none;
  }

  /* Social footer */

  .social_footer {
    margin-top: 30px;
    margin-bottom: 0;
    color: rgba(0,0,0,0.67);
  }

  .disqus-comments {
    margin-right: 30px;
  }

  .disqus-comment-count {
    border-bottom: 1px solid rgba(0, 0, 0, 0.4);
    cursor: pointer;
  }

  #disqus_thread {
    margin-top: 30px;
  }

  .article-sharing a {
    border-bottom: none;
    margin-right: 8px;
  }

  .article-sharing a:hover {
    border-bottom: none;
  }

  .sidebar-section.subscribe {
    font-size: 12px;
    line-height: 1.6em;
  }

  .subscribe p {
    margin-bottom: 0.5em;
  }


  .article-footer .subscribe {
    font-size: 15px;
    margin-top: 45px;
  }


  .sidebar-section.custom {
    font-size: 12px;
    line-height: 1.6em;
  }

  .custom p {
    margin-bottom: 0.5em;
  }

  /* Styles for listing layout (hide title) */
  .layout-listing d-title, .layout-listing .d-title {
    display: none;
  }

  /* Styles for posts lists (not auto-injected) */


  .posts-with-sidebar {
    padding-left: 45px;
    padding-right: 45px;
  }

  .posts-list .description h2,
  .posts-list .description p {
    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Fira Sans", "Droid Sans", "Helvetica Neue", Arial, sans-serif;
  }

  .posts-list .description h2 {
    font-weight: 700;
    border-bottom: none;
    padding-bottom: 0;
  }

  .posts-list h2.post-tag {
    border-bottom: 1px solid rgba(0, 0, 0, 0.2);
    padding-bottom: 12px;
  }
  .posts-list {
    margin-top: 60px;
    margin-bottom: 24px;
  }

  .posts-list .post-preview {
    text-decoration: none;
    overflow: hidden;
    display: block;
    border-bottom: 1px solid rgba(0, 0, 0, 0.1);
    padding: 24px 0;
  }

  .post-preview-last {
    border-bottom: none !important;
  }

  .posts-list .posts-list-caption {
    grid-column: screen;
    font-weight: 400;
  }

  .posts-list .post-preview h2 {
    margin: 0 0 6px 0;
    line-height: 1.2em;
    font-style: normal;
    font-size: 24px;
  }

  .posts-list .post-preview p {
    margin: 0 0 12px 0;
    line-height: 1.4em;
    font-size: 16px;
  }

  .posts-list .post-preview .thumbnail {
    box-sizing: border-box;
    margin-bottom: 24px;
    position: relative;
    max-width: 500px;
  }
  .posts-list .post-preview img {
    width: 100%;
    display: block;
  }

  .posts-list .metadata {
    font-size: 12px;
    line-height: 1.4em;
    margin-bottom: 18px;
  }

  .posts-list .metadata > * {
    display: inline-block;
  }

  .posts-list .metadata .publishedDate {
    margin-right: 2em;
  }

  .posts-list .metadata .dt-authors {
    display: block;
    margin-top: 0.3em;
    margin-right: 2em;
  }

  .posts-list .dt-tags {
    display: block;
    line-height: 1em;
  }

  .posts-list .dt-tags .dt-tag {
    display: inline-block;
    color: rgba(0,0,0,0.6);
    padding: 0.3em 0.4em;
    margin-right: 0.2em;
    margin-bottom: 0.4em;
    font-size: 60%;
    border: 1px solid rgba(0,0,0,0.2);
    border-radius: 3px;
    text-transform: uppercase;
    font-weight: 500;
  }

  .posts-list img {
    opacity: 1;
  }

  .posts-list img[data-src] {
    opacity: 0;
  }

  .posts-more {
    clear: both;
  }


  .posts-sidebar {
    font-size: 16px;
  }

  .posts-sidebar h3 {
    font-size: 16px;
    margin-top: 0;
    margin-bottom: 0.5em;
    font-weight: 400;
    text-transform: uppercase;
  }

  .sidebar-section {
    margin-bottom: 30px;
  }

  .categories ul {
    list-style-type: none;
    margin: 0;
    padding: 0;
  }

  .categories li {
    color: rgba(0, 0, 0, 0.8);
    margin-bottom: 0;
  }

  .categories li>a {
    border-bottom: none;
  }

  .categories li>a:hover {
    border-bottom: 1px solid rgba(0, 0, 0, 0.4);
  }

  .categories .active {
    font-weight: 600;
  }

  .categories .category-count {
    color: rgba(0, 0, 0, 0.4);
  }


  @media(min-width: 768px) {
    .posts-list .post-preview h2 {
      font-size: 26px;
    }
    .posts-list .post-preview .thumbnail {
      float: right;
      width: 30%;
      margin-bottom: 0;
    }
    .posts-list .post-preview .description {
      float: left;
      width: 45%;
    }
    .posts-list .post-preview .metadata {
      float: left;
      width: 20%;
      margin-top: 8px;
    }
    .posts-list .post-preview p {
      margin: 0 0 12px 0;
      line-height: 1.5em;
      font-size: 16px;
    }
    .posts-with-sidebar .posts-list {
      float: left;
      width: 75%;
    }
    .posts-with-sidebar .posts-sidebar {
      float: right;
      width: 20%;
      margin-top: 60px;
      padding-top: 24px;
      padding-bottom: 24px;
    }
  }


  /* Improve display for browsers without grid (IE/Edge <= 15) */

  .downlevel {
    line-height: 1.6em;
    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Fira Sans", "Droid Sans", "Helvetica Neue", Arial, sans-serif;
    margin: 0;
  }

  .downlevel .d-title {
    padding-top: 6rem;
    padding-bottom: 1.5rem;
  }

  .downlevel .d-title h1 {
    font-size: 50px;
    font-weight: 700;
    line-height: 1.1em;
    margin: 0 0 0.5rem;
  }

  .downlevel .d-title p {
    font-weight: 300;
    font-size: 1.2rem;
    line-height: 1.55em;
    margin-top: 0;
  }

  .downlevel .d-byline {
    padding-top: 0.8em;
    padding-bottom: 0.8em;
    font-size: 0.8rem;
    line-height: 1.8em;
  }

  .downlevel .section-separator {
    border: none;
    border-top: 1px solid rgba(0, 0, 0, 0.1);
  }

  .downlevel .d-article {
    font-size: 1.06rem;
    line-height: 1.7em;
    padding-top: 1rem;
    padding-bottom: 2rem;
  }


  .downlevel .d-appendix {
    padding-left: 0;
    padding-right: 0;
    max-width: none;
    font-size: 0.8em;
    line-height: 1.7em;
    margin-bottom: 0;
    color: rgba(0,0,0,0.5);
    padding-top: 40px;
    padding-bottom: 48px;
  }

  .downlevel .footnotes ol {
    padding-left: 13px;
  }

  .downlevel .base-grid,
  .downlevel .distill-header,
  .downlevel .d-title,
  .downlevel .d-abstract,
  .downlevel .d-article,
  .downlevel .d-appendix,
  .downlevel .distill-appendix,
  .downlevel .d-byline,
  .downlevel .d-footnote-list,
  .downlevel .d-citation-list,
  .downlevel .distill-footer,
  .downlevel .appendix-bottom,
  .downlevel .posts-container {
    padding-left: 40px;
    padding-right: 40px;
  }

  @media(min-width: 768px) {
    .downlevel .base-grid,
    .downlevel .distill-header,
    .downlevel .d-title,
    .downlevel .d-abstract,
    .downlevel .d-article,
    .downlevel .d-appendix,
    .downlevel .distill-appendix,
    .downlevel .d-byline,
    .downlevel .d-footnote-list,
    .downlevel .d-citation-list,
    .downlevel .distill-footer,
    .downlevel .appendix-bottom,
    .downlevel .posts-container {
    padding-left: 150px;
    padding-right: 150px;
    max-width: 900px;
  }
  }

  .downlevel pre code {
    display: block;
    border-left: 2px solid rgba(0, 0, 0, .1);
    padding: 0 0 0 20px;
    font-size: 14px;
  }

  .downlevel code, .downlevel pre {
    color: black;
    background: none;
    font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
    text-align: left;
    white-space: pre;
    word-spacing: normal;
    word-break: normal;
    word-wrap: normal;
    line-height: 1.5;

    -moz-tab-size: 4;
    -o-tab-size: 4;
    tab-size: 4;

    -webkit-hyphens: none;
    -moz-hyphens: none;
    -ms-hyphens: none;
    hyphens: none;
  }

  .downlevel .posts-list .post-preview {
    color: inherit;
  }



  </style>

  <script type="application/javascript">

  function is_downlevel_browser() {
    if (bowser.isUnsupportedBrowser({ msie: "12", msedge: "16"},
                                   window.navigator.userAgent)) {
      return true;
    } else {
      return window.load_distill_framework === undefined;
    }
  }

  // show body when load is complete
  function on_load_complete() {

    // add anchors
    if (window.anchors) {
      window.anchors.options.placement = 'left';
      window.anchors.add('d-article > h2, d-article > h3, d-article > h4, d-article > h5');
    }


    // set body to visible
    document.body.style.visibility = 'visible';

    // force redraw for leaflet widgets
    if (window.HTMLWidgets) {
      var maps = window.HTMLWidgets.findAll(".leaflet");
      $.each(maps, function(i, el) {
        var map = this.getMap();
        map.invalidateSize();
        map.eachLayer(function(layer) {
          if (layer instanceof L.TileLayer)
            layer.redraw();
        });
      });
    }

    // trigger 'shown' so htmlwidgets resize
    $('d-article').trigger('shown');
  }

  function init_distill() {

    init_common();

    // create front matter
    var front_matter = $('<d-front-matter></d-front-matter>');
    $('#distill-front-matter').wrap(front_matter);

    // create d-title
    $('.d-title').changeElementType('d-title');

    // create d-byline
    var byline = $('<d-byline></d-byline>');
    $('.d-byline').replaceWith(byline);

    // create d-article
    var article = $('<d-article></d-article>');
    $('.d-article').wrap(article).children().unwrap();

    // move posts container into article
    $('.posts-container').appendTo($('d-article'));

    // create d-appendix
    $('.d-appendix').changeElementType('d-appendix');

    // flag indicating that we have appendix items
    var appendix = $('.appendix-bottom').children('h3').length > 0;

    // replace footnotes with <d-footnote>
    $('.footnote-ref').each(function(i, val) {
      appendix = true;
      var href = $(this).attr('href');
      var id = href.replace('#', '');
      var fn = $('#' + id);
      var fn_p = $('#' + id + '>p');
      fn_p.find('.footnote-back').remove();
      var text = fn_p.html();
      var dtfn = $('<d-footnote></d-footnote>');
      dtfn.html(text);
      $(this).replaceWith(dtfn);
    });
    // remove footnotes
    $('.footnotes').remove();

    // move refs into #references-listing
    $('#references-listing').replaceWith($('#refs'));

    $('h1.appendix, h2.appendix').each(function(i, val) {
      $(this).changeElementType('h3');
    });
    $('h3.appendix').each(function(i, val) {
      var id = $(this).attr('id');
      $('.d-contents a[href="#' + id + '"]').parent().remove();
      appendix = true;
      $(this).nextUntil($('h1, h2, h3')).addBack().appendTo($('d-appendix'));
    });

    // show d-appendix if we have appendix content
    $("d-appendix").css('display', appendix ? 'grid' : 'none');

    // localize layout chunks to just output
    $('.layout-chunk').each(function(i, val) {

      // capture layout
      var layout = $(this).attr('data-layout');

      // apply layout to markdown level block elements
      var elements = $(this).children().not('details, div.sourceCode, pre, script');
      elements.each(function(i, el) {
        var layout_div = $('<div class="' + layout + '"></div>');
        if (layout_div.hasClass('shaded')) {
          var shaded_content = $('<div class="shaded-content"></div>');
          $(this).wrap(shaded_content);
          $(this).parent().wrap(layout_div);
        } else {
          $(this).wrap(layout_div);
        }
      });


      // unwrap the layout-chunk div
      $(this).children().unwrap();
    });

    // remove code block used to force  highlighting css
    $('.distill-force-highlighting-css').parent().remove();

    // remove empty line numbers inserted by pandoc when using a
    // custom syntax highlighting theme
    $('code.sourceCode a:empty').remove();

    // load distill framework
    load_distill_framework();

    // wait for window.distillRunlevel == 4 to do post processing
    function distill_post_process() {

      if (!window.distillRunlevel || window.distillRunlevel < 4)
        return;

      // hide author/affiliations entirely if we have no authors
      var front_matter = JSON.parse($("#distill-front-matter").html());
      var have_authors = front_matter.authors && front_matter.authors.length > 0;
      if (!have_authors)
        $('d-byline').addClass('hidden');

      // article with toc class
      $('.d-contents').parent().addClass('d-article-with-toc');

      // strip links that point to #
      $('.authors-affiliations').find('a[href="#"]').removeAttr('href');

      // add orcid ids
      $('.authors-affiliations').find('.author').each(function(i, el) {
        var orcid_id = front_matter.authors[i].orcidID;
        if (orcid_id) {
          var a = $('<a></a>');
          a.attr('href', 'https://orcid.org/' + orcid_id);
          var img = $('<img></img>');
          img.addClass('orcid-id');
          img.attr('alt', 'ORCID ID');
          img.attr('src','data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg==');
          a.append(img);
          $(this).append(a);
        }
      });

      // hide elements of author/affiliations grid that have no value
      function hide_byline_column(caption) {
        $('d-byline').find('h3:contains("' + caption + '")').parent().css('visibility', 'hidden');
      }

      // affiliations
      var have_affiliations = false;
      for (var i = 0; i<front_matter.authors.length; ++i) {
        var author = front_matter.authors[i];
        if (author.affiliation !== "&nbsp;") {
          have_affiliations = true;
          break;
        }
      }
      if (!have_affiliations)
        $('d-byline').find('h3:contains("Affiliations")').css('visibility', 'hidden');

      // published date
      if (!front_matter.publishedDate)
        hide_byline_column("Published");

      // document object identifier
      var doi = $('d-byline').find('h3:contains("DOI")');
      var doi_p = doi.next().empty();
      if (!front_matter.doi) {
        // if we have a citation and valid citationText then link to that
        if ($('#citation').length > 0 && front_matter.citationText) {
          doi.html('Citation');
          $('<a href="#citation"></a>')
            .text(front_matter.citationText)
            .appendTo(doi_p);
        } else {
          hide_byline_column("DOI");
        }
      } else {
        $('<a></a>')
           .attr('href', "https://doi.org/" + front_matter.doi)
           .html(front_matter.doi)
           .appendTo(doi_p);
      }

       // change plural form of authors/affiliations
      if (front_matter.authors.length === 1) {
        var grid = $('.authors-affiliations');
        grid.children('h3:contains("Authors")').text('Author');
        grid.children('h3:contains("Affiliations")').text('Affiliation');
      }

      // remove d-appendix and d-footnote-list local styles
      $('d-appendix > style:first-child').remove();
      $('d-footnote-list > style:first-child').remove();

      // move appendix-bottom entries to the bottom
      $('.appendix-bottom').appendTo('d-appendix').children().unwrap();
      $('.appendix-bottom').remove();

      // hoverable references
      $('span.citation[data-cites]').each(function() {
        if ($(this).children()[0].nodeName == "D-FOOTNOTE") {
          var fn = $(this).children()[0]
          $(this).html(fn.shadowRoot.querySelector("sup"))
          $(this).id = fn.id
          fn.remove()
        }
        var refs = $(this).attr('data-cites').split(" ");
        var refHtml = refs.map(function(ref) {
          return "<p>" + $('#ref-' + ref).html() + "</p>";
        }).join("\n");
        window.tippy(this, {
          allowHTML: true,
          content: refHtml,
          maxWidth: 500,
          interactive: true,
          interactiveBorder: 10,
          theme: 'light-border',
          placement: 'bottom-start'
        });
      });

      // fix footnotes in tables (#411)
      // replacing broken distill.pub feature
      $('table d-footnote').each(function() {
        // we replace internal showAtNode methode which is triggered when hovering a footnote
        this.hoverBox.showAtNode = function(node) {
          // ported from https://github.com/distillpub/template/pull/105/files
          calcOffset = function(elem) {
              let x = elem.offsetLeft;
              let y = elem.offsetTop;
              // Traverse upwards until an `absolute` element is found or `elem`
              // becomes null.
              while (elem = elem.offsetParent && elem.style.position != 'absolute') {
                  x += elem.offsetLeft;
                  y += elem.offsetTop;
              }

              return { left: x, top: y };
          }
          // https://developer.mozilla.org/en-US/docs/Web/API/HTMLElement/offsetTop
          const bbox = node.getBoundingClientRect();
          const offset = calcOffset(node);
          this.show([offset.left + bbox.width, offset.top + bbox.height]);
        }
      })

      // clear polling timer
      clearInterval(tid);

      // show body now that everything is ready
      on_load_complete();
    }

    var tid = setInterval(distill_post_process, 50);
    distill_post_process();

  }

  function init_downlevel() {

    init_common();

     // insert hr after d-title
    $('.d-title').after($('<hr class="section-separator"/>'));

    // check if we have authors
    var front_matter = JSON.parse($("#distill-front-matter").html());
    var have_authors = front_matter.authors && front_matter.authors.length > 0;

    // manage byline/border
    if (!have_authors)
      $('.d-byline').remove();
    $('.d-byline').after($('<hr class="section-separator"/>'));
    $('.d-byline a').remove();

    // remove toc
    $('.d-contents').remove();

    // move appendix elements
    $('h1.appendix, h2.appendix').each(function(i, val) {
      $(this).changeElementType('h3');
    });
    $('h3.appendix').each(function(i, val) {
      $(this).nextUntil($('h1, h2, h3')).addBack().appendTo($('.d-appendix'));
    });


    // inject headers into references and footnotes
    var refs_header = $('<h3></h3>');
    refs_header.text('References');
    $('#refs').prepend(refs_header);

    var footnotes_header = $('<h3></h3');
    footnotes_header.text('Footnotes');
    $('.footnotes').children('hr').first().replaceWith(footnotes_header);

    // move appendix-bottom entries to the bottom
    $('.appendix-bottom').appendTo('.d-appendix').children().unwrap();
    $('.appendix-bottom').remove();

    // remove appendix if it's empty
    if ($('.d-appendix').children().length === 0)
      $('.d-appendix').remove();

    // prepend separator above appendix
    $('.d-appendix').before($('<hr class="section-separator" style="clear: both"/>'));

    // trim code
    $('pre>code').each(function(i, val) {
      $(this).html($.trim($(this).html()));
    });

    // move posts-container right before article
    $('.posts-container').insertBefore($('.d-article'));

    $('body').addClass('downlevel');

    on_load_complete();
  }


  function init_common() {

    // jquery plugin to change element types
    (function($) {
      $.fn.changeElementType = function(newType) {
        var attrs = {};

        $.each(this[0].attributes, function(idx, attr) {
          attrs[attr.nodeName] = attr.nodeValue;
        });

        this.replaceWith(function() {
          return $("<" + newType + "/>", attrs).append($(this).contents());
        });
      };
    })(jQuery);

    // prevent underline for linked images
    $('a > img').parent().css({'border-bottom' : 'none'});

    // mark non-body figures created by knitr chunks as 100% width
    $('.layout-chunk').each(function(i, val) {
      var figures = $(this).find('img, .html-widget');
      // ignore leaflet img layers (#106)
      figures = figures.filter(':not(img[class*="leaflet"])')
      if ($(this).attr('data-layout') !== "l-body") {
        figures.css('width', '100%');
      } else {
        figures.css('max-width', '100%');
        figures.filter("[width]").each(function(i, val) {
          var fig = $(this);
          fig.css('width', fig.attr('width') + 'px');
        });

      }
    });

    // auto-append index.html to post-preview links in file: protocol
    // and in rstudio ide preview
    $('.post-preview').each(function(i, val) {
      if (window.location.protocol === "file:")
        $(this).attr('href', $(this).attr('href') + "index.html");
    });

    // get rid of index.html references in header
    if (window.location.protocol !== "file:") {
      $('.distill-site-header a[href]').each(function(i,val) {
        $(this).attr('href', $(this).attr('href').replace(/^index[.]html/, "./"));
      });
    }

    // add class to pandoc style tables
    $('tr.header').parent('thead').parent('table').addClass('pandoc-table');
    $('.kable-table').children('table').addClass('pandoc-table');

    // add figcaption style to table captions
    $('caption').parent('table').addClass("figcaption");

    // initialize posts list
    if (window.init_posts_list)
      window.init_posts_list();

    // implmement disqus comment link
    $('.disqus-comment-count').click(function() {
      window.headroom_prevent_pin = true;
      $('#disqus_thread').toggleClass('hidden');
      if (!$('#disqus_thread').hasClass('hidden')) {
        var offset = $(this).offset();
        $(window).resize();
        $('html, body').animate({
          scrollTop: offset.top - 35
        });
      }
    });
  }

  document.addEventListener('DOMContentLoaded', function() {
    if (is_downlevel_browser())
      init_downlevel();
    else
      window.addEventListener('WebComponentsReady', init_distill);
  });

  </script>

  <!--/radix_placeholder_distill-->
  <script src="sales-hypothesis-analysis_files/header-attrs-2.14/header-attrs.js"></script>
  <script src="sales-hypothesis-analysis_files/jquery-3.6.0/jquery-3.6.0.min.js"></script>
  <script src="sales-hypothesis-analysis_files/popper-2.6.0/popper.min.js"></script>
  <link href="sales-hypothesis-analysis_files/tippy-6.2.7/tippy.css" rel="stylesheet" />
  <link href="sales-hypothesis-analysis_files/tippy-6.2.7/tippy-light-border.css" rel="stylesheet" />
  <script src="sales-hypothesis-analysis_files/tippy-6.2.7/tippy.umd.min.js"></script>
  <script src="sales-hypothesis-analysis_files/anchor-4.2.2/anchor.min.js"></script>
  <script src="sales-hypothesis-analysis_files/bowser-1.9.3/bowser.min.js"></script>
  <script src="sales-hypothesis-analysis_files/webcomponents-2.0.0/webcomponents.js"></script>
  <script src="sales-hypothesis-analysis_files/distill-2.2.21/template.v2.js"></script>
  <!--radix_placeholder_site_in_header-->
  <!--/radix_placeholder_site_in_header-->


</head>

<body>

<!--radix_placeholder_front_matter-->

<script id="distill-front-matter" type="text/json">
{"title":"Online Marketing Ads click prediction: End-to-End workflow for machine learning solution","description":"This post demonstrates the process for predicting number of clicks with focus on procedures and logic behind the modelling decision making process. While the ideal aim is to produce a model that accurately predicts number of clicks, a great model with the least error is compromise between cost of computational power and time on one hand, and the expected marginal benefit to the business. Thus, the approach is to develop an accurate model that is computationally least expensive. By this, a benchmark model was defined as a minimal standard against which the model developed is deem acceptable or not. An exploratory analysis was undertaken to decide which algorithmn to employ. The findings of the exploratory analysis which included predictor variables not having a linear relationship with the target variable, potential loss of about 28% of data attributable to missing data, absence of multicollinearity and presence of outliers among others, informed the decision to use a non-parametric model hence decision tree based model. In other to choose a model that handles missing data relatively well, HistGradientBoostingRegressor was used. Various approaches to improving model performance were also demonstrated within the limited time and computational power available. Hyperparameter tuning was undertaken with grid search cross validation using few combinations of parameters. With the improved model from hyperparameter tuning, bagging was employed to further reduce error and overfitting. An alternative ensembel model of decision tree was also implemented with Extreme Gradient boosting Random Forest (XGBRFRegressor). It is duly recognized that not all techniques, some of which are capable of further reducing error, where employed due to the constrain of computation time. Nonetheless, an unexhaustive highlight of how to improve the model has been provided.","authors":[{"author":"Linus Agbleze","authorURL":"https://agbleze.github.io/Portfolio/","affiliation":"&nbsp;","affiliationURL":"#","orcidID":""}],"publishedDate":"2022-09-10T00:00:00.000+02:00","citationText":"Agbleze, 2022"}
</script>

<!--/radix_placeholder_front_matter-->
<!--radix_placeholder_navigation_before_body-->
<!--/radix_placeholder_navigation_before_body-->
<!--radix_placeholder_site_before_body-->
<!--/radix_placeholder_site_before_body-->

<div class="d-title">
<h1>Online Marketing Ads click prediction: End-to-End workflow for machine learning solution</h1>
<!--radix_placeholder_categories-->
<div class="dt-tags">
<div class="dt-tag">Machine learning</div>
<div class="dt-tag">Python</div>
</div>
<!--/radix_placeholder_categories-->
<p><p>This post demonstrates the process for predicting number of clicks with focus on procedures and logic behind the modelling decision making process. While the ideal aim is to produce a model that accurately predicts number of clicks, a great model with the least error is compromise between cost of computational power and time on one hand, and the expected marginal benefit to the business. Thus, the approach is to develop an accurate model that is computationally least expensive. By this, a benchmark model was defined as a minimal standard against which the model developed is deem acceptable or not. An exploratory analysis was undertaken to decide which algorithmn to employ. The findings of the exploratory analysis which included predictor variables not having a linear relationship with the target variable, potential loss of about 28% of data attributable to missing data, absence of multicollinearity and presence of outliers among others, informed the decision to use a non-parametric model hence decision tree based model. In other to choose a model that handles missing data relatively well, HistGradientBoostingRegressor was used. Various approaches to improving model performance were also demonstrated within the limited time and computational power available. Hyperparameter tuning was undertaken with grid search cross validation using few combinations of parameters. With the improved model from hyperparameter tuning, bagging was employed to further reduce error and overfitting. An alternative ensembel model of decision tree was also implemented with Extreme Gradient boosting Random Forest (XGBRFRegressor). It is duly recognized that not all techniques, some of which are capable of further reducing error, where employed due to the constrain of computation time. Nonetheless, an unexhaustive highlight of how to improve the model has been provided.</p></p>
</div>

<div class="d-byline">
  Linus Agbleze <a href="https://agbleze.github.io/Portfolio/" class="uri">https://agbleze.github.io/Portfolio/</a> 
  
<br/>2022-09-10
</div>

<div class="d-article">
<div class="layout-chunk" data-layout="l-body">

</div>
<h2 id="problem-statement">Problem Statement</h2>
<p>The business problem define by trivago requires a strategic decision making which seeks to balance the satisfaction between end-users at opposite divide; mainly advertisers who run campaigns and users (consumers) who consume (click) those campaign ads. Identified among others is the the number of clicks as a Key Performance Indicator (KPI) capturing satisfaction by both users and advertisers. From this, the higher the number of clicks, the higher the perceived campaign success by advertisers and the higher the user utility (to some extent). With this understanding, a higher number of click is likely to result from displaying ads with accommodattion characteristics that satisfy users interests. This suggests that predicting number of clicks can be done by identifying patterns in accommodation ads characteristics that compel users to click more often.</p>
<p>Thus, the number of clicks is the target variable and variables that capture various properties of the accommodation ads are the predictor variable.</p>
<p>In order to translate this understanding into a technical solution that can be communicated to non-technical audience later, there is the need to capture the prevailing challenge as a snapshot with a problem design framework.</p>
<p>The Situation Complication Question (SCQ) framework was employed for that purpose. A key element is identifying the stakeholder and end-user of the solution to be provided. This will require some level of insider information about the various stakeholders who have identified clicks prediction as their pain point. Hypothetically, the stakeholder is defined to be the bidding team with support from the data science team. The SCQ is depicted below;</p>
<div class="layout-chunk" data-layout="l-body">
<p><img src="sales-hypothesis-analysis_files/figure-html5/unnamed-chunk-3-1.png" width="937" data-distill-preview=1 style="display: block; margin: auto;" /></p>
</div>
<h3 id="identifying-variables-in-the-data">Identifying variables in the data</h3>
<p>In developing an algorithm for prediction, identifying the variables to use and how to categorize them is important. The following were deduced;</p>
<h4 id="target-response-outcome-variable">Target / Response / Outcome variable</h4>
<p>n clicks: Continuous variable</p>
<h4 id="predictor-feature-variables">Predictor / feature variables</h4>
<p> city id: Categorical</p>
<p> content score: ordinal</p>
<p> n images: continuous</p>
<p> distance to center: continuous</p>
<p> avg rating: discrete</p>
<p> n reviews: continuous</p>
<p> avg rank: ordinal</p>
<p> avg price: Continuous</p>
<p> avg saving percent: discrete</p>
<p>By identifying the type of variable, appropriate visualization can be undertaken for different variables during exploratory data analysis.</p>
<h3 id="exploratory-data-analysis">Exploratory data analysis</h3>
<p>Given that target variable is present, a supervised machine learning algorithmn will be used for the prediction. Generally, algorithmns can be seen to belong to two categories, namely parametric and non-parametric. Deciding on the category from which an algorithmn is chosen for modelling is dependent on whether the dataset exhibits characteristics that satisfy certain assumptions. This need to be verified by undertaking exploratory analysis and visualization hence the focus of this section. The insights gained from the exploratory analysis serves as basis on which to narrow down the selection of algorithmns that will produce good results</p>
<p>The implementation of the exploratory analysis is highlighted below beginnning with importing packages and loading the dataset</p>
<div class="layout-chunk" data-layout="l-body">

</div>
<div class="layout-chunk" data-layout="l-body">
<pre><code>Data types 
 hotel_id              float64
city_id               float64
content_score         float64
n_images              float64
distance_to_center    float64
avg_rating            float64
stars                 float64
n_reviews             float64
avg_rank              float64
avg_price             float64
avg_saving_percent    float64
n_clicks                int64
dtype: object</code></pre>
<pre><code>       hotel_id   city_id  ...  avg_saving_percent  n_clicks
0  9.767406e+10  134520.0  ...                18.0         0
1  9.768889e+10  133876.0  ...                28.0         4
2  9.811544e+10  133732.0  ...                27.0        44
3  9.824279e+10   43772.0  ...                 2.0         4
4  9.833438e+10   50532.0  ...                 0.0        10

[5 rows x 12 columns]</code></pre>
</div>
<h5 id="basic-descriptive-statistics">Basic descriptive statistics</h5>
<p>One of the first checks to do during exploratory analysis is to obtain a description of the data by identifying</p>
<p>the mean, median, quantiles among others to enable understanding of the data distribution among others. This is done as follows;</p>
<div class="layout-chunk" data-layout="l-body">
<pre><code>           hotel_id        city_id  ...  avg_saving_percent       n_clicks
count  3.964870e+05  395979.000000  ...       396317.000000  396487.000000
mean   1.326304e+11  149193.465376  ...            7.179601      13.781980
std    1.033722e+11  219189.285044  ...           13.081529     123.572896
min    1.557962e+08       2.000000  ...            0.000000       0.000000
25%    4.062255e+10   32014.000000  ...            0.000000       0.000000
50%    1.087280e+11   55122.000000  ...            0.000000       0.000000
75%    2.281935e+11  137464.000000  ...           10.000000       2.000000
max    3.237114e+11  878736.000000  ...           99.000000   13742.000000

[8 rows x 12 columns]</code></pre>
</div>
<p>From the above, the maximum avg_price is 8000 while the 75th percentile is 120 which suggests there could be outliers. This high difference between the maximum and the 75th percentile is also noted for n_views, n_clicks among others. The issue of outliers will be better visualize with a boxplot and is a key consideration in the exploration in determining which model to choose as some models are more robust to outliers than others.</p>
<h4 id="missing-values">Missing values</h4>
<p>An important consideration for analysis and modelling is the proportion of missing values. This usually defines the type of preprocessing tasks which could include imputing missing values or choosing a model that handles that well. Hence, the number of missing values are analyzed below to make such decision.</p>
<div class="layout-chunk" data-layout="l-body">
<pre><code>hotel_id                   0
city_id                  508
content_score            508
n_images                 509
distance_to_center       529
avg_rating            110398
stars                    562
n_reviews                529
avg_rank                   0
avg_price                170
avg_saving_percent       170
n_clicks                   0
dtype: int64</code></pre>
</div>
<div class="layout-chunk" data-layout="l-body">
<pre><code>Percentage of data left if all missing values are removed:  72.14%</code></pre>
</div>
<p>72% (286,026) of data will remain after removing all missing values. While a sizeable amount of data will still be left for modeling, a lost of 28% of data could be quite impactful on the model to be developed particularly when attempt is made to impute missing values. Dropping all missing values will lead to loss of information while an imputation could lead to introducing a sizeable amount of noise or fake data that may deviate from reality. In this regard, a decision is made in favour of choosing an algorithm that natively handles and less impacted by missing values.</p>
<h4 id="data-visualization-to-ascertain-certain-assumptions-required-by-some-models">Data visualization to ascertain certain assumptions required by some models</h4>
<p>Various plots are used to visualize certain characteristics of the data to ascertain if certain assumptions are met for some models to be used. Generally, parametric models like linear regression requires data to be normally distributed and a linear relationship to exist between the predictor and target variable (s).</p>
<p>To investigate this, histogram was used to visualize the distribution of data for continuous variables. This is implemented below.</p>
<div class="layout-chunk" data-layout="l-body">
<pre><code>33213</code></pre>
</div>
<p>city_id is a high cardinality categorical variable with 33213 unique values. Plotting that number of bars will not be legible and requires some computational power.</p>
<div class="layout-chunk" data-layout="l-body">

</div>
<div class="layout-chunk" data-layout="l-body">

</div>
<div class="layout-chunk" data-layout="l-body">
<p><img src="sales-hypothesis-analysis_files/figure-html5/unnamed-chunk-2-1.png" width="672" style="display: block; margin: auto;" /><img src="sales-hypothesis-analysis_files/figure-html5/unnamed-chunk-2-2.png" width="672" style="display: block; margin: auto;" /><img src="sales-hypothesis-analysis_files/figure-html5/unnamed-chunk-2-3.png" width="672" style="display: block; margin: auto;" /><img src="sales-hypothesis-analysis_files/figure-html5/unnamed-chunk-2-4.png" width="672" style="display: block; margin: auto;" /><img src="sales-hypothesis-analysis_files/figure-html5/unnamed-chunk-2-5.png" width="672" style="display: block; margin: auto;" /><img src="sales-hypothesis-analysis_files/figure-html5/unnamed-chunk-2-6.png" width="672" style="display: block; margin: auto;" /><img src="sales-hypothesis-analysis_files/figure-html5/unnamed-chunk-2-7.png" width="672" style="display: block; margin: auto;" /><img src="sales-hypothesis-analysis_files/figure-html5/unnamed-chunk-2-8.png" width="672" style="display: block; margin: auto;" /><img src="sales-hypothesis-analysis_files/figure-html5/unnamed-chunk-2-9.png" width="672" style="display: block; margin: auto;" /><img src="sales-hypothesis-analysis_files/figure-html5/unnamed-chunk-2-10.png" width="672" style="display: block; margin: auto;" /></p>
</div>
<p>The result shows that most of the variables are right skewed. The variable content_score is left skewed (not too bad). The variables n_images, distance_to_center, avg_rating, n_reviews, avg_rank, avg_price, avg_saving_percent and n_clicks are right skewed</p>
<p>city_id is a categorical variable which has been hashed. A number of ways exist to treat categorical variable including OneHot encoding method. This method has the disadvantage of significantly increasing the dimensionality of the dataset when several categorical variables are present or even when a single categorical variable with numerous classes exist. This could lead to significant increase in computation time of model. Hence, the decision on how to handle city_id variable is made by first understanding number of classes it has below.</p>
<div class="layout-chunk" data-layout="l-body">
<pre><code>33213</code></pre>
</div>
<p>city_id is a high cardinality categorical variable with 33213 unique values. This will make OneHot encoding an expensive exercise for modelling. Worthy of notice is that the city_id has been hashed as numeric values which makes it acceptable for the sklearn API.</p>
<h4 id="boxplot-to-visualize-outliers">Boxplot to visualize outliers</h4>
<p>As indicated earlier, a key factor in deciding which algorithm to use is the distribution of the target and predictor variables. Some algorithms are influence by the presence of outliers hence analyzed to make an inform decision on which class of algorithm to choose from.</p>
<div class="layout-chunk" data-layout="l-body">

</div>
<div class="layout-chunk" data-layout="l-body">

</div>
<p>From the boxplot of content_score, the least score of 7 is quite distant from the lower fence and could be seen as an outlier. The boxplot of other variables such as n_images, distance_to_center, avg_rating, n_reviews, avg_price,n_clicks suggest outliers are present upon visual inspection. This information informs our decision making process about which model to choose. From the visualization, a decision is made in favour of choosing a model that is fairly robust against outliers instead of removing or imputing the outliers altogether.</p>
<h3 id="scatterplot-to-visualize-outliers">Scatterplot to visualize outliers</h3>
<p>In order to investigate whether or not a linear relationship between the target variable and predictor variables exists, a scatterplot is used to visualize them. Thus, all the predictor variables are plotted against n_clicks on the y-axis This is implemented below;</p>
<div class="layout-chunk" data-layout="l-body">

</div>
<div class="layout-chunk" data-layout="l-body">
<pre><code>










/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/plotnine/layer.py:411: PlotnineWarning:

geom_point : Removed 508 rows containing missing values.

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/plotnine/layer.py:411: PlotnineWarning:

geom_point : Removed 509 rows containing missing values.

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/plotnine/layer.py:411: PlotnineWarning:

geom_point : Removed 529 rows containing missing values.

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/plotnine/layer.py:411: PlotnineWarning:

geom_point : Removed 110398 rows containing missing values.

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/plotnine/layer.py:411: PlotnineWarning:

geom_point : Removed 562 rows containing missing values.

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/plotnine/layer.py:411: PlotnineWarning:

geom_point : Removed 529 rows containing missing values.

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/plotnine/layer.py:411: PlotnineWarning:

geom_point : Removed 170 rows containing missing values.

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/plotnine/layer.py:411: PlotnineWarning:

geom_point : Removed 170 rows containing missing values.</code></pre>
<p><img src="sales-hypothesis-analysis_files/figure-html5/unnamed-chunk-2-21.png" width="614" style="display: block; margin: auto;" /><img src="sales-hypothesis-analysis_files/figure-html5/unnamed-chunk-2-22.png" width="614" style="display: block; margin: auto;" /><img src="sales-hypothesis-analysis_files/figure-html5/unnamed-chunk-2-23.png" width="614" style="display: block; margin: auto;" /><img src="sales-hypothesis-analysis_files/figure-html5/unnamed-chunk-2-24.png" width="614" style="display: block; margin: auto;" /><img src="sales-hypothesis-analysis_files/figure-html5/unnamed-chunk-2-25.png" width="614" style="display: block; margin: auto;" /><img src="sales-hypothesis-analysis_files/figure-html5/unnamed-chunk-2-26.png" width="614" style="display: block; margin: auto;" /><img src="sales-hypothesis-analysis_files/figure-html5/unnamed-chunk-2-27.png" width="614" style="display: block; margin: auto;" /><img src="sales-hypothesis-analysis_files/figure-html5/unnamed-chunk-2-28.png" width="614" style="display: block; margin: auto;" /><img src="sales-hypothesis-analysis_files/figure-html5/unnamed-chunk-2-29.png" width="614" style="display: block; margin: auto;" /><img src="sales-hypothesis-analysis_files/figure-html5/unnamed-chunk-2-30.png" width="614" style="display: block; margin: auto;" /></p>
</div>
<p>The scatterplots show that none of the predictors has a linear relationship with the response variable. Hence an insight is being gain that parametric algorithms with the assumption of linear relationship such as linear regression are less likely to produce a convincing prediction with low error.</p>
<h3 id="correlation-analysis-to-check-for-multicollinearity">Correlation analysis to check for multicollinearity</h3>
<p>Another exploration to do for the data is checking for multicollinearity among predictor variables given that some algorithmns assume that the variables are not strongly correlated to each other. Strong correlation between variables implies the variables are supplying similar information to the algorithmn hence dimension reduction technique could be used to reduce or select only variables that enable the algorithmn to gain new insights from the data and improve predictive power.</p>
<p>Correlation analysis is undertaken on the predictor variables to check for multicollinearity as follows;.</p>
<div class="layout-chunk" data-layout="l-body">

</div>
<p>hotel_id is not included as predictor because it is just an identifier and offers no real insight about a hotel ad characteristics for modelling.</p>
<div class="layout-chunk" data-layout="l-body">
<pre><code>    city_id  content_score  n_images  ...  avg_rank  avg_price  avg_saving_percent
0  134520.0           70.0       2.0  ...    17.550      81.64                18.0
1  133876.0           67.0       3.0  ...    17.383     189.38                28.0
2  133732.0           39.0       3.0  ...    16.438      57.63                27.0
3   43772.0           59.0       8.0  ...     7.000      72.16                 2.0
4   50532.0           66.0       1.0  ...    12.564     173.25                 0.0

[5 rows x 10 columns]</code></pre>
</div>
<div class="layout-chunk" data-layout="l-body">
<pre><code>                     city_id  content_score  ...  avg_price  avg_saving_percent
city_id             1.000000      -0.038639  ...  -0.052988           -0.057525
content_score      -0.038639       1.000000  ...  -0.074976            0.388301
n_images           -0.003005       0.012113  ...  -0.001429            0.004675
distance_to_center  0.015587      -0.113261  ...   0.007168           -0.014881
avg_rating          0.043550      -0.135439  ...   0.211038           -0.153566
stars              -0.043517       0.535937  ...  -0.010134            0.468344
n_reviews          -0.091224       0.289233  ...  -0.002831            0.372850
avg_rank           -0.029956      -0.142255  ...   0.016033           -0.121451
avg_price          -0.052988      -0.074976  ...   1.000000           -0.028133
avg_saving_percent -0.057525       0.388301  ...  -0.028133            1.000000

[10 rows x 10 columns]</code></pre>
</div>
<div class="layout-chunk" data-layout="l-body">
<pre><code>&lt;string&gt;:1: DeprecationWarning:

`np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations</code></pre>
<p><img src="sales-hypothesis-analysis_files/figure-html5/unnamed-chunk-2-41.png" width="614" style="display: block; margin: auto;" /></p>
</div>
<p>The correlation analysis shows a weak correlation between the predictor variables hence multicollinearity is absent.</p>
<h3 id="using-insights-gained-from-exploratory-analysis-to-inform-modelling-approach">Using insights gained from exploratory analysis to inform modelling approach</h3>
<p>The findings of non-linear relationship between the predictors and the outcome variable(s), presence of outliers, and sizeable missing values suggest that a non-parametric model that handles non-linear relationship, outliers and missing values well needs to be used. Moreover, the business goal is to achieve a good precision rather than interpretability of the model. Given that a bonus question requires assessing and identifying the input variables with high predictive power, the model to be chosen need not be a complete blackbox such that it becomes too difficult to provide such answers.</p>
<p>On the basis of the findings from the exploratory analysis, a model that employs decision tree will be used. Before that, a critical aspect of the findings was that missing data is present and has to handled. For this, a model that natively handles missing data as part of the modelling process is explored. Hence, HistGradientBoostingRegressor from the sklearn library is implemented because it is a decison tree based model with an inbuilt handling of missing data.</p>
<p>The implementation is as follows;</p>
<h3 id="splitting-into-training-and-validation-dataset">Splitting into Training and validation dataset</h3>
<p>The decision on how the data is proportioned for learning and evaluation is one that is sometimes subjective. For this exercise, 70% of the data is used for training and 30% for validation. Given that, there is relatively enough data, 70% dataset will likely provide enough data points to learn and derived as much insight from and 30% will still be probably be enough to test the model on data points capturing enough of the varying charateristerics that are unknown to the model. The implementation is as follows;</p>
<div class="layout-chunk" data-layout="l-body">

</div>
<h4 id="function-for-evaluation-metric-of-model-normalized-weighted-mean-squared-error-nwmse">Function for evaluation metric of model Normalized Weighted Mean Squared Error (NWMSE)</h4>
<p>Usually models are evaluated using Mean Squared Error (MSE), Root Mean Squared Error (RMSE) among others. These metrics have already been implemented in python and will be considered. However, for this work, Normalized Weighted Mean Squared Error (NWMSE) will be used for the evaluation. This needs to implemented according to the formulae of the question hence done below.</p>
<div class="layout-chunk" data-layout="l-body">

</div>
<p>The normalized weighted MSE (NWMSE) is used to create a customized scorer to be used in the pipeline of the model. Given that the aim is to minimize the error, greater_is_better is set to false. This is implemented below</p>
<div class="layout-chunk" data-layout="l-body">

</div>
<h3 id="define-baseline-model">Define baseline model</h3>
<p>Modelling can be an iterative process in attempt to optimize and arrive at the best generalizable model.</p>
<p>A key question that needs to be answered before beginning the modelling process is at what level of performance do we accept the model as doing a better job than random guesses hence adding value to the business. There is the need to ascertain that efforts and resources put into developing a model is worthwhile than doing nothing and making radom guesses (in which case those resources are also saved)</p>
<p>To provide an objective answer to this question requires defining a baseline model that the model to be developed needs to perform better than in order to be viable for acceptance and usage. This implies that, all algorithmns used to develop the model including those for which hyperparameters have been tuned needs to do better than the baseline model. Once this is satisfied, a comparison between the models are made to select the best model using the evaluation metric define.</p>
<p>In defining the baseline model, an assumption is made that, for each observation, the average of all clicks in the training data is predicted as the number of clicks for the validation data. With this, the error is estimated for the baseline model for which the actual model needs to do better. Thus, the baseline model is estimated as follows,</p>
<div class="layout-chunk" data-layout="l-body">
<pre><code>Average click is  14.078</code></pre>
</div>
<p>With the assumption that prediction for all observations in the validation dataset is the average clicks in training data, the the RMSE and NWMSE is estimated as follows;</p>
<div class="layout-chunk" data-layout="l-body">
<pre><code>Validation data NWMSE for baseline model:  0.550</code></pre>
<pre><code>Validation data RMSE for baseline model:  114.057</code></pre>
<pre><code>Validation data MAE for baseline model:  22.300</code></pre>
</div>
<p>From the above analysis, the baseline model metric performance is 0.55 for NWMSE, 114.057 for RMSE and 22.3 for MAE. Thus for the actual model to be accepted as worthy of adding value to business, it has to achieve an error better (lower) than NWMSE estimated or any of those identified metrics.</p>
<h3 id="developing-model-for-click-prediction-histgradientboostingregressor">Developing model for click prediction  HistGradientBoostingRegressor</h3>
<p>After defining the baseline model performance, the actual model development is done as follows;</p>
<div class="layout-chunk" data-layout="l-body">
<pre><code>HistGradientBoostingRegressor(random_state=2022,
                              scoring=make_scorer(norm_weighted_mse, greater_is_better=False))

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning:

X does not have valid feature names, but HistGradientBoostingRegressor was fitted with feature names

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning:

X does not have valid feature names, but HistGradientBoostingRegressor was fitted with feature names

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning:

X does not have valid feature names, but HistGradientBoostingRegressor was fitted with feature names

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning:

X does not have valid feature names, but HistGradientBoostingRegressor was fitted with feature names

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning:

X does not have valid feature names, but HistGradientBoostingRegressor was fitted with feature names

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning:

X does not have valid feature names, but HistGradientBoostingRegressor was fitted with feature names

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning:

X does not have valid feature names, but HistGradientBoostingRegressor was fitted with feature names

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning:

X does not have valid feature names, but HistGradientBoostingRegressor was fitted with feature names

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning:

X does not have valid feature names, but HistGradientBoostingRegressor was fitted with feature names

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning:

X does not have valid feature names, but HistGradientBoostingRegressor was fitted with feature names

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning:

X does not have valid feature names, but HistGradientBoostingRegressor was fitted with feature names

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning:

X does not have valid feature names, but HistGradientBoostingRegressor was fitted with feature names

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning:

X does not have valid feature names, but HistGradientBoostingRegressor was fitted with feature names

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning:

X does not have valid feature names, but HistGradientBoostingRegressor was fitted with feature names

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning:

X does not have valid feature names, but HistGradientBoostingRegressor was fitted with feature names

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning:

X does not have valid feature names, but HistGradientBoostingRegressor was fitted with feature names

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning:

X does not have valid feature names, but HistGradientBoostingRegressor was fitted with feature names

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning:

X does not have valid feature names, but HistGradientBoostingRegressor was fitted with feature names

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning:

X does not have valid feature names, but HistGradientBoostingRegressor was fitted with feature names

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning:

X does not have valid feature names, but HistGradientBoostingRegressor was fitted with feature names

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning:

X does not have valid feature names, but HistGradientBoostingRegressor was fitted with feature names

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning:

X does not have valid feature names, but HistGradientBoostingRegressor was fitted with feature names

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning:

X does not have valid feature names, but HistGradientBoostingRegressor was fitted with feature names

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning:

X does not have valid feature names, but HistGradientBoostingRegressor was fitted with feature names

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning:

X does not have valid feature names, but HistGradientBoostingRegressor was fitted with feature names

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning:

X does not have valid feature names, but HistGradientBoostingRegressor was fitted with feature names

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning:

X does not have valid feature names, but HistGradientBoostingRegressor was fitted with feature names

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning:

X does not have valid feature names, but HistGradientBoostingRegressor was fitted with feature names

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning:

X does not have valid feature names, but HistGradientBoostingRegressor was fitted with feature names

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning:

X does not have valid feature names, but HistGradientBoostingRegressor was fitted with feature names

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning:

X does not have valid feature names, but HistGradientBoostingRegressor was fitted with feature names

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning:

X does not have valid feature names, but HistGradientBoostingRegressor was fitted with feature names

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning:

X does not have valid feature names, but HistGradientBoostingRegressor was fitted with feature names

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning:

X does not have valid feature names, but HistGradientBoostingRegressor was fitted with feature names

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning:

X does not have valid feature names, but HistGradientBoostingRegressor was fitted with feature names

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning:

X does not have valid feature names, but HistGradientBoostingRegressor was fitted with feature names

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning:

X does not have valid feature names, but HistGradientBoostingRegressor was fitted with feature names

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning:

X does not have valid feature names, but HistGradientBoostingRegressor was fitted with feature names

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning:

X does not have valid feature names, but HistGradientBoostingRegressor was fitted with feature names

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning:

X does not have valid feature names, but HistGradientBoostingRegressor was fitted with feature names

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning:

X does not have valid feature names, but HistGradientBoostingRegressor was fitted with feature names

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning:

X does not have valid feature names, but HistGradientBoostingRegressor was fitted with feature names

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning:

X does not have valid feature names, but HistGradientBoostingRegressor was fitted with feature names

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning:

X does not have valid feature names, but HistGradientBoostingRegressor was fitted with feature names

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning:

X does not have valid feature names, but HistGradientBoostingRegressor was fitted with feature names

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning:

X does not have valid feature names, but HistGradientBoostingRegressor was fitted with feature names

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning:

X does not have valid feature names, but HistGradientBoostingRegressor was fitted with feature names

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning:

X does not have valid feature names, but HistGradientBoostingRegressor was fitted with feature names

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning:

X does not have valid feature names, but HistGradientBoostingRegressor was fitted with feature names

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning:

X does not have valid feature names, but HistGradientBoostingRegressor was fitted with feature names

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning:

X does not have valid feature names, but HistGradientBoostingRegressor was fitted with feature names

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning:

X does not have valid feature names, but HistGradientBoostingRegressor was fitted with feature names

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning:

X does not have valid feature names, but HistGradientBoostingRegressor was fitted with feature names

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning:

X does not have valid feature names, but HistGradientBoostingRegressor was fitted with feature names

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning:

X does not have valid feature names, but HistGradientBoostingRegressor was fitted with feature names

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning:

X does not have valid feature names, but HistGradientBoostingRegressor was fitted with feature names

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning:

X does not have valid feature names, but HistGradientBoostingRegressor was fitted with feature names

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning:

X does not have valid feature names, but HistGradientBoostingRegressor was fitted with feature names

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning:

X does not have valid feature names, but HistGradientBoostingRegressor was fitted with feature names

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning:

X does not have valid feature names, but HistGradientBoostingRegressor was fitted with feature names

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning:

X does not have valid feature names, but HistGradientBoostingRegressor was fitted with feature names

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning:

X does not have valid feature names, but HistGradientBoostingRegressor was fitted with feature names

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning:

X does not have valid feature names, but HistGradientBoostingRegressor was fitted with feature names

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning:

X does not have valid feature names, but HistGradientBoostingRegressor was fitted with feature names

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning:

X does not have valid feature names, but HistGradientBoostingRegressor was fitted with feature names

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning:

X does not have valid feature names, but HistGradientBoostingRegressor was fitted with feature names

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning:

X does not have valid feature names, but HistGradientBoostingRegressor was fitted with feature names

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning:

X does not have valid feature names, but HistGradientBoostingRegressor was fitted with feature names

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning:

X does not have valid feature names, but HistGradientBoostingRegressor was fitted with feature names

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning:

X does not have valid feature names, but HistGradientBoostingRegressor was fitted with feature names

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning:

X does not have valid feature names, but HistGradientBoostingRegressor was fitted with feature names

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning:

X does not have valid feature names, but HistGradientBoostingRegressor was fitted with feature names

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning:

X does not have valid feature names, but HistGradientBoostingRegressor was fitted with feature names

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning:

X does not have valid feature names, but HistGradientBoostingRegressor was fitted with feature names

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning:

X does not have valid feature names, but HistGradientBoostingRegressor was fitted with feature names

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning:

X does not have valid feature names, but HistGradientBoostingRegressor was fitted with feature names

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning:

X does not have valid feature names, but HistGradientBoostingRegressor was fitted with feature names

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning:

X does not have valid feature names, but HistGradientBoostingRegressor was fitted with feature names

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning:

X does not have valid feature names, but HistGradientBoostingRegressor was fitted with feature names

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning:

X does not have valid feature names, but HistGradientBoostingRegressor was fitted with feature names

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning:

X does not have valid feature names, but HistGradientBoostingRegressor was fitted with feature names

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning:

X does not have valid feature names, but HistGradientBoostingRegressor was fitted with feature names

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning:

X does not have valid feature names, but HistGradientBoostingRegressor was fitted with feature names

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning:

X does not have valid feature names, but HistGradientBoostingRegressor was fitted with feature names

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning:

X does not have valid feature names, but HistGradientBoostingRegressor was fitted with feature names

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning:

X does not have valid feature names, but HistGradientBoostingRegressor was fitted with feature names

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning:

X does not have valid feature names, but HistGradientBoostingRegressor was fitted with feature names

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning:

X does not have valid feature names, but HistGradientBoostingRegressor was fitted with feature names

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning:

X does not have valid feature names, but HistGradientBoostingRegressor was fitted with feature names

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning:

X does not have valid feature names, but HistGradientBoostingRegressor was fitted with feature names

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning:

X does not have valid feature names, but HistGradientBoostingRegressor was fitted with feature names

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning:

X does not have valid feature names, but HistGradientBoostingRegressor was fitted with feature names

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning:

X does not have valid feature names, but HistGradientBoostingRegressor was fitted with feature names

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning:

X does not have valid feature names, but HistGradientBoostingRegressor was fitted with feature names

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning:

X does not have valid feature names, but HistGradientBoostingRegressor was fitted with feature names

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning:

X does not have valid feature names, but HistGradientBoostingRegressor was fitted with feature names

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning:

X does not have valid feature names, but HistGradientBoostingRegressor was fitted with feature names

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning:

X does not have valid feature names, but HistGradientBoostingRegressor was fitted with feature names

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning:

X does not have valid feature names, but HistGradientBoostingRegressor was fitted with feature names

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning:

X does not have valid feature names, but HistGradientBoostingRegressor was fitted with feature names

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning:

X does not have valid feature names, but HistGradientBoostingRegressor was fitted with feature names

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning:

X does not have valid feature names, but HistGradientBoostingRegressor was fitted with feature names

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning:

X does not have valid feature names, but HistGradientBoostingRegressor was fitted with feature names

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning:

X does not have valid feature names, but HistGradientBoostingRegressor was fitted with feature names

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning:

X does not have valid feature names, but HistGradientBoostingRegressor was fitted with feature names

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning:

X does not have valid feature names, but HistGradientBoostingRegressor was fitted with feature names

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning:

X does not have valid feature names, but HistGradientBoostingRegressor was fitted with feature names

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning:

X does not have valid feature names, but HistGradientBoostingRegressor was fitted with feature names

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning:

X does not have valid feature names, but HistGradientBoostingRegressor was fitted with feature names

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning:

X does not have valid feature names, but HistGradientBoostingRegressor was fitted with feature names

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning:

X does not have valid feature names, but HistGradientBoostingRegressor was fitted with feature names

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning:

X does not have valid feature names, but HistGradientBoostingRegressor was fitted with feature names

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning:

X does not have valid feature names, but HistGradientBoostingRegressor was fitted with feature names

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning:

X does not have valid feature names, but HistGradientBoostingRegressor was fitted with feature names

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning:

X does not have valid feature names, but HistGradientBoostingRegressor was fitted with feature names

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning:

X does not have valid feature names, but HistGradientBoostingRegressor was fitted with feature names

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning:

X does not have valid feature names, but HistGradientBoostingRegressor was fitted with feature names

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning:

X does not have valid feature names, but HistGradientBoostingRegressor was fitted with feature names

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning:

X does not have valid feature names, but HistGradientBoostingRegressor was fitted with feature names

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning:

X does not have valid feature names, but HistGradientBoostingRegressor was fitted with feature names

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning:

X does not have valid feature names, but HistGradientBoostingRegressor was fitted with feature names

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning:

X does not have valid feature names, but HistGradientBoostingRegressor was fitted with feature names

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning:

X does not have valid feature names, but HistGradientBoostingRegressor was fitted with feature names

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning:

X does not have valid feature names, but HistGradientBoostingRegressor was fitted with feature names

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning:

X does not have valid feature names, but HistGradientBoostingRegressor was fitted with feature names

/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning:

X does not have valid feature names, but HistGradientBoostingRegressor was fitted with feature names</code></pre>
</div>
<h4 id="determining-extent-to-which-model-captures-pattern-in-the-data">Determining extent to which model captures pattern in the data</h4>
<p>A common metric used to understand how well the model explains the data and capture variance present is the co-efficient of determination. For regression models, the score function is used to estimate that and implemented below;</p>
<div class="layout-chunk" data-layout="l-body">
<pre><code>HistGradientBoosting explains  57.68% of variance in the training dataset</code></pre>
</div>
<p>The score estimates the co-efficient of determination as a metric. A score of 0.577 indicates that only about 57.7% of variance in the training data is captured by the model leaving about 42.3% unaccounted for. Hence alot of insights and patterns in the data has not been captured by the model.</p>
<h4 id="evaluation-of-model">Evaluation of Model</h4>
<p>The Normalized Weighted Mean Squared Error (NWMSE) metric is used to evaluate the model as follows</p>
<div class="layout-chunk" data-layout="l-body">
<pre><code>NWMSE for HistGradientBoosting on validatation dataset is:  0.345</code></pre>
<pre><code>NWMSE for HistGradientBoosting on training dataset is:  0.119</code></pre>
</div>
<p>The result shows a training normalized weighted mean squared error (NWMSE) of 0.119 and 0.345 for the validation data set. The model performs better than the baseline model and will offer even greater business value when optinized. The lower error on the training dataset compared to the validation dataset suggests the model is overfitting.</p>
<p>Other evaluation metrics such as RMSE and MAE are also used to evaluate the model as follows;</p>
<div class="layout-chunk" data-layout="l-body">
<pre><code>Metrics for HistGradientBoosting on Training dataset</code></pre>
<pre><code>
Regression statistics

               Mean Error (ME) : 0.0206
Root Mean Squared Error (RMSE) : 82.9041
     Mean Absolute Error (MAE) : 13.3683</code></pre>
</div>
<div class="layout-chunk" data-layout="l-body">
<pre><code>Metrics for HistGradientBoosting on Validation dataset</code></pre>
<pre><code>
Regression statistics

               Mean Error (ME) : -0.8717
Root Mean Squared Error (RMSE) : 93.0712
     Mean Absolute Error (MAE) : 14.2191</code></pre>
</div>
<p>The results show a RMSE of 82.9 and 93.07 on training and validation dataset respectively. Thus, it is suggested that HistGradientBoosting model is quite overfitting to the training data. The 82.9 RMSE on the training data implies that on the average, the model predicts 83 more or less clicks than the actual number of clicks made. These evaluation metrics collaborate that of the NWMSE that this model is better than the baseline model. Nonetheless, there is the need to improve the model by tuning hyperparameters and using alternative models.</p>
<h4 id="hyperparameter-tuning">Hyperparameter tuning</h4>
<p>While model with the default parameters is doing a better job than random guess (suggested by the baseline model), the aim is to reduce error as much as possible to increase accuracy in prediction. While it is possible to achieve this by using a different model, a common approach is to tune hyperparameters of the model already developed to verify if improvement is attainable. Hyperparameter tuning can be undertaken using the grid search where a set of parameters are specified and a search is made using various combinations or permutation to test which combination best reduces error. Another approach is the Radomized grid search where a range is specified for numeric parameters and the algorithmn randomly select the values of the hyperparameter within the range or condition specified to optimize the model. Bayesian optimization is also another approach.</p>
<p>For grid search, all possible combinations are used while random search randomly choose some of the combination a number of times equal to the value of n_iter argument specified.</p>
<p>GridSearh is demonstrated below. Given the time and computational constraints, limited combinations of paramters are specified to tune HistGradientBoosting Regressor.</p>
<div class="layout-chunk" data-layout="l-body">
<pre><code>GridSearchCV(cv=5, estimator=HistGradientBoostingRegressor(random_state=2022),
             param_grid={&#39;l2_regularization&#39;: [0.1, 0.8, 1],
                         &#39;learning_rate&#39;: [0.1, 0.8, 0.9],
                         &#39;max_depth&#39;: [10, 11, 12],
                         &#39;min_samples_leaf&#39;: [10, 12, 14]},
             scoring=make_scorer(norm_weighted_mse, greater_is_better=False))</code></pre>
<pre><code>Best parameters:  {&#39;l2_regularization&#39;: 0.8, &#39;learning_rate&#39;: 0.1, &#39;max_depth&#39;: 10, &#39;min_samples_leaf&#39;: 10}</code></pre>
</div>
<p>The hyperparameter tuning was done as a quick search for demonstration purpose only. A well designed parameter set will take a longer computational time to achieve good result. That noted, the best model from the tuning is evaluated as follows;</p>
<div class="layout-chunk" data-layout="l-body">
<pre><code>NWMSE for Best tuned HistGradientBoosting on training dataset is:  0.107</code></pre>
<pre><code>NWMSE for Best tuned HistGradientBoosting on validatation dataset is:  0.338</code></pre>
</div>
<p>Based on the normalized weighted MSE, training data error reduced from 0.119 to 0.107 and that of validation reduced from 0.345 to 0.338. Generally, some reduction in error is attainable with hyperparameter tuning particularly when and time and computational resource is dedicated to it.</p>
<div class="layout-chunk" data-layout="l-body">
<pre><code>Metrics for best tuned HistGradientBoosting on Training dataset</code></pre>
<pre><code>
Regression statistics

               Mean Error (ME) : 0.0253
Root Mean Squared Error (RMSE) : 79.0735
     Mean Absolute Error (MAE) : 13.1996</code></pre>
</div>
<div class="layout-chunk" data-layout="l-body">
<pre><code>Metrics for best tuned HistGradientBoosting on Validation dataset</code></pre>
<pre><code>
Regression statistics

               Mean Error (ME) : -0.8557
Root Mean Squared Error (RMSE) : 92.1132
     Mean Absolute Error (MAE) : 14.0488</code></pre>
</div>
<p>Using a grid search for various combinations of the hyperparameters specified, RMSE reduced from 82.9 to 79.07 on training data and that of validation data reduced from 93.09 to 92.11. Overfitting is suggested in the result and could be handled by tuning the regularization parameter.</p>
<h3 id="bagging-as-an-approach-to-improving-model-performance-and-overfitting">Bagging as an approach to improving model performance and overfitting</h3>
<p>In order to further reduce the error, the tuned model is bagged. This approach undertakes multiple random sampling with replacement and for each sample fits HistGradientBoostingRegressor to produce scores which are aggregated. As expected, this helps reduce overfitting as more samples are fitted hence making the model more stable to unseen data.</p>
<p>Bagging is implemented for the HistGradientBoostingRegressor as follows</p>
<div class="layout-chunk" data-layout="l-body">

</div>
<div class="layout-chunk" data-layout="l-body">
<pre><code>BaggingRegressor(base_estimator=HistGradientBoostingRegressor(l2_regularization=0.8,
                                                              max_depth=10,
                                                              min_samples_leaf=10,
                                                              random_state=2022),
                 n_estimators=100, random_state=2022)</code></pre>
</div>
<div class="layout-chunk" data-layout="l-body">
<pre><code>NWMSE for Bagged HistGradientBoosting on training dataset is:  0.114</code></pre>
<pre><code>NWMSE for Bagged HistGradientBoosting on validatation dataset is:  0.329</code></pre>
</div>
<p>The result for bagging of HistGradientBoosting shows further improvement with NWMSE on the validation data reducing to 0.329 compared to 0.338 by the tuned HistGradientBoosting.</p>
<p>In addition, an insight provided is that bagging equally reduced the amount of overfitting on the training dataset given that it decreased the difference in error between validation NWMSE and training NWMSE (0.329 vs.0.114) compared to HistGradientBoosting which produced 0.338 and 0.107 for validation and training. Generally, the lesser the difference in error between the training and validation dataset, the better and the lower the overfitting or underfitting. The goal is to produce a model that scores lower error with minimal difference on the validation and training dataset.</p>
<div class="layout-chunk" data-layout="l-body">
<pre><code>Metrics for bagged HistGradientBoosting on Validation dataset</code></pre>
<pre><code>
Regression statistics

               Mean Error (ME) : -1.0130
Root Mean Squared Error (RMSE) : 90.2835
     Mean Absolute Error (MAE) : 14.0052</code></pre>
</div>
<div class="layout-chunk" data-layout="l-body">
<pre><code>Metrics for bagged HistGradientBoosting on Training dataset</code></pre>
<pre><code>
Regression statistics

               Mean Error (ME) : -0.0685
Root Mean Squared Error (RMSE) : 80.8758
     Mean Absolute Error (MAE) : 13.3553</code></pre>
</div>
<p>Other metrics such as RMSE and MAE show similar trend.</p>
<h3 id="alternative-models-xgbrfregressor">Alternative models  XGBRFRegressor</h3>
<p>XGBRFRegressor is Extreme Gradient Boosting Random Forest regression model and be seen as an advance variation of decision tree model where multiple trees are grown with random sampling of features and data hence Random Forest. This API also inherently handles missing data hence another option to using decision tree models that cater for missing data as undertaken with HistGradientBoostingRegressor.</p>
<p>Its implementation is implemented as below;</p>
<div class="layout-chunk" data-layout="l-body">

</div>
<div class="layout-chunk" data-layout="l-body">
<pre><code>XGBRFRegressor(base_score=0.5, booster=&#39;gbtree&#39;, callbacks=None,
               colsample_bylevel=1, colsample_bytree=1,
               early_stopping_rounds=None, enable_categorical=False,
               eval_metric=None, gamma=0, gpu_id=-1, grow_policy=&#39;depthwise&#39;,
               importance_type=None, interaction_constraints=&#39;&#39;, max_bin=256,
               max_cat_to_onehot=4, max_delta_step=0, max_depth=6, max_leaves=0,
               min_child_weight=1, missing=nan, monotone_constraints=&#39;()&#39;,
               n_estimators=100, n_jobs=0, num_parallel_tree=100,
               objective=&#39;reg:squarederror&#39;, predictor=&#39;auto&#39;,
               random_state=2022, reg_alpha=0, sampling_method=&#39;uniform&#39;,
               scale_pos_weight=1, ...)</code></pre>
</div>
<div class="layout-chunk" data-layout="l-body">
<pre><code>NWMSE for XGBRFRegressor on training dataset is:  0.160</code></pre>
<pre><code>NWMSE for XGBRFRegressor on Validation dataset is:  0.402</code></pre>
</div>
<p>With 0.402 NWMSE on the validation data, XGBRFRegressor performs better than the baseline model but with a higher error compared to HistGradientBoosting. Thus, HistGradientBoosting will be chosen over XGBRFRegressor on the basis of a lower NWMSE on the validation dataset.</p>
<p>It is equally important to evaluate the model with other metrics considered to gain some insight on performance. This is undertaken below;</p>
<div class="layout-chunk" data-layout="l-body">
<pre><code>Metrics for XGBRFRegressor on Training dataset</code></pre>
<pre><code>
Regression statistics

               Mean Error (ME) : -0.0010
Root Mean Squared Error (RMSE) : 95.5276
     Mean Absolute Error (MAE) : 15.1109</code></pre>
</div>
<div class="layout-chunk" data-layout="l-body">
<pre><code>Metrics for XGBRFRegressor on Validation dataset</code></pre>
<pre><code>
Regression statistics

               Mean Error (ME) : -0.9871
Root Mean Squared Error (RMSE) : 99.1090
     Mean Absolute Error (MAE) : 15.2338</code></pre>
</div>
<p>The RMSE of 99.1 on validation dataset for XGBRFRegressor indicates better performance of XGBRFRegressor compared to the baseline model.</p>
<p>The importance of various features in contributing to the model, otherwise known as predictive power of the variables can be estimated for the model below.</p>
<div class="layout-chunk" data-layout="l-body">
<pre><code>           Predictors  importance
5             city_id    0.054296
9       content_score    0.055762
4            n_images    0.065593
7  distance_to_center    0.071565
1          avg_rating    0.080671
2               stars    0.090017
8           n_reviews    0.092771
3            avg_rank    0.138946
6           avg_price    0.152653
0  avg_saving_percent    0.197725</code></pre>
</div>
<h4 id="visualize-feature-importance">Visualize feature importance</h4>
<div class="layout-chunk" data-layout="l-body">
<div>                        <script type="text/javascript">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>
        <script src="https://cdn.plot.ly/plotly-2.14.0.min.js"></script>                <div id="d11f91ea-645b-4df7-908f-d1ce73c05701" class="plotly-graph-div" style="height:100%; width:100%;"></div>            <script type="text/javascript">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById("d11f91ea-645b-4df7-908f-d1ce73c05701")) {                    Plotly.newPlot(                        "d11f91ea-645b-4df7-908f-d1ce73c05701",                        [{"alignmentgroup":"True","hovertemplate":"Predictors=%{y}<br>importance=%{x}<extra></extra>","legendgroup":"city_id","marker":{"color":"#636efa","pattern":{"shape":""}},"name":"city_id","offsetgroup":"city_id","orientation":"h","showlegend":true,"textposition":"auto","x":[0.05429615452885628],"xaxis":"x","y":["city_id"],"yaxis":"y","type":"bar"},{"alignmentgroup":"True","hovertemplate":"Predictors=%{y}<br>importance=%{x}<extra></extra>","legendgroup":"content_score","marker":{"color":"#EF553B","pattern":{"shape":""}},"name":"content_score","offsetgroup":"content_score","orientation":"h","showlegend":true,"textposition":"auto","x":[0.055762000381946564],"xaxis":"x","y":["content_score"],"yaxis":"y","type":"bar"},{"alignmentgroup":"True","hovertemplate":"Predictors=%{y}<br>importance=%{x}<extra></extra>","legendgroup":"n_images","marker":{"color":"#00cc96","pattern":{"shape":""}},"name":"n_images","offsetgroup":"n_images","orientation":"h","showlegend":true,"textposition":"auto","x":[0.06559327989816666],"xaxis":"x","y":["n_images"],"yaxis":"y","type":"bar"},{"alignmentgroup":"True","hovertemplate":"Predictors=%{y}<br>importance=%{x}<extra></extra>","legendgroup":"distance_to_center","marker":{"color":"#ab63fa","pattern":{"shape":""}},"name":"distance_to_center","offsetgroup":"distance_to_center","orientation":"h","showlegend":true,"textposition":"auto","x":[0.07156529277563095],"xaxis":"x","y":["distance_to_center"],"yaxis":"y","type":"bar"},{"alignmentgroup":"True","hovertemplate":"Predictors=%{y}<br>importance=%{x}<extra></extra>","legendgroup":"avg_rating","marker":{"color":"#FFA15A","pattern":{"shape":""}},"name":"avg_rating","offsetgroup":"avg_rating","orientation":"h","showlegend":true,"textposition":"auto","x":[0.08067073673009872],"xaxis":"x","y":["avg_rating"],"yaxis":"y","type":"bar"},{"alignmentgroup":"True","hovertemplate":"Predictors=%{y}<br>importance=%{x}<extra></extra>","legendgroup":"stars","marker":{"color":"#19d3f3","pattern":{"shape":""}},"name":"stars","offsetgroup":"stars","orientation":"h","showlegend":true,"textposition":"auto","x":[0.09001738578081131],"xaxis":"x","y":["stars"],"yaxis":"y","type":"bar"},{"alignmentgroup":"True","hovertemplate":"Predictors=%{y}<br>importance=%{x}<extra></extra>","legendgroup":"n_reviews","marker":{"color":"#FF6692","pattern":{"shape":""}},"name":"n_reviews","offsetgroup":"n_reviews","orientation":"h","showlegend":true,"textposition":"auto","x":[0.09277134388685226],"xaxis":"x","y":["n_reviews"],"yaxis":"y","type":"bar"},{"alignmentgroup":"True","hovertemplate":"Predictors=%{y}<br>importance=%{x}<extra></extra>","legendgroup":"avg_rank","marker":{"color":"#B6E880","pattern":{"shape":""}},"name":"avg_rank","offsetgroup":"avg_rank","orientation":"h","showlegend":true,"textposition":"auto","x":[0.13894571363925934],"xaxis":"x","y":["avg_rank"],"yaxis":"y","type":"bar"},{"alignmentgroup":"True","hovertemplate":"Predictors=%{y}<br>importance=%{x}<extra></extra>","legendgroup":"avg_price","marker":{"color":"#FF97FF","pattern":{"shape":""}},"name":"avg_price","offsetgroup":"avg_price","orientation":"h","showlegend":true,"textposition":"auto","x":[0.1526525914669037],"xaxis":"x","y":["avg_price"],"yaxis":"y","type":"bar"},{"alignmentgroup":"True","hovertemplate":"Predictors=%{y}<br>importance=%{x}<extra></extra>","legendgroup":"avg_saving_percent","marker":{"color":"#FECB52","pattern":{"shape":""}},"name":"avg_saving_percent","offsetgroup":"avg_saving_percent","orientation":"h","showlegend":true,"textposition":"auto","x":[0.19772541522979736],"xaxis":"x","y":["avg_saving_percent"],"yaxis":"y","type":"bar"}],                        {"template":{"data":{"barpolar":[{"marker":{"line":{"color":"rgb(17,17,17)","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"bar":[{"error_x":{"color":"#f2f5fa"},"error_y":{"color":"#f2f5fa"},"marker":{"line":{"color":"rgb(17,17,17)","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"carpet":[{"aaxis":{"endlinecolor":"#A2B1C6","gridcolor":"#506784","linecolor":"#506784","minorgridcolor":"#506784","startlinecolor":"#A2B1C6"},"baxis":{"endlinecolor":"#A2B1C6","gridcolor":"#506784","linecolor":"#506784","minorgridcolor":"#506784","startlinecolor":"#A2B1C6"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"contour"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"heatmapgl"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"heatmap"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"histogram2dcontour"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"histogram2d"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"line":{"color":"#283442"}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatter":[{"marker":{"line":{"color":"#283442"}},"type":"scatter"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#506784"},"line":{"color":"rgb(17,17,17)"}},"header":{"fill":{"color":"#2a3f5f"},"line":{"color":"rgb(17,17,17)"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#f2f5fa","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#f2f5fa"},"geo":{"bgcolor":"rgb(17,17,17)","lakecolor":"rgb(17,17,17)","landcolor":"rgb(17,17,17)","showlakes":true,"showland":true,"subunitcolor":"#506784"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"dark"},"paper_bgcolor":"rgb(17,17,17)","plot_bgcolor":"rgb(17,17,17)","polar":{"angularaxis":{"gridcolor":"#506784","linecolor":"#506784","ticks":""},"bgcolor":"rgb(17,17,17)","radialaxis":{"gridcolor":"#506784","linecolor":"#506784","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"rgb(17,17,17)","gridcolor":"#506784","gridwidth":2,"linecolor":"#506784","showbackground":true,"ticks":"","zerolinecolor":"#C8D4E3"},"yaxis":{"backgroundcolor":"rgb(17,17,17)","gridcolor":"#506784","gridwidth":2,"linecolor":"#506784","showbackground":true,"ticks":"","zerolinecolor":"#C8D4E3"},"zaxis":{"backgroundcolor":"rgb(17,17,17)","gridcolor":"#506784","gridwidth":2,"linecolor":"#506784","showbackground":true,"ticks":"","zerolinecolor":"#C8D4E3"}},"shapedefaults":{"line":{"color":"#f2f5fa"}},"sliderdefaults":{"bgcolor":"#C8D4E3","bordercolor":"rgb(17,17,17)","borderwidth":1,"tickwidth":0},"ternary":{"aaxis":{"gridcolor":"#506784","linecolor":"#506784","ticks":""},"baxis":{"gridcolor":"#506784","linecolor":"#506784","ticks":""},"bgcolor":"rgb(17,17,17)","caxis":{"gridcolor":"#506784","linecolor":"#506784","ticks":""}},"title":{"x":0.05},"updatemenudefaults":{"bgcolor":"#506784","borderwidth":0},"xaxis":{"automargin":true,"gridcolor":"#283442","linecolor":"#506784","ticks":"","title":{"standoff":15},"zerolinecolor":"#283442","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"#283442","linecolor":"#506784","ticks":"","title":{"standoff":15},"zerolinecolor":"#283442","zerolinewidth":2}}},"xaxis":{"anchor":"y","domain":[0.0,1.0],"title":{"text":"importance"}},"yaxis":{"anchor":"x","domain":[0.0,1.0],"title":{"text":"Predictors"},"categoryorder":"array","categoryarray":["avg_saving_percent","avg_price","avg_rank","n_reviews","stars","avg_rating","distance_to_center","n_images","content_score","city_id"]},"legend":{"title":{"text":"Predictors"},"tracegroupgap":0},"title":{"text":"Feature Importance in XGBRFRegressor"},"barmode":"relative"},                        {"responsive": true}                    )                };                            </script>        </div>
</div>
<p>From the visualization above, avg_saving_percent has the most predictive power in XGBRFRegressor model fitted on the data</p>
<h3 id="potential-measures-for-to-improve-the-model">Potential measures for to improve the model</h3>
<p>Modelling can be a very iterative exercise with time and resource such as computational power constraints and this is duly recognized here. It is therefore concluded that, there is a good chance the model developed here can be further improved with those available resources using appropriate measures. Instead of aimimng to exhaust all implementation to produce the lowest error possible for a model; some of the measures to improve the model are highlighted here as further course of action.</p>
<ol type="1">
<li><p>Feature selection and engineering: Feature selection could be explored when using some other models to select only the most influencial features and probably reduce overfitting. Some categorical features could also be transformed by encoding if computation power allows for the increase in data dimensionality to explore the potential of improving the model. One-Hot encoding was ruled-out for city_id due to its high cardinality. While only normalization or scaling is less likely to significantly improve performance of decision tree based models explored here, the technique should be explore for other type of models. Closely related is the option of augmenting the data with new variables. Potential variables and data to use or collect include availability of special services at hotels like excursions and guided tours, cultural performace among others. This could be an influential variable the appeal to users to click and book. This could be a categorical variable or all special serivces use to rate a hotel as an ordinal variable.</p></li>
<li><p>Experiment with different techniques of handling missing data to discover which approach reliably produce a stable and optimized model. This approach could be iterative but a very interesting domain to put resource to establish domain relevant method of handling missing data since it appears to be a challenge for the data.</p></li>
<li><p>Tuning hyperparameters: The models developed can further be improved by experimenting with hyperparameters to select combinations that best reduces error.</p></li>
</ol>
<h2 id="making-prediction-on-test-data">Making prediction on test data</h2>
<p>To make predictions on the test data, the bagged HistGradienBoostingRegressor model is used. Generally, the prediction made were float values some of which do not reconceal with reality in terms of the unit of measuring the target variable which is count of clicks. For instance, a prediction of 8.387 in reality deviates from logic as there can only be clicks numbering whole numbers and not decimals. Thus, the prediction were round-off to nearest whole numbers which for this example will be 8. It should be noted that, this action in itself could introduce some margin of error.</p>
<div class="layout-chunk" data-layout="l-body">
<pre><code>       hotel_id   city_id  ...  avg_price  avg_saving_percent
0   14942256073  122750.0  ...      90.19                32.0
1   16036037903   28134.0  ...      98.27                19.0
2  288585940112   30578.0  ...      48.77                 0.0
3  129041645070   54398.0  ...      72.32                 0.0
4   12460296563   63890.0  ...      24.54                19.0

[5 rows x 11 columns]</code></pre>
</div>
<div class="layout-chunk" data-layout="l-body">
<pre><code>            hotel_id   city_id  ...  avg_saving_percent  predictions_made
0        14942256073  122750.0  ...                32.0          8.387677
1        16036037903   28134.0  ...                19.0         25.187326
2       288585940112   30578.0  ...                 0.0          0.142417
3       129041645070   54398.0  ...                 0.0          1.738487
4        12460296563   63890.0  ...                19.0          7.663731
...              ...       ...  ...                 ...               ...
132157  146542092044   22896.0  ...                 0.0         10.893020
132158  191459858176   28030.0  ...                 4.0          6.136969
132159   71241988826  137926.0  ...                19.0          2.265503
132160  204878950620   54574.0  ...                 0.0          0.580113
132161   99788791152  774630.0  ...                 0.0          2.434706

[132162 rows x 12 columns]</code></pre>
</div>
<div class="layout-chunk" data-layout="l-body">
<pre><code>            hotel_id   city_id  ...  predictions_made  n_clicks
0        14942256073  122750.0  ...          8.387677       8.0
1        16036037903   28134.0  ...         25.187326      25.0
2       288585940112   30578.0  ...          0.142417       0.0
3       129041645070   54398.0  ...          1.738487       2.0
4        12460296563   63890.0  ...          7.663731       8.0
...              ...       ...  ...               ...       ...
132157  146542092044   22896.0  ...         10.893020      11.0
132158  191459858176   28030.0  ...          6.136969       6.0
132159   71241988826  137926.0  ...          2.265503       2.0
132160  204878950620   54574.0  ...          0.580113       1.0
132161   99788791152  774630.0  ...          2.434706       2.0

[132162 rows x 13 columns]</code></pre>
</div>
<div class="layout-chunk" data-layout="l-body">

</div>
<pre><code>






















































```{.r .distill-force-highlighting-css}</code></pre>
<!--radix_placeholder_article_footer-->
<!--/radix_placeholder_article_footer-->
</div>

<div class="d-appendix">
</div>


<!--radix_placeholder_site_after_body-->
<!--/radix_placeholder_site_after_body-->
<!--radix_placeholder_appendices-->
<div class="appendix-bottom"></div>
<!--/radix_placeholder_appendices-->
<!--radix_placeholder_navigation_after_body-->
<!--/radix_placeholder_navigation_after_body-->

</body>

</html>
