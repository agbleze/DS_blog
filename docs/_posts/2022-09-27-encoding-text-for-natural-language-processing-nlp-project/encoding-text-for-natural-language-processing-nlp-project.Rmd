---
title: "Encoding text for Natural Language Processing (NLP) project"
description: |
  A short description of the post.
author:
  - name: Linus Agbleze
    url: https://agbleze.github.io/Portfolio/
date: 2022-09-27
output:
  distill::distill_article:
    self_contained: false
draft: false
categories:
  - NLP 
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE)
```



```{r}
library(reticulate)
use_virtualenv('/Users/lin/Documents/python_venvs/pytorch_deepL', required=TRUE)


```
Natural Language Processing involves techniques used to transform text data to be understandable to computers. Generally, computers are better at understanding quantitative data than qualitative data and because data science tools and methods mainly produce mathematical models to explain the general relation between inputs and output. Restricting ourselves to only quantitative data would means losing out on the vast majority of text and unstructured data.

In order to employ data science tools to gain insights from qualitative data (text), the most intuitive method that comes to mind is to find a way to represent textual data in a numeric form for computers to understand. Thus, encoding text using numeric representation forms an integral part of almost all NLP projects and this post touches on some of the methods used to encode textual data.


** Objective **
-- To discuss and demonstrate basic text encoding methods


** One-hot encoding method **
One-hot representation is a simple technique to representing text for machine learning algorithms to process by highlighting the presence and absence of words. In NLP, when a sentence is broken down into individual words, each unit word is termed a token.


```{python, echo=TRUE}
from sklearn.feature_extraction.text import CountVectorizer
import seaborn as sns
from sklearn.feature_extraction.text import TfidfVectorizer


```







