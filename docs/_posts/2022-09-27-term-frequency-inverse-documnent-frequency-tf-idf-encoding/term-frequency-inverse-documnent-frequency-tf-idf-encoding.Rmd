---
title: "Term-Frequency Inverse-Documnent-Frequency (TF IDF): Encoding text for Natural Language Processing (NLP) project -Part II"
description: |
  A short description of the post.
author:
  - name: Linus Agbleze
    url: https://agbleze.github.io/Portfolio/
date: 2022-09-27
output:
  distill::distill_article:
    self_contained: false
draft: false
categories:
  - NLP 
  - Python
---

```{r}
library(reticulate)
use_virtualenv('/Users/lin/Documents/python_venvs/pytorch_deepL', required=TRUE)

```


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE,
                      fig.height = 12, fig.width = 12,
                      tidy = 'styler', fig.keep = 'all', 
                      fig.show = 'asis', fig.align = 'center'
                      )
```

# Introduction to TF-IDF
TF-IDF is the acronym for Term Frequency Inverse Document Frequency and is one of the techniques used to represent text data. In the previous discussion of representing text using 




 The TF representation weights words proportionally to their frequency. However, common words such as “claim” do not add anything to our understanding of a specific patent. Conversely, if a rare word (such as “tetrafluoroethylene”) occurs less frequently but is quite likely to be indicative of the nature of the patent document, we would want to give it a larger weight in our representation. The Inverse­ Document­Frequency (IDF) is a heuristic to do exactly that.







