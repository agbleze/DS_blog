---
title: "Term-Frequency Inverse-Documnent-Frequency (TF IDF): Encoding text for Natural Language Processing (NLP) project -Part II"
description: |
  A short description of the post.
author:
  - name: Linus Agbleze
    url: https://agbleze.github.io/Portfolio/
date: 2022-09-27
output:
  distill::distill_article:
    self_contained: false
draft: false
categories:
  - NLP 
  - Python
---

```{r, echo=FALSE}
library(reticulate)
use_virtualenv('/Users/lin/Documents/python_venvs/pytorch_deepL', required=TRUE)

```


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE,
                      fig.height = 12, fig.width = 12,
                      tidy = 'styler', fig.keep = 'all', 
                      fig.show = 'asis', fig.align = 'center'
                      )
```

# Introduction to TF-IDF
TF-IDF is the acronym for Term Frequency Inverse Document Frequency and is one of the techniques used to represent text data. In the previous discussion of representing text using one hot encoding, a major critic is that one hot encoding does not take into account the frequency of words. This is one weakness that TF-IDF reverses. 

TF-IDF takes into consideration the frequency of a word or token as well as the number of documents in which it occurs. Usually, less important words such as helping verbs, prepositions among others have the highest frequency in documents. These words regarded as stop words have less predictive power. 


Imagine a corpus like "This is my first lesson. This lesson is interesting. The lesson is on logistic regression."  If our task is to classify or identify the topic that the document or text describe then certainly, words such as "This", "is", "lesson" which had higher frequency will provide less insight for classifying the text while less frequent tokens such as "logistic" and "regression" will provide more predictive insights in classifying the text. This understanding is what underline TF-IDF in representing text. 


Thus, the formula for TF-IDF is which is a combination of TF and IDF is express as follows;

TF = Number of times a word occurs in a document. A token with a higher frequency receives higher weight.

IDF = $log\frac{N}{n_w}$

IDF indicates the log of the ratio of total number of documents (N) to the number of documents the word has appeared in ($n_w$). The higher the number of documents that a word appears in, the lower the weight it receives. Thus, IDF penalizes higher frequency words or token. TF-IDF is a product of TF AND IDF where a token that frequently appears in a document is highly weighted for TF and when when that token occurs in many documents than it receives lower weight for IDF.


# Implenetation of TF-IDF in python



