[
  {
    "path": "posts/2022-09-03-product-analysis-report/",
    "title": "Product Analysis report",
    "description": "A short description of the post.",
    "author": [
      {
        "name": "Linus Agbleze",
        "url": "https://agbleze.github.io/Portfolio/"
      }
    ],
    "date": "2022-09-03",
    "categories": [],
    "contents": "\nBackground\nThe company is a startup offering an innovative product that is transforming commercial real estate monitoring and management. Its target market is the commercial real estate industry. In order to stay in business, it is not enough to access the market by landing demo request but more essentially turn those requests into conversions and increase conversion rate. Thus, the main goal of the product team is started as follows;\nProduct team goal for analysis\nTo increase conversion from demo request to contract signing\nBusiness Problem statement\nProblem statement 1: Assessing the relationship between sales script type and conversion\nThe product team aims at undertaking a data-based decision making to achieve its goal. To achieve that, data has been collected to be analyzed to gain insights. The challenge is to investigate how various variables influence our goal by asking the right business questions.\nWe have been using different scripts during the demo sessions and as expected not all demo sessions result in conversion. We will want to understand if there is any trend and factor contributing to conversion that needs to be maximized and manage any other factor detrimental to conversion. This is one of our business problems to be analyzed. Given the fact that sales script was one of the tools used during the demo session, we are tempted to hypothesize it is associated with conversion. This understanding have been translated into the following analytical framework for further assessment.\nNull Hypothesis (H0): There is no relationship between type of sales script and conversion\nAlternative Hypothesis (H1): There is a relationship between type of sales script and conversion\nProblem statement 2: Assessing difference in total conversions made by different sales scripts\nEach of the sales script used during the demo session has produce some conversion. While the total conversion for each script can be easily assessed, we need to go beyond that and assess if there is any difference and more importantly if the difference in conversion is significant to suggest a preference for one of them to increase the chances of achieving higher conversion. This need not be a guesswork but data driven hence have been translated into the following hypothesis for analysis;\nNull hypothesis (H0): Difference in mean conversion among all sales scripts is equal\nAlternative hypothesis (H1): There is difference in mean conversion for at least one of sales scripts\nProblem statement 3: To sales scripts that produces significantly higher conversions\nThe result for the hypothesis stated above will determine whether or not there is the need for further enquiry in the form of post-hoc test. If the analysis results in rejection of the null hypothesis, then there will be the need to assess how the different sales scripts compare to each other in order to identify which of the sales scripts produce significantly higher mean conversion.\nProblem statement 4: Assessing factors that influence the decision of a client to convert\nFor our success, the higher the amount of conversions, the better. The business problem at hand is to understand the drivers of conversion. The decision by a client to purchase our product or not during or after a demo session is one that needs to be better understood so we can do more of what makes them convert and less of what deter them from buying our products. This problem requires a deep dive into our data to carefully assess how the various factors for which data is available are playing a role.\nBusiness questions to answer\nBased on the the business problem statements conceptualized, this analysis is undertaken to provide answers to the following business questions as a guide.\nIs there a significant relationship between type of sales script and conversion?\nAre all sales scripts achieving a similar amount of conversions?\nHow do sales scripts compare in terms of conversions and which sales scripts can be used to achiever higher conversions?\nHow do various factors influence the probability of a client to sign-up for our product?\nBusiness objectives\nTo define the data analysis tasks, the business questions are translated into objectives as follows\nTo test the hypothesis that there is a statistically significant relationship between type of sales script and conversion\nTo test the hypothesis that difference in mean conversion among sales script are equal\nTo assess and identify which sales script produces higher total conversions on the average if any\nTo assess various potential drivers of conversion and understand their influence\nData analysis task\nThis section details the procedure used to analyzed the data to derive insights and draw recommendations. First of all it is important to highlight some of the formula used.\nFormula used\nConversion rate (ะก1) = [N of contracts signed] / [N of Demo requests] * 100\nConversion rate for script = (Total conversions where script was used) / (Total number of demo sessions where script was used)\nAssumptions\nIn order to make results reproducible and understandable for contextualized interpretation, much effort is made to lay bare assumptions that may influence how proper insights is drawn. Some of these assumptions are required for the statistical analysis undertaken to hold true. These are highlighted in this section as follows.\nScripts were used independently, that is a single script was applied for a client.\nAll scripts were used for approximately the same time period. It is however realized from the data that the first demo appointment session for which script C was used was on 2020-01-08 16:50:35 and the last date of use was 2021-03-01 19:40:01. For script A, it was first used for a demo session on 2019-12-28 03:57:38 and its last usage was on 2021-03-29 12:44:07. Script B was first used for a demo session on 2019-12-28 11:38:55 and last used was on 2021-03-06 16:57:21. Thus, data exploration shows some differences in date of first usage and last usage of script but this difference is assummed to be negligible in order to compare the mean conversions among them.\nData exploration\n\n\n\n\nfunction (...) \n{\n    dots \n\n## Identify the categories of sales scripts\ndf.sales_script_variant.unique()\nfunction (...) \n{\n    dots \n\n## get description of the variables in the data\n\ndf.info()\nfunction (...) \n{\n    dots \n\n## shows the shape of the data -- number of rows and columns\ndf.shape\nfunction (...) \n{\n    dots \n\n## Cast the data into DplyFrame in order to use dplython functions on it.\n\ndf_dataframe = DplyFrame(df)\nfunction (...) \n{\n    dots \nEstimating total number of conversions for the whole period covered\nKeywords\nNon-conersion are cases where the client did not sign up during and after the demo session\nGiven that our goal is to increase conversion, the analysis will centered around that. First, the total number of conversion is estimated and compared to non-conversion\nFrom the analysis below, total number of conversion was 5,018 which is slightly higher than non-conversion of 4,982. The sum of both conversion and non-conversion equates to the total number of requests made.\n\n## select some columns needed to estimating conversion\n#(df_dplyselect = df_dataframe >> siuba.select(_.is_signed, _.conversion, _.sales_script_type, #_.request_to_1streach_timelength_minutes_, _.company_group))\n \ndf_dplyselect = df_dataframe[['is_signed', 'conversion', 'sales_script_type', 'request_to_1streach_timelength_minutes_', 'company_group']]    \n## Group data based on conversion column and count number of conversion\n#conversion_total = df_dplyselect >> group_by(X.conversion) >> summarize(total_conversion = #X.conversion.count())\nfunction (...) \n{\n    dots df_dplyselect = df_dplyselect.groupby('conversion')[['conversion']].count()#.reset_index()\nfunction (...) \n{\n    dots df_dplyselect\nfunction (...) \n{\n    dots conversion_total = df_dplyselect.copy().rename(columns={'conversion': 'total_conversion'}).reset_index()\nfunction (...) \n{\n    dots conversion_total\n#df_dplyselect\nfunction (...) \n{\n    dots \n\n(ggplot(conversion_total, aes(x='conversion', y='total_conversion'))\n + geom_col(stat = 'identity') + ggtitle('Bar chart of Total Conversion and non-conversion') + theme_light()\n)\n\nfunction (...) \n{\n    dots \nEstimating C1\nAfter estimating conversion and non-conversion, C1 needs to be estimated. This can be considered as one of the major KPI for the period.\nC1 formula\nะก1 = [N of contracts signed] / [N of Demo requests]\n\n## The [N of Demo requests] is estimated below\n\nrequest_total = conversion_total['total_conversion'].sum()\nfunction (...) \n{\n    dots request_total\nfunction (...) \n{\n    dots \n\n## C1 \nC1 = conversion_total.iloc[[0],[1]] / request_total\nfunction (...) \n{\n    dots C1\nfunction (...) \n{\n    dots C1.rename(index=str, columns={\"total_conversion\": \"C1\"})\nfunction (...) \n{\n    dots \nThus, C1 is estimated to be 0.5018.\nIt is concluded that conversion rate in percentage terms if 50.2%\nDisaagregating conversion for indepth analysis\nIn order to better understand conversion, there is the need to explore the data based on certain dimensions.\n\n### group data based on company type and count total conversion for #eash \ncompany_conversion = df_dataframe >> siuba.group_by(_.company_group, _.conversion) >> siuba.summarize(total_count = _.conversion.count())\n\n#df_dplyselect.groupby(['company_group', #'conversion'])['conversion'].count()\nfunction (...) \n{\n    dots company_conversion\nfunction (...) \n{\n    dots \n\n(ggplot(company_conversion, aes(x='company_group', y='total_count', fill='conversion'))\n + geom_col(stat='identity', position='dodge')) + theme_dark() + ggtitle('Conversion based on type of company')\n\nfunction (...) \n{\n    dots \nFrom our research questions and hypothesis, conversion based on sales script type is will offer valuable insight hence the data is grouped and conversion is estimated and visualized for the different sales group before testing their hypothesis further.\n\n## group data based on type of sales script and count total conversion \nscript_conversion = (df_dataframe\n >> siuba.group_by(_.sales_script_type, _.conversion)\n>> siuba.summarize(total_conversion = _.conversion.count())\n)\nfunction (...) \n{\n    dots script_conversion\nfunction (...) \n{\n    dots \n\nThe result of the table can be visualize below\n\n### Plot the total conversion based on script type\n\n(ggplot(script_conversion, aes(x='sales_script_type', y='total_conversion', fill='conversion'))\n + geom_col(stat='identity', position='dodge')) + theme_dark() + ggtitle('Conversion based on type of sales script')\n\nfunction (...) \n{\n    dots \nInsights from the visuals\nIllustrating conversion based on script type shows that both script A and script C made a higher conversion compared to non-conversion. Script A made 83 more conversions than non-conversion while script C made 7 more conversion than non-conversion. On the contrary, script B made 54 less conversions compared to non-conversion.\nWhile this difference gives a clue about performance of the various script, it does not enables us to make decisive conclusion but guesses of what the difference could result in. Foreinstance, the total conversion is summation and hence the difference could be the result of number of demo sessions that a script has been used for. Clarity needs to brought to such guessetimates by considering their mean through further analysis\nIn order to make a data driven decision, hypothesis need to be tested to derive better understanding base on statistical significance.\nObjective 1\nTo test the hypothesis that there is statistically significant relationship between type of sales script and conversion.\nProceeding from the insights gained, this section tests the hypothesis for the first objective\nH0 Conversion is independent of type of sales script used\nH1 Conversion is dependent on type of sales script used\nMethod of analysis\nChi squared test of independence is appropriate for assessing whether there is a relationship between two categorical variable hence used. The procedure involves producing a contigency table and using that for the analysis. This is demonstrated below.\nContigency table\n\n\n## contigency table between sales script type and conversion\n\nconversion_script_type_contengency = pd.crosstab(df_dataframe['sales_script_type'], df_dataframe['conversion'])\nfunction (...) \n{\n    dots conversion_script_type_contengency\nfunction (...) \n{\n    dots \n\n### chi-square test of independence\nchi_square_result = stat()\nfunction (...) \n{\n    dots chi_square_result.chisq(df=conversion_script_type_contengency)\nfunction (...) \n{\n    dots print(chi_square_result.summary)\nfunction (...) \n{\n    dots \nResults for objective one\nWith a p-value of 0.3279 being greater than 0.05 (5% significance level), it is suggested that there is no statistically significant relationship between type of sales script and conversion. Thus, we fail to reject the null hypothesis of independence based on available evidence.\nThis result has an added twist to it, which is that, there is the possibility of a change in result when more and better data is acquired.\nA major research gap that remains is whether the differences in conversion between scripts as deduced from the bar plots is significant. This requires the need to test another hypothesis hence research objective 2.\nResearch objective 2: To test the hypothesis that difference in mean conversion among sales script are equal\nIn order to test this hypothesis, there is the need to have continuous varaible. For this, the total number of conversions on daily basis can analyzed and used as a continuous variable. The rational for estimating daily total conversions instead of monthly or yearly is to ensure that there are enough data points to make statistical inference.\nThe available data covers only a year and a couple of days, hence daily conversions is a logical timeframe for estimating total conversion.\nBy this, the dataset needs to be grouped based on days and the sum of conversions estimated for each sales script type. The question that arise is how are the conversions to be counted? For this, โis_signedโ variable is used; where its boolean data type (true or false) are treated as intergers with 1 counted as a single conversion and 0 as no conversion for each demo session. This allows for the total daily conversion to be estimated.\nAnother question is, which of the dates should be used as a basis for counting the total daily conversion? For this, the demo_appointment_datetime variable was used. This is based on the context that โduring and after the demo session, the sales manager tries to convert the potential client into a contract signingโ hence the assumption that the impact of the script used on conversion came into effect on the demo appointment day.\nIt is assummed that the date used will be of less relevance for testing the hypothesis despite there could changes in the total number of daily conversion for scripts when the date is change. But the impact is assummed to be negligible and again, less relevant to some extent.\nKey assumption\nThe assumption was made that all sales scripts had equal chance of being used during a demo session.\nThere were no cases of repeated sessions where scripts were used again.\n\n### select various columns and split the demo appointment column into date and time\ndf_grp = (df >> siuba.select(_.demo_appointment_datetime, _.request_to_1streach_timelength_minutes_, \n                  _.conversion, _.company_group, _.sales_script_type, _.is_signed)>>\nsiuba.separate(col = _.demo_appointment_datetime, sep = ' ', into = ('demo_appointment_yyyymmdd', \n                                                                     'demo_appointment_hhmmss')) >>\n siuba.group_by(_.demo_appointment_yyyymmdd, _.sales_script_type)\n)\n\n## sum the total conversion for each script on each day\nfunction (...) \n{\n    dots df_grp_sum = df_grp >> siuba.summarize(total_conversion = _.is_signed.sum())\nfunction (...) \n{\n    dots df_grp_sum.head(5)\nfunction (...) \n{\n    dots \nVisualizing the total conversion on demo day for the various scripts\nFirst, the distribution of total conversion based on sales script is visualized using boxplot which shows the mean conversion for sales scripts as well as the minimum, 25th quartile, 75th quartile and maximum.\n\n\n(ggplot(data = df_grp_sum, mapping = aes(x = 'sales_script_type', y = 'total_conversion'))+ geom_boxplot() + \n ggtitle('Total conversion by script type (based on conversion counts of demo day)')) \n\nfunction (...) \n{\n    dots \nInsights from boxplot of total conversions based on demo date\nFrom the boxplot, some points which are outside the normal range of the plot can be regarded as outliers. However, they are not remove for now. Moreover the difference in conversion between scripts is clearer with the boxplot. It is deduced that script C has the lowest average conversion while script A has the highest.\nVisual inspection: Testing statistical assumptions for the analysis of hypothesis\nBefore deciding on the statistical method to use to answer the business questions, there is the need to verify that statistical assumptions hold true for the data. For valid statistical inference to be made about our target market based on our current of clients it is very important that appropriate methods are used. The appropriateness of the method is not a guesswork but one informed from both visual and statistical test.\nGenerally, there are two family of statistical techniques that can be employed in analyzing the data. Namely, Parametric and non-parametric methods. The parametric methods usually have a greater statistical power but this comes at the cost of requiring the data to meet a number of assumptions for proper insights to be gain. Given that, our aim is to present the possible best solution, the assumptions for using a parametric method are tested first before considering their use or otherwise.\nVisual inspection of normality\nTo use a parametric method, normality of data distribution of is assummed. A simple approach to verifying this is with the aid of histogram\n\nsns.histplot(df_grp_sum['total_conversion'], bins=20, kde=True, color='g')\nfunction (...) \n{\n    dots \nResults of the histogram\nThe histogram plotted above does not necessarily answer the question on normality assumption. Nonetheless, it is one of several visualization techniques that gives clues.\nFrom visual inspection, the distribution is right-tailed hence a right-skewed or positive-skewed distribution.\nQ-Q plot for visualizing normality\nIn order to gain a better view on the normality of the distribution, a Q-Q plot is use to compare actual total conversion to an expected value in a normal distribution. The yardstick for detecting normality will be to verify that the actual data distriubtion are linearly along a straignt 45 degrees diagonal line. The result is illustrated below.\n\n### QQ-PLOT\nsm.qqplot(df_grp_sum['total_conversion'], line = '45')\nfunction (...) \n{\n    dots plt.title('Q-Q plot for total conversion')\nfunction (...) \n{\n    dots plt.show()\n\nfunction (...) \n{\n    dots \nInsights from Q-Q plot\nFrom the Q-Q plot, it is deduced that the dataset deviates from the line of expected normal distribution hence heavily skewed. While the visualization so far points to the direction of a non-normal distribution, there is still the opportunity to cross-check these suggestions with some statistically test for normality.\nStatistically test for normality: Shapiro-Wilk test of normality\nThe visual inspections are supported with Shapiro-Wilk test to test the hypothesis that the distribution of data is not different from an expected normal distribution.\n\n#### Shapiro-Wilk test\n\nw, pvalue = stats.shapiro(df_grp_sum['total_conversion'])\nfunction (...) \n{\n    dots print(w, pvalue)\n\n\n#w, pvalue = stats.shapiro(script_anova.anova_model_out.resid)\n#print(w, pvalue)\nfunction (...) \n{\n    dots \nCalculating normality\nThis another approach is to assessing normality using the pingouin module.\n\npg.normality(df_grp_sum, group='sales_script_type', dv='total_conversion')\nfunction (...) \n{\n    dots \nInsight from Shapiro-Wilk test of normality\nThe p-value of the test is less than 0.05 (5% significance level) which suggests a statistically significant difference from a normal distribution. Thus, the null hypothesis is rejected. This is not one of the relevant hypothesis to be tested for our products but its gives us an important clue as to the right statistical method to adopt for assessing our business relevant hypothesis.\nWith the data failing to meet the assumption of normal distribution required for the adoption of a parametric method, the compass of the analysis is gearing towards a non-parametric method. But before ascertaining that, there is the need to test for other assumptions required such as homogeneity which stipulates that variance between categories to analyzed should be equal across the data distribution for methods such ANOVA to be properly use.\nStatistical test for homogeneity\nBartlettโs test\nThe first approach being used to test for equal variance in the distribution is the Bartlettโs test. Given that the distribution is non-normal, Barlettโs test is supported with Leveneโs test which is a much more robust test when the data is not a normal distribution.\n\n### bartlett's test for homogeneity\nscript_bartlett = stat()\nfunction (...) \n{\n    dots script_bartlett.bartlett(df = df_grp_sum, res_var = 'total_conversion', xfac_var = 'sales_script_type')\nfunction (...) \n{\n    dots script_bartlett.bartlett_summary\nfunction (...) \n{\n    dots \nInsight from the Bartlettโs test\nThe bartlett test result shows a p-value that is statistically significant hence the hypothesis that variation in the distribution is equal is rejected. This points in the direction of a non-parametric approach for the analysis but before then the result needs to be verified with a Leveneโs test.\nLeveneโs statistical test for homogeneity\nLeveneโs test is employed as a final approach in this context to verify whether not the variance in the data is equal.\n\n## levene test\nscript_lev = stat()\nfunction (...) \n{\n    dots script_lev.levene(df = df_grp_sum, res_var='total_conversion', xfac_var = 'sales_script_type')\nfunction (...) \n{\n    dots script_lev.levene_summary\nfunction (...) \n{\n    dots \n\n### Usingthe pingouin module for the analysis\n\npg.homoscedasticity(df_grp_sum, group='sales_script_type', dv='total_conversion')\nfunction (...) \n{\n    dots \nInsight from Leveneโs test\nThe leveneโs test depict an unequal variance with a statistical significant p-value < 0.05 (at 5% significant level) hence confirming Bartlettโs test.\nConclusion drawn from testing of statistical assumptions\nWith visual inspections and all statistically tests indicating the data has a non-normal distribution and variance of values are unequal, the statistical approach adopted for testing the various relevant hypothesis devised is a non-parametric one.\nKruskal-Wallis test\nAfter finally deciding on the type of statiscal approach to adopt, a selection is made among several non-parametric methods. For this, consideration is given to the number of categories or groupings in the independent variable. Three types of sales scripts were identified in the datset hence the decision to use Kruskal-Wallis test for the analysis of the second objective.\n\npg.kruskal(data=df_grp_sum, dv='total_conversion', between='sales_script_type')\nfunction (...) \n{\n    dots \nResults\nThe p-value from the Kruskal-Wallis test is statistically significant at 5% significant level which suggests the null hypothesis that conversion among the differenct scripts is rejected.\nConclusion on objective 2\nThe objective 2 seeks to test the hypothesis that all sales scripts produce equal conversions without statistically significant difference. The conclusion drawn is to reject this hypothesis. This can be seen as a breakthrough from the exploratory data analysis yet this results equally raise another enquiry. Though, it is now clear that at least one sales script seems to do better than another for conversion, the analysis does not indicate which sales scripts has a higher conversion and how they differ. This calls for further analysis in the form of post-hoc test.\nRecommendation on objective 2\nWith the suggestion that sales script donot produce equal conversions, it is recommended that, a deliberate effort is made to choose the script with higher rate of conversion. While this recommendation indicates there is opportunity to prioritize sales scripts for higher conversion, it has not specifically suggested which sales script in particular should be used. To provide such a data driven recommendation, further analysis is required to determine which script should be used to maximize conversion.\nObjective 3: To identify which sales script produces higher total conversions on the average\nPost-hoc test is used to satisfy this objective. It provides a better insight through a pairwise comparison of the significant level of differences in conversion between scripts and further identify which scripts are assosiated with higher conversion.\n\n### Post-hoc test with Dunn\n\nsp.posthoc_dunn(a = df_grp_sum, val_col = 'total_conversion', group_col = 'sales_script_type', p_adjust='bonferroni')\nfunction (...) \n{\n    dots \nResult of post-hoc test of difference in conversion\nThe post-hoc test based on Dunn test indicates that there are significant differences in conversion between all the sales scripts and not just at least one. The p-value for the pairwise difference between all the scripts are statistically significant. The result shown above only captures the p-values and in order discern the difference better, another module is used for the estimation.\n\n## Post-hoc test with means of script among other feature output\npg.pairwise_ttests(dv='total_conversion', between='sales_script_type', data=df_grp_sum, parametric = 'false',\n                  alpha = 0.05, padjust = 'bonf', return_desc = 'true').round(3)\nfunction (...) \n{\n    dots \nInterpretation of post-hoc rest results\nFrom the post-hoc test results, the mean of script B is 4.333, mean of script A is 8.342 and the mean of script c is 0.638. The results suggests sales script A has a significantly higher conversion than sales script B and C. In a similar manner, sales script B has a significantly higher conversion than script C in statistical terms.\nConclusion drawn for objective 3\nThe third objective seeks to assess whether difference in conversion is statistically significant and identify which sales script has a significantly higher conversion. The results based on available data suggests that on a given demo day, the number of conversion that sales script A will produce is on the average, significantly higher than the other script types hence should be prioritize in our efforts to increase conversion\nRecommendation for objective 3\nThe sales team should consider prioritizing script A as the best bet to attain a higher conversion rate.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
    "preview": "posts/2022-09-03-product-analysis-report/product-analysis-report_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2022-09-04T11:42:52+02:00",
    "input_file": "product-analysis-report.knit.md",
    "preview_width": 1228,
    "preview_height": 921
  },
  {
    "path": "posts/2022-09-04-logistics-regression/",
    "title": "logistics regression",
    "description": "A short description of the post.",
    "author": [
      {
        "name": "Linus Agbleze",
        "url": "https://agbleze.github.io/Portfolio/"
      }
    ],
    "date": "2022-09-04",
    "categories": [],
    "contents": "\nObjective 4: Assessing factors that influence the likelihood of a client to convert\nThe fourth objective of the product analysis is to move a step further and assess our data with focus on predicting how various factors influence the probability of a client signing up for our product. In order to achieve the goal of higher amount of conversion, as assessment is undertaken to model whether or not a client will sign-up. Knowing how these factors influence clientโs decision to purchase our service will enable us better target clients by managing such predictors well. For this, a review of the various variables available has been undertaken to identify potential predictors.\nThe business problem at hand can be translated into an analytical one that requires a classification solution. In this case, the aim is to classify clients into two groups of convert and non-convert. This is the outcome we seek to predict hence the response or dependent variable. The response variable has two categories or classes as indicated, conversion and non-conversion, hence a regression analysis that caters for such characteristics is chosen.\nMethod: Logistic regression\nLogistic regression will be used for assessing how various factors influence the probability of a client deciding to convert.\nBased on the available limited data guided with domain knowledge of the business operations, the potential predictors identified for the analysis are the following\nType of sales script\nCompany type\nDescription of predictors and rational for their selection\nThe relationship between the type of sales script and conversion has been our main focus for earilier analysis and yet it is still important to understand how they influence the actual decision to convert and not the total number of conversion this time round. It can be presume, as it is the case to hypothesize, that there is a logical understanding of type of sales script being a potential influential determinant.\nCompany type as a potential predictor is based on the assumption that the characteristics of the company will determine its decision to finally purchase our product. Given that, these features are not adequately disclosed, the difference in labelling of company type is taken to be directly related to difference in company characteristics.\nData treatment / preprocessing\nSome predictors have receive a special focus different from earlier, in order to prepare the variables into an acceptable format for modeling. It is worthing highlight such instances for transparency and reproducability of the analysis.\nFirst of all, the company type variable had two categories being 0 and 1, and several missing data in the original dataset. In this case, it is possible that the missiing data was just an instance of unavailable data that was not provided due to some privacy concerns. Moreover, as much as 4,916 missing data is identified in the original dataset which constitutes a sizeable part of the dataset to eliminte from the analysis and very likely to reduce the possibility of capturing as much variations as the reality is on the ground. Supported by this, is the fact that, such a large amount of missing data is enough to constitute a category in its own right for analysis and insight. Based on these available information and context, the decision was made to replace all instances of missing data for company type with โOthersโ as another company type. This led to reclassification of company type into company group A, B and Others corresponding to 0, 1 and missing data respectively;\n\n\n\n",
    "preview": {},
    "last_modified": "2022-09-04T12:00:27+02:00",
    "input_file": "logistics-regression.knit.md"
  },
  {
    "path": "posts/2022-07-18-timeseries-benchmark-models/",
    "title": "Timeseries benchmark models",
    "description": "This post entails timeseries prediction with \nvery simple models that serve as benchmark against \nwhich more advance models .",
    "author": [
      {
        "name": "Linus Agbleze",
        "url": "https://agbleze.github.io/Portfolio/"
      }
    ],
    "date": "2022-07-18",
    "categories": [
      "Timeseries analysis"
    ],
    "contents": "\nTimeseries analysis\nThis post is on timeseries analysis, mainly the basics.\n\n\n\n",
    "preview": {},
    "last_modified": "2022-07-21T01:32:34+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-07-20-hierarchical-clustering-which-states-in-nigeria-have-similar-expenditure-profile/",
    "title": "Hierarchical clustering -- Which states in Nigeria have similar expenditure profile?",
    "description": "Clustering analysis is popular unsupervised machine \nlearning techniques that categorize data into groups of similarity.",
    "author": [
      {
        "name": "Linus Agbleze",
        "url": "https://agbleze.github.io/Portfolio/"
      }
    ],
    "date": "2022-07-18",
    "categories": [
      "Unsupervised learning"
    ],
    "contents": "\nIntroduction to clustering analysis\nClustering analysis is one of the important exploratory data analysis you will consider undertaking in order to identify groupings in your dataset. It belongs to the segment of unsupervised classification which draws the data into clusters of similarity. Infact, many classification schemes have this underpinning of classifying a large set of individual elements or observations into a small number of classes. Talk of species classification, the periodic table, development index among others. Remember, it is groups of similarity. This notion also runs in business or rather dominates business decisions. Differential pricing, product and market segmentation among others require identifying the different groups present and tailoring policies that best serve each of them. So clustering analysis is important there!\nThat being said, there are typically two set of clustering techniques that are use to identify groups of similarity. Namely;\nHierarchical clustering\nK-means clustering\nIn order to do a little deep dive, we will take them in turns. This tutorial will focus on hierarchical clustering where a tree structure is produce for the clusters โ dendrograms.\nHierarchical clustering analysis\nHierarchical clustering is of two types; agglomerative nesting and divisive analysis. The two are oppose in terms of where formation of clusters begin from.\nAgglomerative Nesting (AGNES) merges similar clusters beginning from the bottom-most, into nodes and in a sequential manner continue merging them until all clusters form a single root at the top of the tree.\nDivisive clustering also known as DIANA (Divise Analysis) runs in opposite direction to AGNES. The algorithm starts by disaggregating the root cluster, which is the single cluster containing all other clusters, into dissimilar clusters at each step (with each cluster being internally similar) until all the dissimilarity between clusters are completely exhausted. Thus, it ends with the leafs, which are clusters at the bottom, entirely dissimilar to each other.\nSo AGNES adds up similar cluster leafs to form a cluster node and then a root while DIANA divides a single cluster root into dissimilar cluster nodes and then finally cluster leafs. Thus, AGNES is favoured for finding small clusters while DIANA works well for large clusters.\nWorkflow for hierarchical clustering analysis\nIn order to undertake hierarchical clustering, the data has to be cleaned and prepared for that purpose such that variables are comparable. This involve the following;\nData wraggling and transformation that results in a tidy data such that;\nEach column is a variable\nEach row is a recording or observation\nEach cell is a data\n\nStandardizing / scaling the data\nCalculating dissimilarity measure between clusters\nIn measuring dissimilarity between clusters, different methods can be use and are required to be specified when using AGNES. The methods are identified as follows;\nMaximum / complete linkage: uses the highest dissimilarity found between clusters after pairwise calculation of dissimilarities of elements in different clusters.\nMinimum / single linkage: uses the least dissimilarity between clusters following similar method as complete linkage in calculating dissimilarity\nMean / average linkage: as the name suggests, uses average dissimilarity as distance between clusters after computing pairwise dissimilarity between observations in different clusters.\nCentroid linkage: based on the dissimilarity between the centroid of clusters.\nWardโs minimum variance: focuses on minimizing inward cluster difference by merging groups with least between-cluster distance.\nThe best way to understand the workflow and methods identified for computing hierarchical clustering is by practicing with a project. We are going to define a simple research that clustering analysis can be used for and go through the complete pipeline required to execute it.\nSo letโs take a break from the long discussion and start with practicals.\nResearch objective\nTo identify states in Nigeria with similar expenditure profile on petrol, kerosene and health (excluding insurance)\nDataset\nThe dataset to be used for this tutorial is Living Standard Measurement Study (LSMS) for Nigeria. Specifically, we will use the General Household Survey, 2015-2016 Panel data which is the Wave 3. The dataset can be accessed from (https://microdata.worldbank.org/index.php/catalog/2734/related-materials).\nKnow the Metadata of your data\nAs part of most data science projects in the real world, background checks has to be done even before data exploration and analysis. We have covered our first step which is accessing the data. The data can even be provided to us by our client or project partner or whoever we are working for. But one thing that we will have to do ourselves is to familiarize ourselves with variable names used, questionnaire administered and its design, how data was collected, units of measurement for the various variables among several others. Understanding these elements will influence how we explore the data, organize it in terms of merging datasets and variables, and analyze it.\nTo do this for our dataset, we need to look at the documentation of the dataset which is also available on the site (https://microdata.worldbank.org/index.php/catalog/2734/data-dictionary).\nAfter familiarizing ourselves with the metadata files, it is clear that the variables we want to analyze are in two different files namely sect8b_plantingw3.csv (contains expenditure on various fuels including kerosene and petrol among others) and sect8c_plantingw3.csv (contains expenditure on health). The dataset create opportunity for us to learn key skills of data wrangling and transformation before the actual clustering analysis. This is typical of many projects.\nProject Tasks\nData wrangling and transformation\nSubseting dataset\nfiltering dataset\ngrouping dataset\nsummarizing\n\nScaling dataset\nHierarchical clustering\nAGNES clustering\nDIANA clustering\n\nDetermining optimal clusters\nCustomizing visualization of clusters\nPackages and libraries\nNow that we know the tasks we will be undertaking, we have to install (if not already installed) and load the libraries needed\n\n\n## install librabries if not already available\n### I have commented the code for installation because I have already installed them\n# install.packages(\"tidyverse\")\n# install.packages(\"cluster\")\n# install.packages(\"factoextra\")\n# install.packages(\"dendextend\")\n# install.packages(\"DT\")\n# install.packages(\"skimr\")\n# install.packages(\"dplyr\")\n# install.packages(\"dendextend\")\n# install.packages(\"ape\")\n\n## load librabries\nlibrary(readr)  ## to read csv file\nlibrary(tidyverse) ## to tidy the data\nlibrary(cluster) ##  for cluster analysis\nlibrary(factoextra) ## to visualize clusters\nlibrary(dendextend)  ## to compare dendrograms\nlibrary(stats)  ## for statistical analysis\nlibrary(DT)  ## to present beautiful tables\nlibrary(skimr)  ## for quick descriptive view of the dataset\nlibrary(ape)  ## to visualize dendrograms\n\n\nReading the dataset\nThe first thing to do is to import the dataset (hopefully you have downloaded them) and get a general understanding of it. This can be achieve with the code below\n\n\nfuel_expenditure <- read_csv(\"sect8b_plantingw3.csv\")  ## load the dataset\nhealth_expenditure <- read_csv(\"sect8c_plantingw3.csv\") ## load the dataset\n\nskim(fuel_expenditure)  ## get data summary\n\nTable 1: Data summary\nName\nfuel_expenditure\nNumber of rows\n137729\nNumber of columns\n10\n_______________________\n\nColumn type frequency:\n\ncharacter\n1\nnumeric\n9\n________________________\n\nGroup variables\nNone\nVariable type: character\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\nitem_desc\n0\n1\n3\n30\n0\n30\n0\nVariable type: numeric\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\nzone\n0\n1.00\n3.52\n1.71\n1\n2\n3\n5\n6\nโโโโโ\nstate\n0\n1.00\n18.57\n10.36\n1\n10\n19\n28\n37\nโโโโโ\nlga\n0\n1.00\n1868.92\n1035.81\n102\n1001\n1921\n2806\n3706\nโโโโโ\nsector\n0\n1.00\n1.68\n0.47\n1\n1\n2\n2\n2\nโโโโโ\nea\n0\n1.00\n993.11\n932.12\n0\n330\n760\n1390\n7586\nโโโโโ\nhhid\n0\n1.00\n185571.19\n103683.68\n10001\n90124\n190093\n280038\n370040\nโโโโโ\nitem_cd\n0\n1.00\n315.50\n8.66\n301\n308\n315\n323\n330\nโโโโโ\ns8q3\n2\n1.00\n1.82\n0.39\n1\n2\n2\n2\n2\nโโโโโ\ns8q4\n112727\n0.18\n1911.06\n43304.31\n2\n250\n600\n1500\n6584185\nโโโโโ\n\nskim(health_expenditure) ## get data summary\n\nTable 1: Data summary\nName\nhealth_expenditure\nNumber of rows\n188230\nNumber of columns\n10\n_______________________\n\nColumn type frequency:\n\ncharacter\n1\nnumeric\n9\n________________________\n\nGroup variables\nNone\nVariable type: character\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\nitem_desc\n0\n1\n6\n30\n0\n41\n0\nVariable type: numeric\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\nzone\n0\n1.00\n3.52\n1.71\n1\n2\n3\n5\n6\nโโโโโ\nstate\n0\n1.00\n18.57\n10.36\n1\n10\n19\n28\n37\nโโโโโ\nlga\n0\n1.00\n1868.92\n1035.80\n102\n1001\n1921\n2806\n3706\nโโโโโ\nsector\n0\n1.00\n1.68\n0.47\n1\n1\n2\n2\n2\nโโโโโ\nea\n0\n1.00\n993.11\n932.12\n0\n330\n760\n1390\n7586\nโโโโโ\nhhid\n0\n1.00\n185571.20\n103683.48\n10001\n90124\n190093\n280038\n370040\nโโโโโ\nitem_cd\n0\n1.00\n421.00\n11.83\n401\n411\n421\n431\n441\nโโโโโ\ns8q5\n0\n1.00\n1.84\n0.36\n1\n2\n2\n2\n2\nโโโโโ\ns8q6\n158505\n0.16\n2822.90\n6896.99\n1\n580\n1500\n3000\n300000\nโโโโโ\n\nThe skim() function gives us an overview of the dataset. It shows that there are 10 columns in both the fuel_expenditure and health_expenditure data objects. Among others, it gives the mean and number of missing cases for each variable. Depending on the type of variable, that is whether character or numeric, we get different type of summary. Some of the variables are not in their right data type format. Example, zone, state and lga are numeric instead of being character but that is not the focus here. The variable s8q4 from fuel_expenditure data object has 112727 missing records.\nWe keep saying the variable and that brings up the question, what does the variable actually mean? This is where our earlier exercise of viewing the metadata plays its role. If we indeed went through the various documentations and questionnaire that comes with the data, we will realize that variable s8q4 (fuel_expenditure data) represents household monthly expenditure measured in Naire for various items identified in item_desc variable. Likewise, s8q6 (health_expenditure data) represents expenditure for items but this time round it covers 6 month duration.\nOur dataset is quite large to be viewed in a table nicely and adequately; so we will subset the first 100 rows (remember rows are individual records or observations) and show them in an interactive table using the datatable() function from DT package.\n\n\ndatatable(fuel_expenditure[1:100, ])  ## view the first 100 observations in fuel_expenditure data\n\n\n\nThe fuel_expenditure table above shows that we have more variables than needed for our analysis as we are only interested in Petrol and Kerosene captured in the item_desc variable. Moreover, the variable names are not understandable. So just by viewing the table, we will realize that some data wrangling tasks await us. The same can be said for the health_expenditure data object.\nLetโs view the first 100 rows of health_expenditure data.\n\n\ndatatable(health_expenditure[1:100, ]) ## view the health_expenditure data \n\n\n\nData wrangling and transformation\nWe have imported and viewed our dataset successfully and now we have to clean, and transform our data to the required format for analysis. Data wrangling and transformation activities are not a cast in a stone despite some activities are very common. The type and structure of dataset and analysis we want to perform determines what we need to do.\nLooking at our dataset, we need to reduce it to only columns that we are interested in, add some new columns with transformed data and clean out the missing data. We will be utilizing piping approach (%>%) to harness its benefit of logical and clear flow of analysis. This can be achieve with the code below for fuel_expenditure.\n\n\n\nNow, we have reduced the data to only variables needed. Yet, it is obvious there is still work to be done. First of all, the varaible โstateโ has meaningless numbers instead of names of states in Nigeria. This was useful for data collection and data entry but not our analysis where audience have to know state names explicitely. How do we know exactly what these numbers represent? Again, this is where our metadata exploration comes to our rescue. When we reviewed the documentation that came with the data, we realized that under โstate codeโ, the corresponding state names are provided. With this, we can add a new column that has the corresponding state names.\nAnother issue to note, which also influences how to transform our data is that, the number of observations recorded for each state varies. This has to do with the sampling technique used during data collection in order to achieve good representativeness. We want to make sure that, the states are comparable in terms of expenditure. So instead of summarizing base on the total expenditure for each state, we will do so based on the mean. With that, we will see mean expenditure of state and not sum of households. In order to captured this as our labeling when we do the clustering analysis, we will convert the column of state names to row names.\nNow, letโs code all that we have discussed so far.\n\n\nfuel_expenditure_transformed <- fuel_expenditure_transformed %>%\n  dplyr::mutate(state_names = case_when(  ## add a new column containing state names\n     state == 1 ~ \"Abia\",\n     state == 2 ~ \"Adamawa\",\n     state == 3 ~ \"Akwa Ibom\",\n     state == 4 ~ \"Anambra\",\n     state == 5 ~ \"Bauchi\",\n     state == 6 ~ \"Bayelsa\",\n     state == 7 ~ \"Benue\",\n     state == 8 ~ \"Borno\", \n     state == 9 ~ \"Cross River\", \n     state == 10 ~ \"Delta\", \n     state == 11 ~ \"Ebonyi\", \n     state == 12 ~ \"Edo\",\n     state == 13 ~ \"Ekiti\", \n     state == 14 ~ \"Enugu\", \n     state == 15 ~ \"Gombe\", \n     state == 16 ~ \"Imo\", \n     state == 17 ~ \"Jigawa\", \n     state == 18 ~ \"Kaduna\", \n     state == 19 ~ \"Kano\", \n     state == 20 ~ \"Katsina\", \n     state == 21 ~ \"Kebbi\",\n     state ==22 ~ \"Kogi\", \n     state == 23 ~ \"Kwara\", \n     state == 24 ~ \"Lagos\", \n     state == 25 ~ \"Nasarawa\",\n     state == 26 ~ \"Niger\", \n     state == 27 ~ \"Ogun\", \n     state == 28 ~ \"Ondo\", \n     state == 29 ~ \"Osun\", \n     state == 30 ~ \"Oyo\",\n     state == 31 ~ \"Plateau\", \n     state == 32 ~ \"Rivers\", \n     state == 33 ~ \"Sokoto\", \n     state == 34 ~ \"Taraba\",\n     state == 35 ~ \"Yobe\",\n     state == 36 ~ \"Zamfara\", \n     state == 37 ~ \"FCT Abuja\")\n   ) %>%\n  tidyr::pivot_wider(names_from = item_desc, values_from = s8q4, values_fn = mean)\n\ndatatable(fuel_expenditure_transformed)\n\n\n\nAfter cleaning and transforming fuel_expenditure data, we need to make sure that the second data is also transform to be at the same quality level or format. Remember that health_expenditure is for 6 months which means that in order to analyze both data properly for the same time scale, we need to convert it to monthly data and then calculate the mean.\nThe code to transform health expenditure data to required format is below;\n\n\nhealth_expenditure_transformed <- health_expenditure %>%\n  dplyr::select(state, item_desc, s8q6)%>%\n  dplyr::filter(item_desc %in% \"HEALTH EXPENDITURES (EXCLUDING\") %>%\n  na.omit() %>%\n   dplyr::mutate(state_names = case_when(\n     state == 1 ~ \"Abia\",\n     state == 2 ~ \"Adamawa\",\n     state == 3 ~ \"Akwa Ibom\",\n     state == 4 ~ \"Anambra\",\n     state == 5 ~ \"Bauchi\",\n     state == 6 ~ \"Bayelsa\",\n     state == 7 ~ \"Benue\",\n     state == 8 ~ \"Borno\",\n     state == 9 ~ \"Cross River\",\n     state == 10 ~ \"Delta\",\n     state == 11 ~ \"Ebonyi\",\n     state == 12 ~ \"Edo\",\n     state == 13 ~ \"Ekiti\",\n     state == 14 ~ \"Enugu\",\n     state == 15 ~ \"Gombe\",\n     state == 16 ~ \"Imo\",\n     state == 17 ~ \"Jigawa\",\n     state == 18 ~ \"Kaduna\",\n     state == 19 ~ \"Kano\",\n     state == 20 ~ \"Katsina\",\n     state == 21 ~ \"Kebbi\",\n     state ==22 ~ \"Kogi\",\n     state == 23 ~ \"Kwara\",\n     state == 24 ~ \"Lagos\",\n     state == 25 ~ \"Nasarawa\",\n     state == 26 ~ \"Niger\",\n     state == 27 ~ \"Ogun\",\n     state == 28 ~ \"Ondo\",\n     state == 29 ~ \"Osun\",\n     state == 30 ~ \"Oyo\",\n     state == 31 ~ \"Plateau\",\n     state == 32 ~ \"Rivers\",\n     state == 33 ~ \"Sokoto\",\n     state == 34 ~ \"Taraba\",\n     state == 35 ~ \"Yobe\",\n     state == 36 ~ \"Zamfara\",\n     state == 37 ~ \"FCT Abuja\")\n   ) %>%\n  dplyr::group_by(state_names) \n\n  health_expenditure_transformed <- dplyr::summarize(health_expenditure_transformed, Monthly_mean_health_expenditure = mean(s8q6/6))\n \n datatable(health_expenditure_transformed)\n\n\n\nNow, we have transformed both datasets and we need to verify if they are equal in terms of number of rows and columns before joining together for further analysis.\n\n\nnrow(fuel_expenditure_transformed)\n\n[1] 37\n\nnrow(health_expenditure_transformed)\n\n[1] 36\n\nWe will realize that the number of rows are different for fuel_expenditure_transformed (37) and health_expenditure_transformed (36). This is a common occurrence in data exploration and preparation. There is a state without data in health_expenditure_transformed. If we are to inspect the data, we will realize that there is no record for Jigawa state. This is an instance where we face the decision of either employing a data imputation method to provide values for Jigasaw or leaving it out altogether. We will tackle data imputation in a different post. For now, the better choice will be to leave Jigawa state out of the analysis. So we will remove the row for Jigawa state in fuel_expenditure_transformed in order to have 36 rows for both datasets and join them together. Then, we will convert the column โstate_namesโ to row names for labeling the cluster leafs to be more meaningful. The code to achieve that is as follows;\n\n\ndataset_all <- subset.data.frame(fuel_expenditure_transformed, fuel_expenditure_transformed$state != 17) %>%\n  dplyr::left_join(health_expenditure_transformed) %>%  ## join the two datasets\n  tibble::column_to_rownames(var = \"state_names\") %>% ## transform variable state_names to rownames\n  select(-1) ## select all columns except the first one (state) which we don't need again\n\ndatatable(dataset_all)\n\n\n\nNow our data is ready for analysis!. But before then, we have to do clustering relevant data preparation\nClustering relevant data preparation\nThe data preparation done at this stage is not one that we will do for all types of analysis. We are doing them in order to successfully undertake a clustering analysis that is appropriate.\nData standardization: Before applying various clustering algorithms, we need to standardize the data to have mean zero and standard deviation one. This is done to make our variables have a common scale hence comparable.\nComputation of dissimilarity measure: This is done to identify observations that belong to the same clusters.\nNow, letโs write our code for that;\n\n\n## scale the data\n dataset_all_scaled <- scale(dataset_all)\n \n ## calculate dissimilarity \n dataset_dissimilarity <- dist(dataset_all_scaled, method = \"euclidean\")\n\n\nWe will use all the clustering methods we have discussed above and compare the results.\nAgglomerative Nesting (AGNES)\nAgglomerative nesting can be undertaken using hclust(), flashClust or agnes()\n\n\n## hierarchical clustering wtih complete linkage\nhcl_complete<-  hclust(dataset_dissimilarity, method = \"complete\")\n \nplot(hcl_complete, cex = 0.6, hang = -1) \n\n\n\nUsing agnes() function gives us the agglomerative co-efficient (ac) which measures the strength of clustering. The higher the ac, the stronger the clustering structure.\n\n\n## agnes clustering\nagnes_complete <- agnes(dataset_dissimilarity, method = \"complete\")\n\nplot(agnes_complete)\n\n\n\nNow, letโs look at other methods we have discussed for AGNES.\n\n\n## compute agnes clustering with various methods\nagnes_single <- agnes(dataset_dissimilarity, method = \"single\")\nagnes_average <- agnes(dataset_dissimilarity, method = \"average\")\nagnes_ward <- agnes(dataset_dissimilarity, method = \"ward\")\n\n#Plot results\nplot(agnes_single)\n\n\nplot(agnes_average)\n\n\nplot(agnes_ward)\n\n\n\nFrom the various methods, agnes with the ward method produces an ac of 0.9 which is the highest compared to others hence depicts the strongest structure.\nBefore we draw the curtains down on agnes, letโs also note that hcluster() function also accepts other methods\n\n\nhclust_mcquitty <- hclust(dataset_dissimilarity, method = \"mcquitty\")\nhclust_median <- hclust(dataset_dissimilarity, method = \"median\")\nhclust_centriod <- hclust(dataset_dissimilarity, method = \"centroid\")\n\nplot(hclust_mcquitty, cex = 0.6, hang = -1)\n\n\nplot(hclust_median, cex = 0.6, hang = -1)\n\n\nplot(hclust_centriod, cex = 0.6, hang = -1)\n\n\n\nDivisive clustering\nDivisive clustering does not require a method argument to be specified. letโs analyze the data using diana() function.\n\n\n#### divisive clustering\n# computes divisive hierarchical clustering\ndivisive_cluster <- cluster::diana(dataset_dissimilarity)\n\n# divisive coefficient -- amount of clustering structure found\ndivisive_cluster$dc\n\n[1] 0.8510366\n\n#plot dendrogram for divisive clustering\npltree(divisive_cluster, cex = 0.6, hang = .2, main = \"Dendrogram of diana\")\n\n\ndivisive_cluster$dc\n\n[1] 0.8510366\n\nThe divisive co-efficient (dc) is computed to be 0.85 which indicates strong clustering structure. The ac and dc are measured on a scale of 0 to 1 with lower values close to 0 indicating weak clustering and higher values close to 1 indicating strong cluster structure.\nIdentifying clusters of subgroups\nWe can identify observations that falls into similar groups or otherwise by stating the number of groups we want to cut into. letโs cut our observations into 5 clusters with the code below. This can be done for both agnes and divise\n\n\n# CUT TREE INTO 5 GROUPS\nhcl_complete_group <- cutree(hcl_complete, k = 5)\n\n### Number of members in each cluster\ntable(hcl_complete_group)\n\nhcl_complete_group\n 1  2  3  4  5 \n 8 20  5  2  1 \n\n##add cluster groupings to data\ndataset_all_cluster <- dataset_all%>%\n  mutate(cluster = hcl_complete_group) %>%\n  head()\n\n\nWe can visualize how the various states are grouped when we cut into 5 clusters using the code below\n\n\n## visualize with fviz_cluster\nfviz_cluster(list(data = dataset_all_scaled, cluster = hcl_complete_group))\n\n\n\nComparing dendrograms\nWe can compare clustering based on different methods by creating dendrogram objects for them and plotting.\n\n\n### create dendrogram object\nagnes_complete_dendrogram <- as.dendrogram(agnes_complete)\nagnes_ward_dendrogram <- as.dendrogram(agnes_ward)\n\n### plot the dendrograms to compare and match\ntanglegram(agnes_complete_dendrogram, agnes_ward_dendrogram)\n\n\n\nThe dashed lines show cluster combinations different from others.\nCustomizing the dendrogram\nVisualizing results has never been less important for cluster analysis where observations are grouped and there is the need for more colourful differentiation of groups to better understand the results presented. There are options for this. First, we can chain multiple dendrograms together to customize them easily.\n\n\n# create a dendlist\ndend_list_agnes_complete_ward <- dendlist(agnes_complete_dendrogram, agnes_ward_dendrogram)\n\ntanglegram(dend1 = agnes_complete_dendrogram, dend2 = agnes_ward_dendrogram,\n           highlight_distinct_edges = TRUE, # Turn off dashed lines\n           common_subtrees_color_lines = TRUE, # turn-off line colors\n           common_subtrees_color_branches = TRUE, # color common branches\n           highlight_branches_col = TRUE,\n           main_left = \"Agglomerative Nesting (complete method)\",\n           main_right = \"Agglomerative Nesting (ward method)\",\n           sub = \"Expenditure on Petrol, Kerosene and Health -States in Nigeria\",\n           cex_main_left = 1.1,\n           cex_main = 0.8,\n            cex_sub = .6,\n           lab.cex = 1,\n            margin_inner = 5,\n           main = paste(\"Entanglement = \", round(entanglement(dend_list_agnes_complete_ward), 2)))\n\n\nagnes_complete_dendrogram\n\n'dendrogram' with 2 branches and 36 members total, at height 6.648864 \n\nWe have added entanglement co-efficient to the graph. It measures how well-align the trees are on a scale of 0 to 1. The low co-oefficient of 0.17 indicates low entanglement with good alignment.\nDetermining optimal number of clusters\nA number of methods can be used to determine the optimal number of clusters for grouping\nElbow method of determing optimal number of clusters\nTo determine optimal number of clusters with elbow method, we need to look at where the bend is located which can be gauged to be 3 from our analysis below\n\n\nfviz_nbclust(dataset_all_scaled, FUN = hcut, method = \"wss\")\n\n\n\nAverage silhouette method\nAverage silhouette method indicates how well the clusters form with within cluster difference minimized at the optimal cluster number. Thus, the highest average silhouette width corresponds to the optimal number of clusters.\n\n\nfviz_nbclust(dataset_all_scaled, FUN = hcut, method = \"silhouette\")\n\n\n\nFrom the plot above, the optimal number of clusters is 3.\nGap statistics method\n\n\ndataset_gapstat <- clusGap(dataset_all_scaled, FUN = hcut, K.max = 10)\n\nfviz_gap_stat(dataset_gapstat)\n\n\n\nThe gaps method identified 1 to be the optimal number of clusters within a range of 1 to 10 clusters. Well, as we can deduce, different methods can identify different number of optimal clusters. With 2 of the methods suggesting 3 to be the optimal number of clusters, we can go by that.\nNow that we know the optimal number of clusters, letโs use that to visualize the data but in a way different from the previous.\n\n\n# Cut the dendrogram into 4 clusters\ncolors = c(\"brown\", \"orange\", \"#1DEfbb\", \"yellow\")\n\nward_cut <- cutree(hcl_complete, k = 3)\nplot(as.phylo(hcl_complete), type = \"fan\", tip.color = colors[ward_cut], label.offset = .5, cex = 0.7, no.margin = TRUE, use.edge.length = TRUE)\n\n\n\nConclusion\nWith this simple exercise, we learnt how to undertake hierarchical clustering analysis using a project that is typical of the real world. We went through process of data wrangling and transformation, clustering and determining the optimal number of clusters.\n\n\n\n",
    "preview": "posts/2022-07-20-hierarchical-clustering-which-states-in-nigeria-have-similar-expenditure-profile/hierarchical-clustering-which-states-in-nigeria-have-similar-expenditure-profile_files/figure-html5/customize-dendro-1.png",
    "last_modified": "2022-07-21T01:29:08+02:00",
    "input_file": {},
    "preview_width": 1536,
    "preview_height": 1152
  },
  {
    "path": "posts/2022-07-17-data-visualization/",
    "title": "Data visualization",
    "description": "This post entails the use of ggplot for data visualization.",
    "author": [
      {
        "name": "Linus Agbleze",
        "url": "https://agbleze.github.io/Portfolio/"
      }
    ],
    "date": "2022-07-17",
    "categories": [
      "visualization"
    ],
    "contents": "\n\n\n\n\n\n\n",
    "preview": "posts/2022-07-17-data-visualization/data-visualization_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2022-07-20T03:40:04+02:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/welcome/",
    "title": "Welcome to Datasiast!",
    "description": "Datasiast exists for enthusiatic data science knowledge sharing and practice.",
    "author": [
      {
        "name": "Linus Agbleze",
        "url": "https://agbleze.github.io/Portfolio/"
      }
    ],
    "date": "2022-07-17",
    "categories": [],
    "contents": "\n\n\n\n",
    "preview": {},
    "last_modified": "2022-07-20T03:40:25+02:00",
    "input_file": {}
  }
]
