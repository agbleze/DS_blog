[
  {
    "path": "posts/2022-07-20-hierarchical-clustering-which-states-in-nigeria-have-similar-expenditure-profile/",
    "title": "Hierarchical clustering -- Which states in Nigeria have similar expenditure profile?",
    "description": "Clustering analysis is popular unsupervised machine \nlearning techniques that categorize data into groups of similarity.",
    "author": [
      {
        "name": "Linus Agbleze",
        "url": "https://agbleze.github.io/Portfolio/"
      }
    ],
    "date": "2022-07-18",
    "categories": [
      "Unsupervised learning"
    ],
    "contents": "\nIntroduction to clustering analysis\nClustering analysis is one of the important exploratory data analysis you will consider undertaking in order to identify groupings in your dataset. It belongs to the segment of unsupervised classification which draws the data into clusters of similarity. Infact, many classification schemes have this underpinning of classifying a large set of individual elements or observations into a small number of classes. Talk of species classification, the periodic table, development index among others. Remember, it is groups of similarity. This notion also runs in business or rather dominates business decisions. Differential pricing, product and market segmentation among others require identifying the different groups present and tailoring policies that best serve each of them. So clustering analysis is important there!\nThat being said, there are typically two set of clustering techniques that are use to identify groups of similarity. Namely;\nHierarchical clustering\nK-means clustering\nIn order to do a little deep dive, we will take them in turns. This tutorial will focus on hierarchical clustering where a tree structure is produce for the clusters – dendrograms.\nHierarchical clustering analysis\nHierarchical clustering is of two types; agglomerative nesting and divisive analysis. The two are oppose in terms of where formation of clusters begin from.\nAgglomerative Nesting (AGNES) merges similar clusters beginning from the bottom-most, into nodes and in a sequential manner continue merging them until all clusters form a single root at the top of the tree.\nDivisive clustering also known as DIANA (Divise Analysis) runs in opposite direction to AGNES. The algorithm starts by disaggregating the root cluster, which is the single cluster containing all other clusters, into dissimilar clusters at each step (with each cluster being internally similar) until all the dissimilarity between clusters are completely exhausted. Thus, it ends with the leafs, which are clusters at the bottom, entirely dissimilar to each other.\nSo AGNES adds up similar cluster leafs to form a cluster node and then a root while DIANA divides a single cluster root into dissimilar cluster nodes and then finally cluster leafs. Thus, AGNES is favoured for finding small clusters while DIANA works well for large clusters.\nWorkflow for hierarchical clustering analysis\nIn order to undertake hierarchical clustering, the data has to be cleaned and prepared for that purpose such that variables are comparable. This involve the following;\nData wraggling and transformation that results in a tidy data such that;\nEach column is a variable\nEach row is a recording or observation\nEach cell is a data\n\nStandardizing / scaling the data\nCalculating dissimilarity measure between clusters\nIn measuring dissimilarity between clusters, different methods can be use and are required to be specified when using AGNES. The methods are identified as follows;\nMaximum / complete linkage: uses the highest dissimilarity found between clusters after pairwise calculation of dissimilarities of elements in different clusters.\nMinimum / single linkage: uses the least dissimilarity between clusters following similar method as complete linkage in calculating dissimilarity\nMean / average linkage: as the name suggests, uses average dissimilarity as distance between clusters after computing pairwise dissimilarity between observations in different clusters.\nCentroid linkage: based on the dissimilarity between the centroid of clusters.\nWard’s minimum variance: focuses on minimizing inward cluster difference by merging groups with least between-cluster distance.\nThe best way to understand the workflow and methods identified for computing hierarchical clustering is by practicing with a project. We are going to define a simple research that clustering analysis can be used for and go through the complete pipeline required to execute it.\nSo let’s take a break from the long discussion and start with practicals.\nResearch objective\nTo identify states in Nigeria with similar expenditure profile on petrol, kerosene and health (excluding insurance)\nDataset\nThe dataset to be used for this tutorial is Living Standard Measurement Study (LSMS) for Nigeria. Specifically, we will use the General Household Survey, 2015-2016 Panel data which is the Wave 3. The dataset can be accessed from (https://microdata.worldbank.org/index.php/catalog/2734/related-materials).\nKnow the Metadata of your data\nAs part of most data science projects in the real world, background checks has to be done even before data exploration and analysis. We have covered our first step which is accessing the data. The data can even be provided to us by our client or project partner or whoever we are working for. But one thing that we will have to do ourselves is to familiarize ourselves with variable names used, questionnaire administered and its design, how data was collected, units of measurement for the various variables among several others. Understanding these elements will influence how we explore the data, organize it in terms of merging datasets and variables, and analyze it.\nTo do this for our dataset, we need to look at the documentation of the dataset which is also available on the site (https://microdata.worldbank.org/index.php/catalog/2734/data-dictionary).\nAfter familiarizing ourselves with the metadata files, it is clear that the variables we want to analyze are in two different files namely sect8b_plantingw3.csv (contains expenditure on various fuels including kerosene and petrol among others) and sect8c_plantingw3.csv (contains expenditure on health). The dataset create opportunity for us to learn key skills of data wrangling and transformation before the actual clustering analysis. This is typical of many projects.\nProject Tasks\nData wrangling and transformation\nSubseting dataset\nfiltering dataset\ngrouping dataset\nsummarizing\n\nScaling dataset\nHierarchical clustering\nAGNES clustering\nDIANA clustering\n\nDetermining optimal clusters\nCustomizing visualization of clusters\nPackages and libraries\nNow that we know the tasks we will be undertaking, we have to install (if not already installed) and load the libraries needed\n\n\n## install librabries if not already available\n### I have commented the code for installation because I have already installed them\n# install.packages(\"tidyverse\")\n# install.packages(\"cluster\")\n# install.packages(\"factoextra\")\n# install.packages(\"dendextend\")\n# install.packages(\"DT\")\n# install.packages(\"skimr\")\n# install.packages(\"dplyr\")\n# install.packages(\"dendextend\")\n# install.packages(\"ape\")\n\n## load librabries\nlibrary(readr)  ## to read csv file\nlibrary(tidyverse) ## to tidy the data\nlibrary(cluster) ##  for cluster analysis\nlibrary(factoextra) ## to visualize clusters\nlibrary(dendextend)  ## to compare dendrograms\nlibrary(stats)  ## for statistical analysis\nlibrary(DT)  ## to present beautiful tables\nlibrary(skimr)  ## for quick descriptive view of the dataset\nlibrary(ape)  ## to visualize dendrograms\n\n\nReading the dataset\nThe first thing to do is to import the dataset (hopefully you have downloaded them) and get a general understanding of it. This can be achieve with the code below\n\n\nfuel_expenditure <- read_csv(\"sect8b_plantingw3.csv\")  ## load the dataset\nhealth_expenditure <- read_csv(\"sect8c_plantingw3.csv\") ## load the dataset\n\nskim(fuel_expenditure)  ## get data summary\n\nTable 1: Data summary\nName\nfuel_expenditure\nNumber of rows\n137729\nNumber of columns\n10\n_______________________\n\nColumn type frequency:\n\ncharacter\n1\nnumeric\n9\n________________________\n\nGroup variables\nNone\nVariable type: character\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\nitem_desc\n0\n1\n3\n30\n0\n30\n0\nVariable type: numeric\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\nzone\n0\n1.00\n3.52\n1.71\n1\n2\n3\n5\n6\n▇▅▅▅▅\nstate\n0\n1.00\n18.57\n10.36\n1\n10\n19\n28\n37\n▇▆▇▇▇\nlga\n0\n1.00\n1868.92\n1035.81\n102\n1001\n1921\n2806\n3706\n▇▆▇▇▇\nsector\n0\n1.00\n1.68\n0.47\n1\n1\n2\n2\n2\n▃▁▁▁▇\nea\n0\n1.00\n993.11\n932.12\n0\n330\n760\n1390\n7586\n▇▂▁▁▁\nhhid\n0\n1.00\n185571.19\n103683.68\n10001\n90124\n190093\n280038\n370040\n▇▆▇▇▇\nitem_cd\n0\n1.00\n315.50\n8.66\n301\n308\n315\n323\n330\n▇▇▇▇▇\ns8q3\n2\n1.00\n1.82\n0.39\n1\n2\n2\n2\n2\n▂▁▁▁▇\ns8q4\n112727\n0.18\n1911.06\n43304.31\n2\n250\n600\n1500\n6584185\n▇▁▁▁▁\n\nskim(health_expenditure) ## get data summary\n\nTable 1: Data summary\nName\nhealth_expenditure\nNumber of rows\n188230\nNumber of columns\n10\n_______________________\n\nColumn type frequency:\n\ncharacter\n1\nnumeric\n9\n________________________\n\nGroup variables\nNone\nVariable type: character\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\nitem_desc\n0\n1\n6\n30\n0\n41\n0\nVariable type: numeric\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\nzone\n0\n1.00\n3.52\n1.71\n1\n2\n3\n5\n6\n▇▅▅▅▅\nstate\n0\n1.00\n18.57\n10.36\n1\n10\n19\n28\n37\n▇▆▇▇▇\nlga\n0\n1.00\n1868.92\n1035.80\n102\n1001\n1921\n2806\n3706\n▇▆▇▇▇\nsector\n0\n1.00\n1.68\n0.47\n1\n1\n2\n2\n2\n▃▁▁▁▇\nea\n0\n1.00\n993.11\n932.12\n0\n330\n760\n1390\n7586\n▇▂▁▁▁\nhhid\n0\n1.00\n185571.20\n103683.48\n10001\n90124\n190093\n280038\n370040\n▇▆▇▇▇\nitem_cd\n0\n1.00\n421.00\n11.83\n401\n411\n421\n431\n441\n▇▇▇▇▇\ns8q5\n0\n1.00\n1.84\n0.36\n1\n2\n2\n2\n2\n▂▁▁▁▇\ns8q6\n158505\n0.16\n2822.90\n6896.99\n1\n580\n1500\n3000\n300000\n▇▁▁▁▁\n\nThe skim() function gives us an overview of the dataset. It shows that there are 10 columns in both the fuel_expenditure and health_expenditure data objects. Among others, it gives the mean and number of missing cases for each variable. Depending on the type of variable, that is whether character or numeric, we get different type of summary. Some of the variables are not in their right data type format. Example, zone, state and lga are numeric instead of being character but that is not the focus here. The variable s8q4 from fuel_expenditure data object has 112727 missing records.\nWe keep saying the variable and that brings up the question, what does the variable actually mean? This is where our earlier exercise of viewing the metadata plays its role. If we indeed went through the various documentations and questionnaire that comes with the data, we will realize that variable s8q4 (fuel_expenditure data) represents household monthly expenditure measured in Naire for various items identified in item_desc variable. Likewise, s8q6 (health_expenditure data) represents expenditure for items but this time round it covers 6 month duration.\nOur dataset is quite large to be viewed in a table nicely and adequately; so we will subset the first 100 rows (remember rows are individual records or observations) and show them in an interactive table using the datatable() function from DT package.\n\n\ndatatable(fuel_expenditure[1:100, ])  ## view the first 100 observations in fuel_expenditure data\n\n\n\nThe fuel_expenditure table above shows that we have more variables than needed for our analysis as we are only interested in Petrol and Kerosene captured in the item_desc variable. Moreover, the variable names are not understandable. So just by viewing the table, we will realize that some data wrangling tasks await us. The same can be said for the health_expenditure data object.\nLet’s view the first 100 rows of health_expenditure data.\n\n\ndatatable(health_expenditure[1:100, ]) ## view the health_expenditure data \n\n\n\nData wrangling and transformation\nWe have imported and viewed our dataset successfully and now we have to clean, and transform our data to the required format for analysis. Data wrangling and transformation activities are not a cast in a stone despite some activities are very common. The type and structure of dataset and analysis we want to perform determines what we need to do.\nLooking at our dataset, we need to reduce it to only columns that we are interested in, add some new columns with transformed data and clean out the missing data. We will be utilizing piping approach (%>%) to harness its benefit of logical and clear flow of analysis. This can be achieve with the code below for fuel_expenditure.\n\n\n\nNow, we have reduced the data to only variables needed. Yet, it is obvious there is still work to be done. First of all, the varaible “state” has meaningless numbers instead of names of states in Nigeria. This was useful for data collection and data entry but not our analysis where audience have to know state names explicitely. How do we know exactly what these numbers represent? Again, this is where our metadata exploration comes to our rescue. When we reviewed the documentation that came with the data, we realized that under “state code”, the corresponding state names are provided. With this, we can add a new column that has the corresponding state names.\nAnother issue to note, which also influences how to transform our data is that, the number of observations recorded for each state varies. This has to do with the sampling technique used during data collection in order to achieve good representativeness. We want to make sure that, the states are comparable in terms of expenditure. So instead of summarizing base on the total expenditure for each state, we will do so based on the mean. With that, we will see mean expenditure of state and not sum of households. In order to captured this as our labeling when we do the clustering analysis, we will convert the column of state names to row names.\nNow, let’s code all that we have discussed so far.\n\n\nfuel_expenditure_transformed <- fuel_expenditure_transformed %>%\n  dplyr::mutate(state_names = case_when(  ## add a new column containing state names\n     state == 1 ~ \"Abia\",\n     state == 2 ~ \"Adamawa\",\n     state == 3 ~ \"Akwa Ibom\",\n     state == 4 ~ \"Anambra\",\n     state == 5 ~ \"Bauchi\",\n     state == 6 ~ \"Bayelsa\",\n     state == 7 ~ \"Benue\",\n     state == 8 ~ \"Borno\", \n     state == 9 ~ \"Cross River\", \n     state == 10 ~ \"Delta\", \n     state == 11 ~ \"Ebonyi\", \n     state == 12 ~ \"Edo\",\n     state == 13 ~ \"Ekiti\", \n     state == 14 ~ \"Enugu\", \n     state == 15 ~ \"Gombe\", \n     state == 16 ~ \"Imo\", \n     state == 17 ~ \"Jigawa\", \n     state == 18 ~ \"Kaduna\", \n     state == 19 ~ \"Kano\", \n     state == 20 ~ \"Katsina\", \n     state == 21 ~ \"Kebbi\",\n     state ==22 ~ \"Kogi\", \n     state == 23 ~ \"Kwara\", \n     state == 24 ~ \"Lagos\", \n     state == 25 ~ \"Nasarawa\",\n     state == 26 ~ \"Niger\", \n     state == 27 ~ \"Ogun\", \n     state == 28 ~ \"Ondo\", \n     state == 29 ~ \"Osun\", \n     state == 30 ~ \"Oyo\",\n     state == 31 ~ \"Plateau\", \n     state == 32 ~ \"Rivers\", \n     state == 33 ~ \"Sokoto\", \n     state == 34 ~ \"Taraba\",\n     state == 35 ~ \"Yobe\",\n     state == 36 ~ \"Zamfara\", \n     state == 37 ~ \"FCT Abuja\")\n   ) %>%\n  tidyr::pivot_wider(names_from = item_desc, values_from = s8q4, values_fn = mean)\n\ndatatable(fuel_expenditure_transformed)\n\n\n\nAfter cleaning and transforming fuel_expenditure data, we need to make sure that the second data is also transform to be at the same quality level or format. Remember that health_expenditure is for 6 months which means that in order to analyze both data properly for the same time scale, we need to convert it to monthly data and then calculate the mean.\nThe code to transform health expenditure data to required format is below;\n\n\nhealth_expenditure_transformed <- health_expenditure %>%\n  dplyr::select(state, item_desc, s8q6)%>%\n  dplyr::filter(item_desc %in% \"HEALTH EXPENDITURES (EXCLUDING\") %>%\n  na.omit() %>%\n   dplyr::mutate(state_names = case_when(\n     state == 1 ~ \"Abia\",\n     state == 2 ~ \"Adamawa\",\n     state == 3 ~ \"Akwa Ibom\",\n     state == 4 ~ \"Anambra\",\n     state == 5 ~ \"Bauchi\",\n     state == 6 ~ \"Bayelsa\",\n     state == 7 ~ \"Benue\",\n     state == 8 ~ \"Borno\",\n     state == 9 ~ \"Cross River\",\n     state == 10 ~ \"Delta\",\n     state == 11 ~ \"Ebonyi\",\n     state == 12 ~ \"Edo\",\n     state == 13 ~ \"Ekiti\",\n     state == 14 ~ \"Enugu\",\n     state == 15 ~ \"Gombe\",\n     state == 16 ~ \"Imo\",\n     state == 17 ~ \"Jigawa\",\n     state == 18 ~ \"Kaduna\",\n     state == 19 ~ \"Kano\",\n     state == 20 ~ \"Katsina\",\n     state == 21 ~ \"Kebbi\",\n     state ==22 ~ \"Kogi\",\n     state == 23 ~ \"Kwara\",\n     state == 24 ~ \"Lagos\",\n     state == 25 ~ \"Nasarawa\",\n     state == 26 ~ \"Niger\",\n     state == 27 ~ \"Ogun\",\n     state == 28 ~ \"Ondo\",\n     state == 29 ~ \"Osun\",\n     state == 30 ~ \"Oyo\",\n     state == 31 ~ \"Plateau\",\n     state == 32 ~ \"Rivers\",\n     state == 33 ~ \"Sokoto\",\n     state == 34 ~ \"Taraba\",\n     state == 35 ~ \"Yobe\",\n     state == 36 ~ \"Zamfara\",\n     state == 37 ~ \"FCT Abuja\")\n   ) %>%\n  dplyr::group_by(state_names) \n\n  health_expenditure_transformed <- dplyr::summarize(health_expenditure_transformed, Monthly_mean_health_expenditure = mean(s8q6/6))\n \n datatable(health_expenditure_transformed)\n\n\n\nNow, we have transformed both datasets and we need to verify if they are equal in terms of number of rows and columns before joining together for further analysis.\n\n\nnrow(fuel_expenditure_transformed)\n\n[1] 37\n\nnrow(health_expenditure_transformed)\n\n[1] 36\n\nWe will realize that the number of rows are different for fuel_expenditure_transformed (37) and health_expenditure_transformed (36). This is a common occurrence in data exploration and preparation. There is a state without data in health_expenditure_transformed. If we are to inspect the data, we will realize that there is no record for Jigawa state. This is an instance where we face the decision of either employing a data imputation method to provide values for Jigasaw or leaving it out altogether. We will tackle data imputation in a different post. For now, the better choice will be to leave Jigawa state out of the analysis. So we will remove the row for Jigawa state in fuel_expenditure_transformed in order to have 36 rows for both datasets and join them together. Then, we will convert the column “state_names” to row names for labeling the cluster leafs to be more meaningful. The code to achieve that is as follows;\n\n\ndataset_all <- subset.data.frame(fuel_expenditure_transformed, fuel_expenditure_transformed$state != 17) %>%\n  dplyr::left_join(health_expenditure_transformed) %>%  ## join the two datasets\n  tibble::column_to_rownames(var = \"state_names\") %>% ## transform variable state_names to rownames\n  select(-1) ## select all columns except the first one (state) which we don't need again\n\ndatatable(dataset_all)\n\n\n\nNow our data is ready for analysis!. But before then, we have to do clustering relevant data preparation\nClustering relevant data preparation\nThe data preparation done at this stage is not one that we will do for all types of analysis. We are doing them in order to successfully undertake a clustering analysis that is appropriate.\nData standardization: Before applying various clustering algorithms, we need to standardize the data to have mean zero and standard deviation one. This is done to make our variables have a common scale hence comparable.\nComputation of dissimilarity measure: This is done to identify observations that belong to the same clusters.\nNow, let’s write our code for that;\n\n\n## scale the data\n dataset_all_scaled <- scale(dataset_all)\n \n ## calculate dissimilarity \n dataset_dissimilarity <- dist(dataset_all_scaled, method = \"euclidean\")\n\n\nWe will use all the clustering methods we have discussed above and compare the results.\nAgglomerative Nesting (AGNES)\nAgglomerative nesting can be undertaken using hclust(), flashClust or agnes()\n\n\n## hierarchical clustering wtih complete linkage\nhcl_complete<-  hclust(dataset_dissimilarity, method = \"complete\")\n \nplot(hcl_complete, cex = 0.6, hang = -1) \n\n\n\nUsing agnes() function gives us the agglomerative co-efficient (ac) which measures the strength of clustering. The higher the ac, the stronger the clustering structure.\n\n\n## agnes clustering\nagnes_complete <- agnes(dataset_dissimilarity, method = \"complete\")\n\nplot(agnes_complete)\n\n\n\nNow, let’s look at other methods we have discussed for AGNES.\n\n\n## compute agnes clustering with various methods\nagnes_single <- agnes(dataset_dissimilarity, method = \"single\")\nagnes_average <- agnes(dataset_dissimilarity, method = \"average\")\nagnes_ward <- agnes(dataset_dissimilarity, method = \"ward\")\n\n#Plot results\nplot(agnes_single)\n\n\nplot(agnes_average)\n\n\nplot(agnes_ward)\n\n\n\nFrom the various methods, agnes with the ward method produces an ac of 0.9 which is the highest compared to others hence depicts the strongest structure.\nBefore we draw the curtains down on agnes, let’s also note that hcluster() function also accepts other methods\n\n\nhclust_mcquitty <- hclust(dataset_dissimilarity, method = \"mcquitty\")\nhclust_median <- hclust(dataset_dissimilarity, method = \"median\")\nhclust_centriod <- hclust(dataset_dissimilarity, method = \"centroid\")\n\nplot(hclust_mcquitty, cex = 0.6, hang = -1)\n\n\nplot(hclust_median, cex = 0.6, hang = -1)\n\n\nplot(hclust_centriod, cex = 0.6, hang = -1)\n\n\n\nDivisive clustering\nDivisive clustering does not require a method argument to be specified. let’s analyze the data using diana() function.\n\n\n#### divisive clustering\n# computes divisive hierarchical clustering\ndivisive_cluster <- cluster::diana(dataset_dissimilarity)\n\n# divisive coefficient -- amount of clustering structure found\ndivisive_cluster$dc\n\n[1] 0.8510366\n\n#plot dendrogram for divisive clustering\npltree(divisive_cluster, cex = 0.6, hang = .2, main = \"Dendrogram of diana\")\n\n\ndivisive_cluster$dc\n\n[1] 0.8510366\n\nThe divisive co-efficient (dc) is computed to be 0.85 which indicates strong clustering structure. The ac and dc are measured on a scale of 0 to 1 with lower values close to 0 indicating weak clustering and higher values close to 1 indicating strong cluster structure.\nIdentifying clusters of subgroups\nWe can identify observations that falls into similar groups or otherwise by stating the number of groups we want to cut into. let’s cut our observations into 5 clusters with the code below. This can be done for both agnes and divise\n\n\n# CUT TREE INTO 5 GROUPS\nhcl_complete_group <- cutree(hcl_complete, k = 5)\n\n### Number of members in each cluster\ntable(hcl_complete_group)\n\nhcl_complete_group\n 1  2  3  4  5 \n 8 20  5  2  1 \n\n##add cluster groupings to data\ndataset_all_cluster <- dataset_all%>%\n  mutate(cluster = hcl_complete_group) %>%\n  head()\n\n\nWe can visualize how the various states are grouped when we cut into 5 clusters using the code below\n\n\n## visualize with fviz_cluster\nfviz_cluster(list(data = dataset_all_scaled, cluster = hcl_complete_group))\n\n\n\nComparing dendrograms\nWe can compare clustering based on different methods by creating dendrogram objects for them and plotting.\n\n\n### create dendrogram object\nagnes_complete_dendrogram <- as.dendrogram(agnes_complete)\nagnes_ward_dendrogram <- as.dendrogram(agnes_ward)\n\n### plot the dendrograms to compare and match\ntanglegram(agnes_complete_dendrogram, agnes_ward_dendrogram)\n\n\n\nThe dashed lines show cluster combinations different from others.\nCustomizing the dendrogram\nVisualizing results has never been less important for cluster analysis where observations are grouped and there is the need for more colourful differentiation of groups to better understand the results presented. There are options for this. First, we can chain multiple dendrograms together to customize them easily.\n\n\n# create a dendlist\ndend_list_agnes_complete_ward <- dendlist(agnes_complete_dendrogram, agnes_ward_dendrogram)\n\ntanglegram(dend1 = agnes_complete_dendrogram, dend2 = agnes_ward_dendrogram,\n           highlight_distinct_edges = TRUE, # Turn off dashed lines\n           common_subtrees_color_lines = TRUE, # turn-off line colors\n           common_subtrees_color_branches = TRUE, # color common branches\n           highlight_branches_col = TRUE,\n           main_left = \"Agglomerative Nesting (complete method)\",\n           main_right = \"Agglomerative Nesting (ward method)\",\n           sub = \"Expenditure on Petrol, Kerosene and Health -States in Nigeria\",\n           cex_main_left = 1.1,\n           cex_main = 0.8,\n            cex_sub = .6,\n           lab.cex = 1,\n            margin_inner = 5,\n           main = paste(\"Entanglement = \", round(entanglement(dend_list_agnes_complete_ward), 2)))\n\n\nagnes_complete_dendrogram\n\n'dendrogram' with 2 branches and 36 members total, at height 6.648864 \n\nWe have added entanglement co-efficient to the graph. It measures how well-align the trees are on a scale of 0 to 1. The low co-oefficient of 0.17 indicates low entanglement with good alignment.\nDetermining optimal number of clusters\nA number of methods can be used to determine the optimal number of clusters for grouping\nElbow method of determing optimal number of clusters\nTo determine optimal number of clusters with elbow method, we need to look at where the bend is located which can be gauged to be 3 from our analysis below\n\n\nfviz_nbclust(dataset_all_scaled, FUN = hcut, method = \"wss\")\n\n\n\nAverage silhouette method\nAverage silhouette method indicates how well the clusters form with within cluster difference minimized at the optimal cluster number. Thus, the highest average silhouette width corresponds to the optimal number of clusters.\n\n\nfviz_nbclust(dataset_all_scaled, FUN = hcut, method = \"silhouette\")\n\n\n\nFrom the plot above, the optimal number of clusters is 3.\nGap statistics method\n\n\ndataset_gapstat <- clusGap(dataset_all_scaled, FUN = hcut, K.max = 10)\n\nfviz_gap_stat(dataset_gapstat)\n\n\n\nThe gaps method identified 1 to be the optimal number of clusters within a range of 1 to 10 clusters. Well, as we can deduce, different methods can identify different number of optimal clusters. With 2 of the methods suggesting 3 to be the optimal number of clusters, we can go by that.\nNow that we know the optimal number of clusters, let’s use that to visualize the data but in a way different from the previous.\n\n\n# Cut the dendrogram into 4 clusters\ncolors = c(\"brown\", \"orange\", \"#1DEfbb\", \"yellow\")\n\nward_cut <- cutree(hcl_complete, k = 3)\nplot(as.phylo(hcl_complete), type = \"fan\", tip.color = colors[ward_cut], label.offset = .5, cex = 0.7, no.margin = TRUE, use.edge.length = TRUE)\n\n\n\nConclusion\nWith this simple exercise, we learnt how to undertake hierarchical clustering analysis using a project that is typical of the real world. We went through process of data wrangling and transformation, clustering and determining the optimal number of clusters.\n\n\n\n",
    "preview": "posts/2022-07-20-hierarchical-clustering-which-states-in-nigeria-have-similar-expenditure-profile/hierarchical-clustering-which-states-in-nigeria-have-similar-expenditure-profile_files/figure-html5/customize-dendro-1.png",
    "last_modified": "2022-07-21T01:28:44+02:00",
    "input_file": "hierarchical-clustering-which-states-in-nigeria-have-similar-expenditure-profile.knit.md",
    "preview_width": 1536,
    "preview_height": 1152
  },
  {
    "path": "posts/2022-07-18-timeseries-benchmark-models/",
    "title": "Timeseries benchmark models",
    "description": "This post entails timeseries prediction with \nvery simple models that serve as benchmark against \nwhich more advance models .",
    "author": [
      {
        "name": "Linus Agbleze",
        "url": "https://agbleze.github.io/Portfolio/"
      }
    ],
    "date": "2022-07-18",
    "categories": [
      "Timeseries analysis"
    ],
    "contents": "\nTimeseries analysis\nThis post is on timeseries analysis, mainly the basics.\n\n\n\n",
    "preview": {},
    "last_modified": "2022-07-21T01:31:58+02:00",
    "input_file": "timeseries-benchmark-models.knit.md"
  },
  {
    "path": "posts/2022-07-17-data-visualization/",
    "title": "Data visualization",
    "description": "This post entails the use of ggplot for data visualization.",
    "author": [
      {
        "name": "Linus Agbleze",
        "url": "https://agbleze.github.io/Portfolio/"
      }
    ],
    "date": "2022-07-17",
    "categories": [
      "visualization"
    ],
    "contents": "\n\n\n\n\n\n\n",
    "preview": "posts/2022-07-17-data-visualization/data-visualization_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2022-07-20T03:40:04+02:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/welcome/",
    "title": "Welcome to Datasiast!",
    "description": "Datasiast exists for enthusiatic data science knowledge sharing and practice.",
    "author": [
      {
        "name": "Linus Agbleze",
        "url": "https://agbleze.github.io/Portfolio/"
      }
    ],
    "date": "2022-07-17",
    "categories": [],
    "contents": "\n\n\n\n",
    "preview": {},
    "last_modified": "2022-07-20T03:40:25+02:00",
    "input_file": {}
  }
]
