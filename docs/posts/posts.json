[
  {
    "path": "posts/2022-09-10-sales-hypothesis-analysis/",
    "title": "sales hypothesis analysis",
    "description": "A short description of the post.",
    "author": [
      {
        "name": "Linus Agbleze",
        "url": "https://agbleze.github.io/Portfolio/"
      }
    ],
    "date": "2022-09-10",
    "categories": [],
    "contents": "\nBackground\nIn most companies where new products need to be lauched, a great concern is whether it will fetch as much revenue as projected hence the need to investigate the all factors needed to promote sales.\n\n\n\n",
    "preview": {},
    "last_modified": "2022-09-11T23:55:58+02:00",
    "input_file": "sales-hypothesis-analysis.knit.md"
  },
  {
    "path": "posts/2022-09-05-dygraphs/",
    "title": "Interactive time series data visualization – Dygraphs in R",
    "description": "This post demonstrates how to undertake customized interactive visualization using time series data.",
    "author": [
      {
        "name": "Linus Agbleze",
        "url": "https://agbleze.github.io/Portfolio/"
      }
    ],
    "date": "2022-09-05",
    "categories": [
      "visualization",
      "time series"
    ],
    "contents": "\nIntroduction\nDygraph is a data visualization library originally written in Javascript and handles large datasets such as time series data really well. Among its many advantages that has attracted a large community of users and developers alike is interactivity.\nInteractive by default: With default interactivity enabled for mouse hovers to shows values, you don’t need to go through a hell of time to produce interactive charts. And yet if this is not enough, you are still covered by additional functions for the extra. Like many important visualization librabries, dygraphs has an R package which serve as an interface to the dygraph library. This means that, instead of worrying about learning javascript in order to start using dygraphs, we can just use our R programming knowledge. As usual, all we need is to install the package which in this case is Dygraphs. The dygraphs package is accessible from the CRAN repository. Afterwards, we have to load the dygraph library and we are good to go!\nThis post is a simple beginner-friendly tutorial to using dygraphs package in R and Rstudio for time series visualization. You will also learn the following along side; * Making an api call to retrieve data * Preparing the dataset for visualization in dygraphs\nDataset to be used\nNitrous Oxide data\nInput data format\ndygraph accepts time series data that are ts or xts objects.\nTools used for this tutorial\nR 4.1 and Rstudio version 1.4.1\nAt this point, introduction is enough, now its time to jump into our Rstudio and start coding. First, we install the packages needed if they are not already installed and load them as libraries for the work.\n\n\n## I have commented-out installation of packages because I have are already installed them locally\n# install.packages(\"magrittr\")\n# install.packages(\"rjson\")\n# install.packages(\"jsonlite\")\n# install.packages(\"dygraphs\")\n\n\nlibrary(magrittr)\nlibrary(rjson)\nlibrary(jsonlite)\nlibrary(dygraphs)  ## dygraphs \n\n\nRetrieving data for analysis\nWe will be using Nitrous Oxide data from global-warning.org. They have an API with an endpoint that returns json as response.\nIn order to download the data, we need to get API endpoint and assign it to a variable. Then, we will use fromJSON() function from the jsonlite package to download the data and convert it to dataframe. This can be achieve with the code below;\n\n\n## retrieve NO2 data from API\nno2_url <- \"https://global-warming.org/api/nitrous-oxide-api\"\nno2_json <- fromJSON(no2_url)\nno2_dataframe <- as.data.frame(no2_json)\n\n\nConvert data to time series\nNow that we have our data ready, we need to convert it to a time series object using the ts() function. Various arguments need to be passed to ts() in order to correctly define the period of occurrence of the series. The frequency argument indicates the number of observations made for the time period. For a monthly data recording, this will be 12 indicating 12 months in a year. The start argument is specified as a vector c(2001, 3) indicating that the first recording of the data was made on March, 2001.\nAfter converting the data to a time series object, its ready for plotting with dygraph( ). We want to plot the average monthly Nitrous Oxide emission which is the second column; so we will subset our data and pass it to dygraph( ) function. Because we want to demonstrate the use of various dygraph() elements by adding them incrementally, we will create a dygraph object by assigning the plot to a variable. The code below does just that.\n\n\nts_no2 <- ts(data = no2_dataframe, frequency = 12, start = c(2001, 3))\ndy_ts_no2 <- dygraph(ts_no2[, 2], main = \"Nitrous dioxide emissions\", xlab = \"Years\", \n                     ylab = \"Emissions in part per billion (ppb)\")\ndy_ts_no2\n\n\n\nFocusing on data highlights\nSometimes, we are not only interested in visualizing your data but also focusing on highlighting certain insights that are of interest to your audience. This can be achieved using a number of functions.\ndyAnnotation()\nThe dyAnnotation () function enables us to annonate our chart and create a tooltip that details the message or highlight we want to convey for that data point. The arguments of dyAnnonation () allows us to specify where to place the annotation on the chart (x argument), the label or annotation on the chart (text argument) and the message to convey when we hover over it (tooltip argument). When we hover over the point, it displays the message\ndyEvent()\nThe dyEvent( ) function as the name suggests is used to specified the time that an event occurred on the time series. This is usually done when we believe that event probably influenced a noticeable change in the time series and should noted. For instance, if we are visualizing sales data we notice a spike in sales on a particular day, it may be useful to use dyEvent( ) to indicate on the time series chart that it was Christmas day, a festival or a related event that cause it.\ndyLimit()\nThe dyLimit() function is similar to dyEvent() function but different in its usage and orientation. The dyLimit( ) can be used as a reference point on the time series based on some descriptive statistics such as the median value of the entire time series. For example, we can use the dyLimit() to position a horizon line at the mean value of Nitrous Oxide emissions and be able to discern which periods were above the mean emission.\ndyShading()\nThe dyShading() function is used to draw attention to a period or several periods on the time series data. For instance, if there is a period in time where an anomaly or change in trend seems to be captured in the time series and we want to highlight that; dyShading() will be of good use. dyShading() achieves this with the arguments from, to, and color.\nThe code for demonstrating this is below;\n\n\ndy_ts_no2 %>%\n  dyAnnotation(x = \"2019-12-1\", text = \"C2\", tooltip = \"COVID-19 emission period start\") %>%\n  dyAnnotation(x = \"2002-7-1\", text = \"C1\", tooltip = \"COVID-02 emission start\") %>%\n  dyShading(from = \"2005-1-1\", to = \"2010-1-1\", color = \"#FFECBB\") %>%\n  dyShading(from = \"2015-1-1\", to = \"2020-1-1\", color = \"#FCDBEE\") %>%\n  dyEvent(x = \"2008-1-1\", label = \"Global economic recession\", labelLoc = \"top\", \n          color = \"red\") %>%\n  dyLimit(limit = mean(ts_no2), label = \"Mean\", labelLoc = \"left\", color = \"blue\")\n\n\n\nOur time series plot above is seemingly crowded with the combination of various dygraph() functions for drawing focus to an aspect of the time series. Nonetheless, it enables us to communicate important insights using their combination. For example, we can deduce that during the global economic recession and period prior to 2010, Nitrous Oxide emissions were below the mean for the time series period. We can have indicators better than mean to make reference to. The message that remains is that a combination of dyLimit(), dyEvent() and dyShading() and possiblly dyAnnotation() functions enables better insights, that are not readily available visually, to be gained.\nFocusing on sections of time series interactively\nFor a large time series data covering a long time period, some trends and seasonality may be hidden visually or we may just be interested in allowing our audience the option to focus on time periods of interest to them. This can be achieved with the dyRangeSelector() function.\ndyRangeSelector( )\nThe dyRangeSelector() function accepts a dygraph or time series object as an argument and allows for various customization with other arguments. We can zoom-in to a time period by default using the dateWindow argument while allowing users to interactively choose other time periods. Among others, we can customize colors to reflect our desired aesthetics using the fillColor and strokeColor arguments.\nThe code below shows how to achieve that;\n\n\ndy_ts_no2 %>%\n  dyRangeSelector(dateWindow = c(\"2010-01-01\", \"2021-12-31\"), fillColor = \"#febacd\", strokeColor = \"#bffdea\")\n\n\n\nCustomizing time series charts\nWhile the various functions have a flair for customizing the outlook of charts, a full set of customization that meets the mock-up of a client will require a dedicated tutorial with discussions on css styling among others. To have digestible chunks of information, this tutorial will draw down the curtains with using dyOptions for customizing graphs and leave the use of css for another day (sooner than later). That being said, lets look at using dyOptions( ) function for dygraph customization.\nDyOptions() function accepts numerous arguments such as line colours, widths, labels among several others to be customized. In fact, the range of customization possible implies that they have to be used with care to prevent introducing noise into the data visualization.\nWe can use the code below to just demonstrate a limited selection of the options possible. In fact, the graph is not the best in terms of colour and width used. It is only for demonstration purposes and should even be seen to buttress the point that not all arguments can be used together for a visually appealing graph.\n\n\ndy_ts_no2 %>%\n  dyOptions(fillGraph = TRUE, stepPlot = T, drawPoints = T, colors = \"#abecbd\", axisLineColor = \"red\", axisLineWidth = 3, gridLineColor = \"gray\", axisLabelColor = \"#fbbdcc\")\n\n\n\nConclusion\nIn this tutorial we practiced how to use dygraphs package in R to visualize a time series data. The focus was only on a univariate dataset (a single variable being Nitrous Oxide emissions) so the type of plots produced were limited to that. Nonetheless, we used various functions that are relevant for other types of data and highlighted situations where they will be appropriate. The default interactivity that is available to us without stress and options to customize their looks proves worthwhile to invest our efforts in dygraphs as a time series visualization library.\nThis is just the part 1 of using dygraphs for time series visualization. In our next tutorial of these series, we will visualize multivariate time series datasets and also look at customization with css.\n\n\n\n",
    "preview": {},
    "last_modified": "2022-09-05T11:33:13+02:00",
    "input_file": "dygraphs.knit.md"
  },
  {
    "path": "posts/2022-09-05-linear-regression/",
    "title": "Analyzing the impact of various factors on apartment booking sales",
    "description": "This post uses linear regression to analyze how various factors influence booking sales.",
    "author": [
      {
        "name": "Linus Agbleze",
        "url": "https://agbleze.github.io/Portfolio/"
      }
    ],
    "date": "2022-09-05",
    "categories": [
      "Linear regression",
      "revenue impact assessment"
    ],
    "contents": "\nAnalyzing the impact of various factors on apartment booking sales\nA key question that is likely to be on the minds of the product team is how user interaction with our products will impact sales. This can be understond to be an early stage of trying to forecast sales. The available data can be used to demonstrate this.\nThe aim here is not to develop a well optimized model that predict sales considering that the dataset does not provide key indicators needed for such an analysis. Nonetheless, in order to draw a clue to such trend, this analysis proceeds to answer the following key questions\nWhat is the impact of user verification, instant booking, and the rcsp:show feature for the A/B testing on booking sales?\nTo what extent does these variables explain booking sales?\nProblem Design framework\nData analysis\nFor this analysis, the following variables were used\nOutcome variable\nTotal booking sales: This variable was extracted from the “params” column in the dataset as indicated by “total”.\nPredictors\nTest status: as a binary predictor is deduced from the A/B testing groupings in the data. All visitors identified with “rcsp:show” in the “params” column were classified as test group and all others as control group. For the analysis, the reference group was designated to be users without “rcsp:show”.\nUser verification is a binary predictor indicating whether a user has been verified or not. This is captured as “user_verified” in the “params” column. The reference group for the regression analysis for this variable was users who are not verified.\nInstant booking: is a binary predictor indicating whether or not a customer used the instant booking feature. Customers who used the instant booking feature were designated as the reference group for the analysis\n\n\nlibrary(readr)\nlibrary(tidyverse) # data manipulation and visualization\nlibrary(ggplot2)\nlibrary(GGally)\nlibrary(ggstatsplot)\nlibrary(plotly)\nlibrary(highcharter)\nlibrary(DT)\nlibrary(modelr)     # provides easy pipeline modeling functions\nlibrary(broom)      # helps to tidy up model outputs\nlibrary(car)  ## for regression\nlibrary(haven)\nlibrary(caret)\nlibrary(rsample)\n\n\n\n\nall_conversions_variables = read_csv(\"all_conversions_variables.csv\")\n\n\n\n\n##### Spliting dataset into training and testing samples\nset.seed(123)\nsample_all_conversions <- sample(c(TRUE, FALSE), nrow(all_conversions_variables), replace = T, prob = c(0.6,0.4))\ntrain_all_convern.lmvar <- all_conversions_variables[sample_all_conversions, ]\ntest_all_convern.lmvar <- all_conversions_variables[!sample_all_conversions, ]\n\n## regression for qualitative predictors (categorical dataset -- factors) \noptions('contrasts' = c('contr.treatment','contr.treatment') )\nmodel4_all_convern_factors <- lm(total_paid.EUR ~ test_status + instant_booking +\n                                    user_verified, data = train_all_convern.lmvar)\n\n\n# add model diagnostics to our training data\nmodel4_results <- augment(model4_all_convern_factors, train_all_convern.lmvar)\n\n\nPlot of regression analysis\n\n\nbooking_sales.predictioplot <- ggcoefstats(\n  x = stats::lm(formula = total_paid.EUR ~ test_status + instant_booking +\n                  user_verified, data = train_all_convern.lmvar),\n  ggtheme = ggplot2::theme_gray(), # changing the default theme\n  title = \"Regression analysis: Predicting the influence of various factors on booking sales\",\n) \n\nbooking_sales.predictioplot\n\n\n\n\n\nrsquare(model4_all_convern_factors, data = train_all_convern.lmvar)\n\n[1] 0.0416999\n\nrsquare(model4_all_convern_factors, data = test_all_convern.lmvar)\n\n[1] 0.009705932\n\nResult interpretation\nThe results shows that visitors in the test group(rcsp:show) made bookings worth 452 EUR less than what is ordered by the control group (not rcsp:show). Also, visitors who did not use instant booking made purchase worth 2,319 EUR more than those who used instant booking. Moreover, users who are verified made bookings worth 1,085 EUR more than those who are not verified. This result however suggests that there is no statistically significant difference in purchases among visitors based on the variables analyzed given that the p-value found was greater than 0.05 in all cases.\nMoreover, the predictors analyzed explains only 4% of variations in booking sales (rsquare = 0.0416). Thus, there is the need to consider other more important variables if we are too develop good models for forecasting booking sales.\n\n\n\n",
    "preview": "posts/2022-09-05-linear-regression/linear-regression_files/figure-html5/unnamed-chunk-4-1.png",
    "last_modified": "2022-09-09T23:43:12+02:00",
    "input_file": "linear-regression.knit.md",
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2022-09-04-logistics-regression/",
    "title": "Logistics regression: Assessing factors that inlfuence client conversion",
    "description": "This post uses logistic regression to support sales team to decipher the impact of various factors on their conversion rate.",
    "author": [
      {
        "name": "Linus Agbleze",
        "url": "https://agbleze.github.io/Portfolio/"
      }
    ],
    "date": "2022-09-04",
    "categories": [
      "logistic regression"
    ],
    "contents": "\nObjective 4: Assessing factors that influence the likelihood of a client to convert\nThe fourth objective of the product analysis is to move a step further and assess our data with focus on predicting how various factors influence the probability of a client signing up for our product. In order to achieve the goal of higher amount of conversion, as assessment is undertaken to model whether or not a client will sign-up. Knowing how these factors influence client’s decision to purchase our service will enable us better target clients by managing such predictors well. For this, a review of the various variables available has been undertaken to identify potential predictors.\nThe business problem at hand can be translated into an analytical one that requires a classification solution. In this case, the aim is to classify clients into two groups of convert and non-convert. This is the outcome we seek to predict hence the response or dependent variable. The response variable has two categories or classes as indicated, conversion and non-conversion, hence a regression analysis that caters for such characteristics is chosen.\nMethod: Logistic regression\nLogistic regression will be used for assessing how various factors influence the probability of a client deciding to convert.\nBased on the available limited data guided with domain knowledge of the business operations, the potential predictors identified for the analysis are the following\nType of sales script\nCompany type\nDescription of predictors and rational for their selection\nThe relationship between the type of sales script and conversion has been our main focus for earilier analysis and yet it is still important to understand how they influence the actual decision to convert and not the total number of conversion this time round. It can be presume, as it is the case to hypothesize, that there is a logical understanding of type of sales script being a potential influential determinant.\nCompany type as a potential predictor is based on the assumption that the characteristics of the company will determine its decision to finally purchase our product. Given that, these features are not adequately disclosed, the difference in labelling of company type is taken to be directly related to difference in company characteristics.\nData treatment / preprocessing\nSome predictors have receive a special focus different from earlier, in order to prepare the variables into an acceptable format for modeling. It is worthing highlight such instances for transparency and reproducability of the analysis.\nFirst of all, the company type variable had two categories being 0 and 1, and several missing data in the original dataset. In this case, it is possible that the missiing data was just an instance of unavailable data that was not provided due to some privacy concerns. Moreover, as much as 4,916 missing data is identified in the original dataset which constitutes a sizeable part of the dataset to eliminte from the analysis and very likely to reduce the possibility of capturing as much variations as the reality is on the ground. Supported by this, is the fact that, such a large amount of missing data is enough to constitute a category in its own right for analysis and insight. Based on these available information and context, the decision was made to replace all instances of missing data for company type with ‘Others’ as another company type. This led to reclassification of company type into company group A, B and Others corresponding to 0, 1 and missing data respectively;\n\n\n\n\n\n\n\n\n    convert not_convert \n       3001        2999 \n\n    convert not_convert \n       2017        1983 \n[1] \"Others\"    \"company_A\" \"company_B\"\n\nCall:\nglm(formula = is_signed ~ company_group + sales_script_type, \n    family = \"binomial\", data = train_data)\n\nDeviance Residuals: \n   Min      1Q  Median      3Q     Max  \n-1.316  -1.166   1.045   1.189   1.210  \n\nCoefficients:\n                          Estimate Std. Error z value Pr(>|z|)  \n(Intercept)               -0.02699    0.04103  -0.658   0.5107  \ncompany_groupcompany_A     0.04523    0.06780   0.667   0.5047  \ncompany_groupcompany_B     0.10317    0.05969   1.728   0.0839 .\nsales_script_typescript_B -0.04903    0.05534  -0.886   0.3756  \nsales_script_typescript_C  0.24365    0.23812   1.023   0.3062  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 8317.8  on 5999  degrees of freedom\nResidual deviance: 8312.9  on 5995  degrees of freedom\nAIC: 8322.9\n\nNumber of Fisher Scoring iterations: 3\n\nInsights from multiple logistic regression\nThe logistic regression for predicting the probability that a client will sign-up produced a non-significant p-value for all cases. Script A was used as reference point for script B and C. Thus, with a co-efficient of -0.049 for script B, it is suggested that using script B results in a decrease in the log odds of signing a client by 0.049 in comparison to script A. This is however statistically insignificant. Script C has a co-efficient of 0.24 which suggests that when a sales manager uses script C, the log odds of signing a client increases by 0.24 compared to script A which is statistically insignificant.\nWith regards to prioritizing companies to reach out to in order to increase conversion, when sales managers take a session with company A or company B, the log odds of landing a conversion increases compared to when they target companies categorized as ‘Others’ in this analysis. The result is however statistically insignificant.\nRecommendation for objective 4\nFrom the analysis undertaken, it is recommended that the sales team consider not prioritizing demo sessions with company type ‘Others’. This should however not be treated as a de facto discrimination favouring company A and B to increase conversion given that the results are statistically insignificant.\n\nfitting null model for pseudo-r2\n\n\n\n\n\n[[1]]\n[1] 0.5012221\n\ninteger(0)\n\n\n\n\n",
    "preview": "posts/2022-09-04-logistics-regression/logistics-regression_files/figure-html5/unnamed-chunk-6-1.png",
    "last_modified": "2022-09-04T14:16:25+02:00",
    "input_file": "logistics-regression.knit.md",
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2022-09-04-homelikeassignmentcode/",
    "title": "Product analysis",
    "description": "A short description of the post.",
    "author": [
      {
        "name": "Linus Agbleze",
        "url": "https://agbleze.github.io/Portfolio/"
      }
    ],
    "date": "2022-09-04",
    "categories": [
      "product analysis"
    ],
    "contents": "\nTask 1: What KPIs / Metrics do you recommend for the product team to use?\nKey Performance Indicators (KPIs) are design to monitor business goals and objectives and in this case product goals. By this, there is the need to first set business goals or objectives that need to be achieved. As a candidate for this position, I will proceed to set the following hypothetical goals. I am aware the business situation at Homelike may be different.\nThe main business goals considered in recommending the KPIs are as follows\n(i) To achieve an increase of 10% in monthly product sales\n(ii) To increase the number of new customers acquired by 5% every month\n(iii) To accelerate web traffic and customer outreach by 3% on monthly basis\nThe KPI recommended for the product team to monitor the achievement of the aforementioned hypothetical goals are as follows\nI. Conversion rate:\nThis should be defined to be purchase oriented. That is, a conversion is defined to have occurred when a visitor successfully book an apartment on Homelike webpage. The conversion rate will be the total bookings successfully requested divided by total number of sessions and then multiply the result by 100%. The rational behind recommending this KPI is to enable the product team assess the monetary value contribution of their product to the company as this is likely the primary means of achieving financial viability of Homelike. More importantly, conversion rate is likely to have a positive relationship with sales hence this KPI will enable the monitoring of our first business goal.\nII. Customer acquisition:\nCustomer acquisition is one of the key indicators for measuring market outreach which is related to sales. For Homelike, customers can be conceptualized to be online visitors who register on the platform in hopes of using the platform to book an apartment. Therefore, customer acquisition can be monitored using the number of people who register on the platform.\nIII. Total number of unique visitors: The number of visitors who access Homelike page will provide clues to its web traffic which has the potential of translating into sales. This KPI is also a measure for monitoring marketing efforts and popularity of the products\nIV. Average session per User: Average sessions per user helps monitor web traffic flow and a high average session per user indicates that several visitors are requesting multiple sessions hence continue to use our product overtime.\nV. Bounce rate: Bounce rate indicates single page view without further interaction with the product and this KPI should be monitored to keep it to the minimum possible. Bounce rate is likely to be negatively correlated with revenue and indicative of the user experience of the products.\n●What is the overall conversion rate on a given day for all users that visited our platform.\nData analysis\nThe analysis of the data was undertaken using R programming and Rstudio and several other packages.\nMethod\nIn order to estimate conversion rate, there is the need to define what is regarded as conversion for our products. After studying the dataset and trying to make meaning of the variables, I concluded that conversion is said to have occured when ‘page_type == request/success’. This means that a visitor’s request for booking an apartment has been successful hence shown the request/success webpage.\nThe day chosen for the analysis is 2021-07-18 hence the corresponding dataset bq-results-20210718.csv. The code for the analysis is provided below. First all packages are load and the data is clean and transformed for the analysis.\n\n\nlibrary(readr)\nlibrary(tidyverse) # data manipulation and visualization\nlibrary(ggplot2)\nlibrary(GGally)\nlibrary(ggstatsplot)\nlibrary(plotly)\nlibrary(highcharter)\nlibrary(cluster)  ### working with clusters\nlibrary(factoextra) ## cal and visualizing clusters\nlibrary(gridExtra) ## plotting multiple graphs\nlibrary(DT)\nlibrary(stringr)\nlibrary(stringi)\nlibrary(modelr)     # provides easy pipeline modeling functions\nlibrary(broom)      # helps to tidy up model outputs\nlibrary(car)  ## for regression\nlibrary(haven)\nlibrary(caret)\nlibrary(h2o)\nlibrary(rsample)\n\n\nNow the datasets are loaded. I achieved this using the code below;\n\n\nbq_results_20210718 <- read_csv(\"bq-results-20210718.csv\")\nbq_results_20210717 <- read_csv(\"bq-results-20210717.csv\")\nbq_results_20210716 <- read_csv(\"bq-results-20210716.csv\")\nbq_results_20210715 <- read_csv(\"bq-results-20210715.csv\")\nbq_results_20210714 <- read_csv(\"bq-results-20210714.csv\")\n\ndata14 <- bq_results_20210714\ndata15 <- bq_results_20210715\ndata16 <- bq_results_20210716\ndata17 <- bq_results_20210717\ndata18 <- bq_results_20210718\n\n\nI will first view the first 100 rows for the data using the code below\n\n\ndatatable(data18[1:100, ]) ## view the data \n\n\n\nI undertook data cleaning and omitted missing vales for the selected variables. Then, I analyzed the total number of unique visitors and sessions using the code below\n\n\n#### Total number of unique visitors\nunique_visitors_count<- data18 %>%\n  dplyr::select(visitor_id)%>%\n  na.omit()%>%\n  count(visitor_id)\n\n### Total number of unique sessions\nunique_session_count <- data18 %>%\n  dplyr::select(session_id)%>%\n  na.omit() %>%\n  count(session_id)\n\n\nThe total number of unique visitors is 17,453 and total number of unique sessions is 18,872.\nThe total number of unique sessions is required to calculate the conversion rate which is estimated with the code below;\n\n\n#############################################  Task 1:  #######################################################\n########### What KPIs / Metrics do you recommend for the product team to use?\n## Conversion rate\n## Conversion occurs when page_type == request/success\n# find number of conversions made\nrequest_success_data <- data18 %>%\n  dplyr::select(page_type)%>%\n  na.omit()%>%\n  filter(page_type == \"request/success\")\n## conversion rate\nconversion_rate <- (count(request_success_data)/count(unique_session_count)) * 100\nconversion_rate$n  ## conversion rate is 0.159 %\n\n[1] 0.1589657\n\nTherefore, the overall conversion rate for 2021-07-18 is 0.159%\nBefore, proceeding with the next question, I will demonstrate how to estimate some of KPIs I have recommended in my first question using the code below;\nBounce_rate om 2021-07-18\n\n\n## Bounce_rate\nbounce <- data18%>%\n  dplyr::select(session_id, page_type)%>%\n  na.omit()%>%\n  group_by(session_id)%>%\n  count(page_type)%>%\n  tally(wt = n)%>%\n  mutate(num_pages = n)%>%\n  mutate(bounce_status = case_when(num_pages == 1 ~ \"bounce\",\n                                   num_pages > 1 ~ \"not_bounce\"))\n\nbounce_group <- bounce%>%\n  group_by(bounce_status)%>%\n  count(bounce_status)\n\n## bounce rate\nbounce_rate <- (bounce_group[1,2]/sum(bounce_group$n)) * 100\nbounce_rate$n  ## 13.54% bounce rate\n\n[1] 13.53858\n\nFrom the above analysis, it is concluded that bounce rate is 13.54% on 2021-07-18\nCustomer acquisition as KPI\nFor the customer acquisition KPI that I recommended, the code below can be used to estimate it.\n\n\n## Customer acquisition as KPI \ncustomer_acquisition <- data18%>%\n  dplyr::select(event_type) %>%\n  na.omit()%>%\n  filter(event_type == \"user_register_success\")\ncustomer_acquisition_count<- count(customer_acquisition)   ## 76 new users register for our services\ncustomer_acquisition_count$n\n\n[1] 76\n\nIt is estimated that 76 new users register for our services on 2021-07-18. This represents our customer acquisition.\n●What is the conversion rate for users searching in Berlin.\nIn order to estimate conversion rate for users searching in Berlin, we need to filter the dataset to focus on Berlin users, then estimate the number of sessions and conversion using the code below. This is done for 2021-07-18.\n\n\n########## ●What is the conversion rate for users searching in Berlin.\n## select users searching in Berlin\nberlin_users <- data18%>%\n  filter(user_location_city == \"Berlin\")%>%\n  dplyr::select(session_id, page_type) \n  \n  \nberlin_conversion <- berlin_users%>%\n  filter(page_type == \"request/success\")\n\nberlin_session <- berlin_users%>%\n#  unique(berlin_users$session_id)%>%\n    count(session_id)\n\nberlin_session_count <- count(berlin_session) \nberlin_session_count$n ## Total number of sessions from Berlin is 804\n\n[1] 804\n\nberlin_conversion_rate <- (count(berlin_conversion) / count(berlin_session))\nberlin_conversion_rate$n ### conversion rate in Berlin is 0%\n\n[1] 0\n\nFrom the analysis above, it is clear that there were no conversions on 2021-01-18 for users searching in Berlin despite 804 sessions were made.\nTask 2: Analyse\n●Analyse results of our “rcsp’ A/B test and present results to the product team.\nBackground\nIn order to provide a baseline for the A/B testing, I defined goals for the product. It is assumed that we are striving to assess which of our product user groups will help as achieve our goals better. I translated these goals into KPIs to be measured for the A/B testing.\nKPI compared for the various groups\nI. Conversion rateII.User journey\nData analysis\nSplitting of data into test and control group\nFrom the “test_groups” column, I identified “rcsp=ref” as the control group and “rcsp=show” as the test group. The KPI identified for the analysis was used as a benchmark to analyze the test group and control group. The code below was used to analyze the data.\nConversion rate based on A/B testing\n\n\n################################     Task 2: Analyse  ################################################\n########## ●Analyse results of our “rcsp’ A/B test and present results to the product team. \n####   ○The “rcsp=show” test users of page_type=search_page experience a faster loading time of the page. \n\n## divide data into test group and control group\n# subset test group\nrcsp_show <- subset(data18, grepl('\"rcsp\":\"show\"', test_groups))\n# subset control group\nrcsp_ref <- subset(data18, grepl(pattern = '\"rcsp\":\"ref\"', test_groups))\n\n######## conversion rate for control\nrcsp_ref_conversion <- rcsp_ref%>%\n  filter(page_type == \"request/success\") %>%\n  na.omit()\nrcsp_ref_conversion_count <- count(rcsp_ref_conversion) \nrcsp_ref_conversion_count$n   ## Conversions for control group is 19\n\n[1] 19\n\n## number of sessions made by rcsp_ref\nrcsp_ref_session <- rcsp_ref%>%\n  dplyr::select(session_id)%>%\n  na.omit()%>%\n  count(session_id)\n\nrcsp_ref_session_count <- count(rcsp_ref_session) \nrcsp_ref_session_count$n # # 9467 sessions for rcsp_ref \n\n[1] 9467\n\nrcsp_ref_conversion_rate <- (count(rcsp_ref_conversion)/count(rcsp_ref_session)) * 100\nrcsp_ref_conversion_rate$n ## 0.2% conversion rate for rcsp_ref\n\n[1] 0.2006972\n\n######### conversion rate for test group rcsp:show\nrcsp_show_conversion <- rcsp_show%>%\n  dplyr::select(page_type, session_id)%>%\n  na.omit()%>%\n  filter(page_type == \"request/success\")\nrcsp_show_conversion_count <- count(rcsp_show_conversion)  \nrcsp_show_conversion_count$n  ## conversion for test group is 11\n\n[1] 11\n\n## number of sessions for rcsp_show\nrcsp_show_session <- rcsp_show%>%\n  dplyr::select(session_id, page_type)%>%\n  na.omit() %>%\n  count(session_id)\nrcsp_show_session_count <- count(rcsp_show_session) \nrcsp_show_session_count$n  ## 9,463 sessions were made by  rcsp_show test group\n\n[1] 9462\n\n## conversion rate rcsp_show\nrcsp_show_conversion_rate <- (count(rcsp_show_conversion)/count(rcsp_show_session)) * 100\nrcsp_show_conversion_rate$n ## 0.116 % conversion rate for rcsp_show\n\n[1] 0.1162545\n\n###### Thus the test group ( test_groups == rcsp:show) achieved a lower conversion rate compared to the\n## control group (test_groups == rcsp:ref )\n\n\nFrom the above analysis of A/B testing, it was estimated that the control group (rcsp:ref) had 19 conversions and 9467 user sessions which translates into a conversion rate of 0.2%.\nFor the test user group (rcsp:show), there were 11 conversions and 9,462 user sessions hence a conversion rate of 0.116%\nThus, it is concluded that based on conversion rate the test user group had a lower conversion rate (0.116%) compared to the control group (0.2%). Therefore, if we are to solely base our decision on conversion, the new feature which is shown to the test group is not recommended.\nBounce rate (A/B testing)\nA key concern for the product team will be to ensure that they do not roll out a feature that has detrimental effect on user engagement and experience hence bounce rate. Thus, the A/B testing could also be undertaken based on bounce rate.\nAssessing bounce rate, it was estimated that the control user group (rcsp:ref) had a bounce rate of 13.62% while the test user group (rcsp:show) had 13.59 % bounce rate. Thus, in terms of bounce rate, the test group had a slightly lower bounce rate compared to the control group. The result is however not conclusive of which feature is better as there is the need to conduct further analysis to determine whether the difference is statistically significant.\nThe code for this analysis is provided below\n\n\n#### bounce rate for A/B testing\n## Bounce_rate for rcsp_ref\nbounce_rcsp_ref <- rcsp_ref %>%\n  dplyr::select(session_id, page_type)%>%\n  na.omit()%>%\n  group_by(session_id)%>%\n  count(page_type)%>%\n  tally(wt = n)%>%\n  mutate(num_pages = n)%>%\n  mutate(bounce_status = case_when(num_pages == 1 ~ \"bounce\",\n                                   num_pages > 1 ~ \"not_bounce\"))\n\nbounce_group_rcsp_ref <- bounce_rcsp_ref%>%\n  group_by(bounce_status)%>%\n  count(bounce_status)\n\n## bounce rate for rcsp_ref\nbounce_rate_rcsp_ref <- (bounce_group_rcsp_ref[1,2]/sum(bounce_group_rcsp_ref$n)) * 100\nbounce_rate_rcsp_ref$n  ## 13.61572% bounce rate\n\n[1] 13.61572\n\n#####  Bounce_rate for A/B testing (test group)\n## Bounce rate rcsp_show\nbounce_rcsp_show <- rcsp_show%>%\n  dplyr::select(session_id, page_type)%>%\n  na.omit()%>%\n  group_by(session_id)%>%\n  count(page_type)%>%\n  tally(wt = n)%>%\n  mutate(num_pages = n)%>%\n  mutate(bounce_status = case_when(num_pages == 1 ~ \"bounce\",\n                                   num_pages > 1 ~ \"not_bounce\"))\n\nbounce_group_rcsp_show <- bounce_rcsp_show%>%\n  group_by(bounce_status)%>%\n  count(bounce_status)\n\n## bounce rate for rcsp_show\nbounce_rate_rcsp_show <- (bounce_group_rcsp_show[1,2]/sum(bounce_group_rcsp_show$n)) * 100\nbounce_rate_rcsp_show$n  ## 13.59 % bounce rate for rcsp_show\n\n[1] 13.59121\n\nUser journey for A/B testing\nGiven that the difference in bounce rate between the two groups is a very small margin, further inquiry will be to analyze the user journey and display the result using a funnel chart. This will enable visualizing pages where users leave Homelike website and get a sense of which group makes is closer to final conversion.\nThe code for user journey for A/B testing is provided below.\n\n\n####### User journey for A/B testing\n##### user journey for user_ref\nrcsp_ref_user_jour <- rcsp_ref%>%\n  dplyr::select(page_type)%>%\n  na.omit()%>%\n  group_by(page_type)%>%\n  count()\n\n## user journey rcsp_show \nrcsp_show_user_jour <- rcsp_show%>%\n  dplyr::select(page_type)%>%\n  na.omit()%>%\n  group_by(page_type)%>%\n  count()\n\nrcsp_ref_user_jour.desc<- dplyr::arrange(rcsp_ref_user_jour, desc(n) )\nrcsp_show_user_jour.desc <- dplyr::arrange(rcsp_show_user_jour, desc(n))\n\nrcsp_funl <- plot_ly(\n  type = \"funnel\",\n  name = 'rcsp:ref (control group)',\n  y = as.vector(rcsp_ref_user_jour.desc$page_type),\n  x = as.vector(rcsp_ref_user_jour.desc$n),\n  textinfo = \"value+percent initial\") \n\n\n\n\nrcsp_funl <- rcsp_funl %>%\n  add_trace(\n    type = \"funnel\",\n    name = 'rcsp:show (test group)',\n    orientation = \"h\",\n    y = as.vector(rcsp_show_user_jour.desc$page_type),\n    x = as.vector(rcsp_show_user_jour.desc$n),\n    textposition = \"inside\",\n    textinfo = \"value+percent initial\") \nrcsp_funl <- rcsp_funl %>%\n  layout(yaxis = list(categoryarray = as.vector(rcsp_show_user_jour.desc$page_type)))%>%\n  layout(hovermode = 'compare')\n\nrcsp_funl\n\n\n\n●Cluster our users in logical groups based on the data you have in hand.\nThere are a number of ways to segment our users into groups. The first approach will be to simply group users based on features that they have in common and second approach will be to use a clustering algorithm such as k-means clustering to segment them into groups of similarity.\nI will first visualize the simple groupings using the code below. This funnel chart for user journey is based on dataset of 2021-07-18 and not just the A/B testing groups as above. Hover of the funnel chart to display number of users for each page.\n\n\n##########   ●Cluster our users in logical groups based on the data you have in hand. \n\n## funnel charts for user journey\npage_type_count <- data18%>%\n  dplyr::select(page_type)%>%\n  na.omit()%>%\n  group_by(page_type)%>%\n  count()%>%\n  arrange(desc(n))\n\nuser_jour_funnel <- page_type_count %>%\n  hchart(\n    \"funnel\", hcaes(x = page_type, y = n),\n    name = \"user_journey\"\n  )\nuser_jour_funnel\n\n\n\nGrouping users based on device type\nFor this, I analyzed the number of visitors based on device used to access Homelike website using the code below.\n\n\n### clusters users into groups based on device type\nusers_device_class <- data18%>%\n  dplyr::select(visitor_id, device_class)%>%\n  na.omit()%>%\n  group_by(device_class)%>%\n  distinct(visitor_id)%>%\n  count(visitor_id)%>%\n  tally(wt = n)%>%\n  arrange(desc(n)) %>%\n  dplyr::rename(\"Number of users\" = n)\n\nggplot(data = users_device_class, mapping = aes(x = reorder(device_class, -`Number of users`), y = `Number of users`)) + geom_col() + ggtitle(label = \"Number of users based on devices used\") + xlab(\"Device class\")\n\n\n\nCluster users into groups based on user country\nBased on the country users access Homelike page from, the total number of users is analyzed and the top twenty (20) countries with highest number of visitors are visualized.\n\n\n### clusters users into groups based on user country \nusers_country <- data18%>%\n  dplyr::select(visitor_id, user_location_country)%>%\n  na.omit()%>%\n  group_by(user_location_country)%>%\n  distinct(visitor_id)%>%\n  count(visitor_id)%>%\n  tally(wt = n)%>%\n  arrange(desc(n)) %>%\n  dplyr::rename(\"Number of users\" = n)\n\ntop20_users_country <- dplyr::top_n(users_country, n = 20)\n\nggplot(data = top20_users_country, mapping = aes(x = reorder(user_location_country, -`Number of users`), y = `Number of users`)) + geom_col() + ggtitle(\"Top 20 countries with highest number of visitors\") + xlab(\"Countries of users\")\n\n\n\nCluster users into groups based on device brower used\n\n\n### clusters users into groups based on device brower used\nusers_browser <- data18%>%\n  dplyr::select(visitor_id, device_browser)%>%\n  na.omit()%>%\n  group_by(device_browser)%>%\n  distinct(visitor_id)%>%\n  count(visitor_id)%>%\n  tally(wt = n)%>%\n  arrange(desc(n)) %>%\n  dplyr::rename(\"Number of users\" = n)\n\nggplot(data = top_n(users_browser, n =5), mapping= aes(x = reorder(device_browser, -`Number of users`), y = `Number of users`)) + geom_col() + ggtitle(\"Number of users based on browser used\") + xlab(\"Type of brower used\")\n\n\n\nK-means clustering to cluster users into groups of similarity\nA more robust way to segment users into groups of similarity involves using K-means clustering analysis. This method have been employed here to segment our users to groups. The number of groups to segment users into can vary. In order to choose the optimal number of groups for the clustering, I used average silhouettes method to determine the optimal group number.\nThe analysis was undertaken by first assessing the number of page views per user and sessions per users; and using these as the criteria to cluster users into groups of similarity.\nIt was assessed that optimal group to clusters into groups of similarity was 2 as deduced from average silhouette method.\nThe code below was used for the analysis. First, the number of pageview and sessions per user is analyzed.\n\n\n####################### cluster analysis\n## pageviews per visitor\nuser_pageviews <- data18%>%\n  dplyr::select(visitor_id,page_type)%>%\n  na.omit()%>%\n  group_by(visitor_id)%>%\n  count(page_type)%>%\n  tally(wt = n)%>%\n  mutate(num_pages = n)%>%\n  dplyr::select(-2)\n\n## number of sessions per visitor\nuser_sess <- data18%>%\n  dplyr::select(visitor_id, session_id) %>%\n  na.omit()%>%\n  group_by(visitor_id)%>%\n  distinct(session_id)\n\nuser_sess_num <- user_sess%>%\n  group_by(visitor_id)%>%\n  count()%>%\n  mutate(num_sessions = n)%>%\n  dplyr::select(-2)\n\n### merge num_pages and num_sessions for visitors\nusers_all <- merge(user_pageviews, user_sess_num)\nvisitor_pageviews_sessions_num <- merge(user_pageviews, user_sess_num)\n\n\nAfter preparing the variables, they are scaled, their distance measure estimated and clustering analysis undertaken. For this analysis, outliers were not remove and may influence the result even though scaling and standardization was undertaken.\n\n\n## k-mean clustering\nvisitor_pageviews_sessions_num <- na.omit(visitor_pageviews_sessions_num)\n## scale data\nvisitor_pageviews_sessions_num <- scale(visitor_pageviews_sessions_num[,c(2:3)])\n\n##### K means clustering\nk2_visitor <- kmeans(visitor_pageviews_sessions_num, centers = 2, nstart = 25)\n\nk3_visitor <- kmeans(visitor_pageviews_sessions_num, centers = 3, nstart = 25) ## kmeans with 3 clusters\nk4_visitor <- kmeans(visitor_pageviews_sessions_num, centers = 4, nstart = 25) ## kmeans with 4 clusters\nk5_visitor <- kmeans(visitor_pageviews_sessions_num, centers = 5, nstart = 25) ## kmeans with 5 clusters\n\n# plots to compare\np1 <- fviz_cluster(k2_visitor, geom = \"point\", data = visitor_pageviews_sessions_num) + ggtitle(\"k = 2\")\np2 <- fviz_cluster(k3_visitor, geom = \"point\", data = visitor_pageviews_sessions_num) + ggtitle(\"k = 3\")\np3 <- fviz_cluster(k4_visitor, geom = \"point\", data = visitor_pageviews_sessions_num) + ggtitle(\"k = 4\")\np4 <- fviz_cluster(k5_visitor, geom = \"point\", data = visitor_pageviews_sessions_num) + ggtitle(\"k = 5\")\n\n############# plot all graphs together  #######\ngrid.arrange(p1, p2, p3, p4, nrow = 2)\n\n\n\nDetermining the optimal number of clusters for grouping users\nThe code below was used for the analysis. Inferred from the average silhouette method, the optimal number of clusters for grouping users is 2.\n\n\n##Determing optimal number of clusters using average silhouettes method \nsilh_visitors <- fviz_nbclust(visitor_pageviews_sessions_num, kmeans, method = \"silhouette\")\nsilh_visitors\n\n\n\nClustering users into 2 groups as the optimal number of cluster groups is shown below\n\n\n#####  compute k-means clustering with k = 2  // the optimal clusters according to silhoutte\nset.seed(123)\noptimal_clusters_visitors <- kmeans(visitor_pageviews_sessions_num, 2, nstart = 25)\n\noptimal_clust_viz <- fviz_cluster(optimal_clusters_visitors, data = visitor_pageviews_sessions_num)\noptimal_clust_viz\n\n\n\nThe cluster group for each user is identified by adding the clustering result to the data. The first 50 users are shown below.\n\n\n###extracting clustering and adding to data\nuser_cluster_groups <- users_all%>%\n  mutate(Cluster_group = optimal_clusters_visitors$cluster) %>%\n  group_by(Cluster_group) #%>%\n#  summarise_all(\"mean\")\nDT::datatable(user_cluster_groups[1:50,])\n\n\n\nNOTE: From the data, it is deduced that most of the first 50 users are in cluster group 2 and indeed only a few users are in cluster group one as a whole. This is likely the influence of outliers since outliers were not removed and some users had an extermely high number of pageviews and sessions. This could be an error in the data collection process.\n\n\n\n",
    "preview": "posts/2022-09-04-homelikeassignmentcode/homelikeassignmentcode_files/figure-html5/unnamed-chunk-14-1.png",
    "last_modified": "2022-09-05T01:50:08+02:00",
    "input_file": "homelikeassignmentcode.knit.md",
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2022-07-18-timeseries-benchmark-models/",
    "title": "Timeseries benchmark models",
    "description": "This post entails timeseries prediction with \nvery simple models that serve as benchmark against \nwhich more advance models .",
    "author": [
      {
        "name": "Linus Agbleze",
        "url": "https://agbleze.github.io/Portfolio/"
      }
    ],
    "date": "2022-07-18",
    "categories": [
      "Timeseries analysis"
    ],
    "contents": "\nTimeseries analysis\nThis post is on timeseries analysis, mainly the basics.\n\n\n\n",
    "preview": {},
    "last_modified": "2022-07-21T01:32:34+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-07-20-hierarchical-clustering-which-states-in-nigeria-have-similar-expenditure-profile/",
    "title": "Hierarchical clustering -- Which states in Nigeria have similar expenditure profile?",
    "description": "Clustering analysis is popular unsupervised machine \nlearning techniques that categorize data into groups of similarity.",
    "author": [
      {
        "name": "Linus Agbleze",
        "url": "https://agbleze.github.io/Portfolio/"
      }
    ],
    "date": "2022-07-18",
    "categories": [
      "Unsupervised learning"
    ],
    "contents": "\nIntroduction to clustering analysis\nClustering analysis is one of the important exploratory data analysis you will consider undertaking in order to identify groupings in your dataset. It belongs to the segment of unsupervised classification which draws the data into clusters of similarity. Infact, many classification schemes have this underpinning of classifying a large set of individual elements or observations into a small number of classes. Talk of species classification, the periodic table, development index among others. Remember, it is groups of similarity. This notion also runs in business or rather dominates business decisions. Differential pricing, product and market segmentation among others require identifying the different groups present and tailoring policies that best serve each of them. So clustering analysis is important there!\nThat being said, there are typically two set of clustering techniques that are use to identify groups of similarity. Namely;\nHierarchical clustering\nK-means clustering\nIn order to do a little deep dive, we will take them in turns. This tutorial will focus on hierarchical clustering where a tree structure is produce for the clusters – dendrograms.\nHierarchical clustering analysis\nHierarchical clustering is of two types; agglomerative nesting and divisive analysis. The two are oppose in terms of where formation of clusters begin from.\nAgglomerative Nesting (AGNES) merges similar clusters beginning from the bottom-most, into nodes and in a sequential manner continue merging them until all clusters form a single root at the top of the tree.\nDivisive clustering also known as DIANA (Divise Analysis) runs in opposite direction to AGNES. The algorithm starts by disaggregating the root cluster, which is the single cluster containing all other clusters, into dissimilar clusters at each step (with each cluster being internally similar) until all the dissimilarity between clusters are completely exhausted. Thus, it ends with the leafs, which are clusters at the bottom, entirely dissimilar to each other.\nSo AGNES adds up similar cluster leafs to form a cluster node and then a root while DIANA divides a single cluster root into dissimilar cluster nodes and then finally cluster leafs. Thus, AGNES is favoured for finding small clusters while DIANA works well for large clusters.\nWorkflow for hierarchical clustering analysis\nIn order to undertake hierarchical clustering, the data has to be cleaned and prepared for that purpose such that variables are comparable. This involve the following;\nData wraggling and transformation that results in a tidy data such that;\nEach column is a variable\nEach row is a recording or observation\nEach cell is a data\n\nStandardizing / scaling the data\nCalculating dissimilarity measure between clusters\nIn measuring dissimilarity between clusters, different methods can be use and are required to be specified when using AGNES. The methods are identified as follows;\nMaximum / complete linkage: uses the highest dissimilarity found between clusters after pairwise calculation of dissimilarities of elements in different clusters.\nMinimum / single linkage: uses the least dissimilarity between clusters following similar method as complete linkage in calculating dissimilarity\nMean / average linkage: as the name suggests, uses average dissimilarity as distance between clusters after computing pairwise dissimilarity between observations in different clusters.\nCentroid linkage: based on the dissimilarity between the centroid of clusters.\nWard’s minimum variance: focuses on minimizing inward cluster difference by merging groups with least between-cluster distance.\nThe best way to understand the workflow and methods identified for computing hierarchical clustering is by practicing with a project. We are going to define a simple research that clustering analysis can be used for and go through the complete pipeline required to execute it.\nSo let’s take a break from the long discussion and start with practicals.\nResearch objective\nTo identify states in Nigeria with similar expenditure profile on petrol, kerosene and health (excluding insurance)\nDataset\nThe dataset to be used for this tutorial is Living Standard Measurement Study (LSMS) for Nigeria. Specifically, we will use the General Household Survey, 2015-2016 Panel data which is the Wave 3. The dataset can be accessed from (https://microdata.worldbank.org/index.php/catalog/2734/related-materials).\nKnow the Metadata of your data\nAs part of most data science projects in the real world, background checks has to be done even before data exploration and analysis. We have covered our first step which is accessing the data. The data can even be provided to us by our client or project partner or whoever we are working for. But one thing that we will have to do ourselves is to familiarize ourselves with variable names used, questionnaire administered and its design, how data was collected, units of measurement for the various variables among several others. Understanding these elements will influence how we explore the data, organize it in terms of merging datasets and variables, and analyze it.\nTo do this for our dataset, we need to look at the documentation of the dataset which is also available on the site (https://microdata.worldbank.org/index.php/catalog/2734/data-dictionary).\nAfter familiarizing ourselves with the metadata files, it is clear that the variables we want to analyze are in two different files namely sect8b_plantingw3.csv (contains expenditure on various fuels including kerosene and petrol among others) and sect8c_plantingw3.csv (contains expenditure on health). The dataset create opportunity for us to learn key skills of data wrangling and transformation before the actual clustering analysis. This is typical of many projects.\nProject Tasks\nData wrangling and transformation\nSubseting dataset\nfiltering dataset\ngrouping dataset\nsummarizing\n\nScaling dataset\nHierarchical clustering\nAGNES clustering\nDIANA clustering\n\nDetermining optimal clusters\nCustomizing visualization of clusters\nPackages and libraries\nNow that we know the tasks we will be undertaking, we have to install (if not already installed) and load the libraries needed\n\n\n## install librabries if not already available\n### I have commented the code for installation because I have already installed them\n# install.packages(\"tidyverse\")\n# install.packages(\"cluster\")\n# install.packages(\"factoextra\")\n# install.packages(\"dendextend\")\n# install.packages(\"DT\")\n# install.packages(\"skimr\")\n# install.packages(\"dplyr\")\n# install.packages(\"dendextend\")\n# install.packages(\"ape\")\n\n## load librabries\nlibrary(readr)  ## to read csv file\nlibrary(tidyverse) ## to tidy the data\nlibrary(cluster) ##  for cluster analysis\nlibrary(factoextra) ## to visualize clusters\nlibrary(dendextend)  ## to compare dendrograms\nlibrary(stats)  ## for statistical analysis\nlibrary(DT)  ## to present beautiful tables\nlibrary(skimr)  ## for quick descriptive view of the dataset\nlibrary(ape)  ## to visualize dendrograms\n\n\nReading the dataset\nThe first thing to do is to import the dataset (hopefully you have downloaded them) and get a general understanding of it. This can be achieve with the code below\n\n\nfuel_expenditure <- read_csv(\"sect8b_plantingw3.csv\")  ## load the dataset\nhealth_expenditure <- read_csv(\"sect8c_plantingw3.csv\") ## load the dataset\n\nskim(fuel_expenditure)  ## get data summary\n\nTable 1: Data summary\nName\nfuel_expenditure\nNumber of rows\n137729\nNumber of columns\n10\n_______________________\n\nColumn type frequency:\n\ncharacter\n1\nnumeric\n9\n________________________\n\nGroup variables\nNone\nVariable type: character\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\nitem_desc\n0\n1\n3\n30\n0\n30\n0\nVariable type: numeric\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\nzone\n0\n1.00\n3.52\n1.71\n1\n2\n3\n5\n6\n▇▅▅▅▅\nstate\n0\n1.00\n18.57\n10.36\n1\n10\n19\n28\n37\n▇▆▇▇▇\nlga\n0\n1.00\n1868.92\n1035.81\n102\n1001\n1921\n2806\n3706\n▇▆▇▇▇\nsector\n0\n1.00\n1.68\n0.47\n1\n1\n2\n2\n2\n▃▁▁▁▇\nea\n0\n1.00\n993.11\n932.12\n0\n330\n760\n1390\n7586\n▇▂▁▁▁\nhhid\n0\n1.00\n185571.19\n103683.68\n10001\n90124\n190093\n280038\n370040\n▇▆▇▇▇\nitem_cd\n0\n1.00\n315.50\n8.66\n301\n308\n315\n323\n330\n▇▇▇▇▇\ns8q3\n2\n1.00\n1.82\n0.39\n1\n2\n2\n2\n2\n▂▁▁▁▇\ns8q4\n112727\n0.18\n1911.06\n43304.31\n2\n250\n600\n1500\n6584185\n▇▁▁▁▁\n\nskim(health_expenditure) ## get data summary\n\nTable 1: Data summary\nName\nhealth_expenditure\nNumber of rows\n188230\nNumber of columns\n10\n_______________________\n\nColumn type frequency:\n\ncharacter\n1\nnumeric\n9\n________________________\n\nGroup variables\nNone\nVariable type: character\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\nitem_desc\n0\n1\n6\n30\n0\n41\n0\nVariable type: numeric\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\nzone\n0\n1.00\n3.52\n1.71\n1\n2\n3\n5\n6\n▇▅▅▅▅\nstate\n0\n1.00\n18.57\n10.36\n1\n10\n19\n28\n37\n▇▆▇▇▇\nlga\n0\n1.00\n1868.92\n1035.80\n102\n1001\n1921\n2806\n3706\n▇▆▇▇▇\nsector\n0\n1.00\n1.68\n0.47\n1\n1\n2\n2\n2\n▃▁▁▁▇\nea\n0\n1.00\n993.11\n932.12\n0\n330\n760\n1390\n7586\n▇▂▁▁▁\nhhid\n0\n1.00\n185571.20\n103683.48\n10001\n90124\n190093\n280038\n370040\n▇▆▇▇▇\nitem_cd\n0\n1.00\n421.00\n11.83\n401\n411\n421\n431\n441\n▇▇▇▇▇\ns8q5\n0\n1.00\n1.84\n0.36\n1\n2\n2\n2\n2\n▂▁▁▁▇\ns8q6\n158505\n0.16\n2822.90\n6896.99\n1\n580\n1500\n3000\n300000\n▇▁▁▁▁\n\nThe skim() function gives us an overview of the dataset. It shows that there are 10 columns in both the fuel_expenditure and health_expenditure data objects. Among others, it gives the mean and number of missing cases for each variable. Depending on the type of variable, that is whether character or numeric, we get different type of summary. Some of the variables are not in their right data type format. Example, zone, state and lga are numeric instead of being character but that is not the focus here. The variable s8q4 from fuel_expenditure data object has 112727 missing records.\nWe keep saying the variable and that brings up the question, what does the variable actually mean? This is where our earlier exercise of viewing the metadata plays its role. If we indeed went through the various documentations and questionnaire that comes with the data, we will realize that variable s8q4 (fuel_expenditure data) represents household monthly expenditure measured in Naire for various items identified in item_desc variable. Likewise, s8q6 (health_expenditure data) represents expenditure for items but this time round it covers 6 month duration.\nOur dataset is quite large to be viewed in a table nicely and adequately; so we will subset the first 100 rows (remember rows are individual records or observations) and show them in an interactive table using the datatable() function from DT package.\n\n\ndatatable(fuel_expenditure[1:100, ])  ## view the first 100 observations in fuel_expenditure data\n\n\n\nThe fuel_expenditure table above shows that we have more variables than needed for our analysis as we are only interested in Petrol and Kerosene captured in the item_desc variable. Moreover, the variable names are not understandable. So just by viewing the table, we will realize that some data wrangling tasks await us. The same can be said for the health_expenditure data object.\nLet’s view the first 100 rows of health_expenditure data.\n\n\ndatatable(health_expenditure[1:100, ]) ## view the health_expenditure data \n\n\n\nData wrangling and transformation\nWe have imported and viewed our dataset successfully and now we have to clean, and transform our data to the required format for analysis. Data wrangling and transformation activities are not a cast in a stone despite some activities are very common. The type and structure of dataset and analysis we want to perform determines what we need to do.\nLooking at our dataset, we need to reduce it to only columns that we are interested in, add some new columns with transformed data and clean out the missing data. We will be utilizing piping approach (%>%) to harness its benefit of logical and clear flow of analysis. This can be achieve with the code below for fuel_expenditure.\n\n\n\nNow, we have reduced the data to only variables needed. Yet, it is obvious there is still work to be done. First of all, the varaible “state” has meaningless numbers instead of names of states in Nigeria. This was useful for data collection and data entry but not our analysis where audience have to know state names explicitely. How do we know exactly what these numbers represent? Again, this is where our metadata exploration comes to our rescue. When we reviewed the documentation that came with the data, we realized that under “state code”, the corresponding state names are provided. With this, we can add a new column that has the corresponding state names.\nAnother issue to note, which also influences how to transform our data is that, the number of observations recorded for each state varies. This has to do with the sampling technique used during data collection in order to achieve good representativeness. We want to make sure that, the states are comparable in terms of expenditure. So instead of summarizing base on the total expenditure for each state, we will do so based on the mean. With that, we will see mean expenditure of state and not sum of households. In order to captured this as our labeling when we do the clustering analysis, we will convert the column of state names to row names.\nNow, let’s code all that we have discussed so far.\n\n\nfuel_expenditure_transformed <- fuel_expenditure_transformed %>%\n  dplyr::mutate(state_names = case_when(  ## add a new column containing state names\n     state == 1 ~ \"Abia\",\n     state == 2 ~ \"Adamawa\",\n     state == 3 ~ \"Akwa Ibom\",\n     state == 4 ~ \"Anambra\",\n     state == 5 ~ \"Bauchi\",\n     state == 6 ~ \"Bayelsa\",\n     state == 7 ~ \"Benue\",\n     state == 8 ~ \"Borno\", \n     state == 9 ~ \"Cross River\", \n     state == 10 ~ \"Delta\", \n     state == 11 ~ \"Ebonyi\", \n     state == 12 ~ \"Edo\",\n     state == 13 ~ \"Ekiti\", \n     state == 14 ~ \"Enugu\", \n     state == 15 ~ \"Gombe\", \n     state == 16 ~ \"Imo\", \n     state == 17 ~ \"Jigawa\", \n     state == 18 ~ \"Kaduna\", \n     state == 19 ~ \"Kano\", \n     state == 20 ~ \"Katsina\", \n     state == 21 ~ \"Kebbi\",\n     state ==22 ~ \"Kogi\", \n     state == 23 ~ \"Kwara\", \n     state == 24 ~ \"Lagos\", \n     state == 25 ~ \"Nasarawa\",\n     state == 26 ~ \"Niger\", \n     state == 27 ~ \"Ogun\", \n     state == 28 ~ \"Ondo\", \n     state == 29 ~ \"Osun\", \n     state == 30 ~ \"Oyo\",\n     state == 31 ~ \"Plateau\", \n     state == 32 ~ \"Rivers\", \n     state == 33 ~ \"Sokoto\", \n     state == 34 ~ \"Taraba\",\n     state == 35 ~ \"Yobe\",\n     state == 36 ~ \"Zamfara\", \n     state == 37 ~ \"FCT Abuja\")\n   ) %>%\n  tidyr::pivot_wider(names_from = item_desc, values_from = s8q4, values_fn = mean)\n\ndatatable(fuel_expenditure_transformed)\n\n\n\nAfter cleaning and transforming fuel_expenditure data, we need to make sure that the second data is also transform to be at the same quality level or format. Remember that health_expenditure is for 6 months which means that in order to analyze both data properly for the same time scale, we need to convert it to monthly data and then calculate the mean.\nThe code to transform health expenditure data to required format is below;\n\n\nhealth_expenditure_transformed <- health_expenditure %>%\n  dplyr::select(state, item_desc, s8q6)%>%\n  dplyr::filter(item_desc %in% \"HEALTH EXPENDITURES (EXCLUDING\") %>%\n  na.omit() %>%\n   dplyr::mutate(state_names = case_when(\n     state == 1 ~ \"Abia\",\n     state == 2 ~ \"Adamawa\",\n     state == 3 ~ \"Akwa Ibom\",\n     state == 4 ~ \"Anambra\",\n     state == 5 ~ \"Bauchi\",\n     state == 6 ~ \"Bayelsa\",\n     state == 7 ~ \"Benue\",\n     state == 8 ~ \"Borno\",\n     state == 9 ~ \"Cross River\",\n     state == 10 ~ \"Delta\",\n     state == 11 ~ \"Ebonyi\",\n     state == 12 ~ \"Edo\",\n     state == 13 ~ \"Ekiti\",\n     state == 14 ~ \"Enugu\",\n     state == 15 ~ \"Gombe\",\n     state == 16 ~ \"Imo\",\n     state == 17 ~ \"Jigawa\",\n     state == 18 ~ \"Kaduna\",\n     state == 19 ~ \"Kano\",\n     state == 20 ~ \"Katsina\",\n     state == 21 ~ \"Kebbi\",\n     state ==22 ~ \"Kogi\",\n     state == 23 ~ \"Kwara\",\n     state == 24 ~ \"Lagos\",\n     state == 25 ~ \"Nasarawa\",\n     state == 26 ~ \"Niger\",\n     state == 27 ~ \"Ogun\",\n     state == 28 ~ \"Ondo\",\n     state == 29 ~ \"Osun\",\n     state == 30 ~ \"Oyo\",\n     state == 31 ~ \"Plateau\",\n     state == 32 ~ \"Rivers\",\n     state == 33 ~ \"Sokoto\",\n     state == 34 ~ \"Taraba\",\n     state == 35 ~ \"Yobe\",\n     state == 36 ~ \"Zamfara\",\n     state == 37 ~ \"FCT Abuja\")\n   ) %>%\n  dplyr::group_by(state_names) \n\n  health_expenditure_transformed <- dplyr::summarize(health_expenditure_transformed, Monthly_mean_health_expenditure = mean(s8q6/6))\n \n datatable(health_expenditure_transformed)\n\n\n\nNow, we have transformed both datasets and we need to verify if they are equal in terms of number of rows and columns before joining together for further analysis.\n\n\nnrow(fuel_expenditure_transformed)\n\n[1] 37\n\nnrow(health_expenditure_transformed)\n\n[1] 36\n\nWe will realize that the number of rows are different for fuel_expenditure_transformed (37) and health_expenditure_transformed (36). This is a common occurrence in data exploration and preparation. There is a state without data in health_expenditure_transformed. If we are to inspect the data, we will realize that there is no record for Jigawa state. This is an instance where we face the decision of either employing a data imputation method to provide values for Jigasaw or leaving it out altogether. We will tackle data imputation in a different post. For now, the better choice will be to leave Jigawa state out of the analysis. So we will remove the row for Jigawa state in fuel_expenditure_transformed in order to have 36 rows for both datasets and join them together. Then, we will convert the column “state_names” to row names for labeling the cluster leafs to be more meaningful. The code to achieve that is as follows;\n\n\ndataset_all <- subset.data.frame(fuel_expenditure_transformed, fuel_expenditure_transformed$state != 17) %>%\n  dplyr::left_join(health_expenditure_transformed) %>%  ## join the two datasets\n  tibble::column_to_rownames(var = \"state_names\") %>% ## transform variable state_names to rownames\n  select(-1) ## select all columns except the first one (state) which we don't need again\n\ndatatable(dataset_all)\n\n\n\nNow our data is ready for analysis!. But before then, we have to do clustering relevant data preparation\nClustering relevant data preparation\nThe data preparation done at this stage is not one that we will do for all types of analysis. We are doing them in order to successfully undertake a clustering analysis that is appropriate.\nData standardization: Before applying various clustering algorithms, we need to standardize the data to have mean zero and standard deviation one. This is done to make our variables have a common scale hence comparable.\nComputation of dissimilarity measure: This is done to identify observations that belong to the same clusters.\nNow, let’s write our code for that;\n\n\n## scale the data\n dataset_all_scaled <- scale(dataset_all)\n \n ## calculate dissimilarity \n dataset_dissimilarity <- dist(dataset_all_scaled, method = \"euclidean\")\n\n\nWe will use all the clustering methods we have discussed above and compare the results.\nAgglomerative Nesting (AGNES)\nAgglomerative nesting can be undertaken using hclust(), flashClust or agnes()\n\n\n## hierarchical clustering wtih complete linkage\nhcl_complete<-  hclust(dataset_dissimilarity, method = \"complete\")\n \nplot(hcl_complete, cex = 0.6, hang = -1) \n\n\n\nUsing agnes() function gives us the agglomerative co-efficient (ac) which measures the strength of clustering. The higher the ac, the stronger the clustering structure.\n\n\n## agnes clustering\nagnes_complete <- agnes(dataset_dissimilarity, method = \"complete\")\n\nplot(agnes_complete)\n\n\n\nNow, let’s look at other methods we have discussed for AGNES.\n\n\n## compute agnes clustering with various methods\nagnes_single <- agnes(dataset_dissimilarity, method = \"single\")\nagnes_average <- agnes(dataset_dissimilarity, method = \"average\")\nagnes_ward <- agnes(dataset_dissimilarity, method = \"ward\")\n\n#Plot results\nplot(agnes_single)\n\n\nplot(agnes_average)\n\n\nplot(agnes_ward)\n\n\n\nFrom the various methods, agnes with the ward method produces an ac of 0.9 which is the highest compared to others hence depicts the strongest structure.\nBefore we draw the curtains down on agnes, let’s also note that hcluster() function also accepts other methods\n\n\nhclust_mcquitty <- hclust(dataset_dissimilarity, method = \"mcquitty\")\nhclust_median <- hclust(dataset_dissimilarity, method = \"median\")\nhclust_centriod <- hclust(dataset_dissimilarity, method = \"centroid\")\n\nplot(hclust_mcquitty, cex = 0.6, hang = -1)\n\n\nplot(hclust_median, cex = 0.6, hang = -1)\n\n\nplot(hclust_centriod, cex = 0.6, hang = -1)\n\n\n\nDivisive clustering\nDivisive clustering does not require a method argument to be specified. let’s analyze the data using diana() function.\n\n\n#### divisive clustering\n# computes divisive hierarchical clustering\ndivisive_cluster <- cluster::diana(dataset_dissimilarity)\n\n# divisive coefficient -- amount of clustering structure found\ndivisive_cluster$dc\n\n[1] 0.8510366\n\n#plot dendrogram for divisive clustering\npltree(divisive_cluster, cex = 0.6, hang = .2, main = \"Dendrogram of diana\")\n\n\ndivisive_cluster$dc\n\n[1] 0.8510366\n\nThe divisive co-efficient (dc) is computed to be 0.85 which indicates strong clustering structure. The ac and dc are measured on a scale of 0 to 1 with lower values close to 0 indicating weak clustering and higher values close to 1 indicating strong cluster structure.\nIdentifying clusters of subgroups\nWe can identify observations that falls into similar groups or otherwise by stating the number of groups we want to cut into. let’s cut our observations into 5 clusters with the code below. This can be done for both agnes and divise\n\n\n# CUT TREE INTO 5 GROUPS\nhcl_complete_group <- cutree(hcl_complete, k = 5)\n\n### Number of members in each cluster\ntable(hcl_complete_group)\n\nhcl_complete_group\n 1  2  3  4  5 \n 8 20  5  2  1 \n\n##add cluster groupings to data\ndataset_all_cluster <- dataset_all%>%\n  mutate(cluster = hcl_complete_group) %>%\n  head()\n\n\nWe can visualize how the various states are grouped when we cut into 5 clusters using the code below\n\n\n## visualize with fviz_cluster\nfviz_cluster(list(data = dataset_all_scaled, cluster = hcl_complete_group))\n\n\n\nComparing dendrograms\nWe can compare clustering based on different methods by creating dendrogram objects for them and plotting.\n\n\n### create dendrogram object\nagnes_complete_dendrogram <- as.dendrogram(agnes_complete)\nagnes_ward_dendrogram <- as.dendrogram(agnes_ward)\n\n### plot the dendrograms to compare and match\ntanglegram(agnes_complete_dendrogram, agnes_ward_dendrogram)\n\n\n\nThe dashed lines show cluster combinations different from others.\nCustomizing the dendrogram\nVisualizing results has never been less important for cluster analysis where observations are grouped and there is the need for more colourful differentiation of groups to better understand the results presented. There are options for this. First, we can chain multiple dendrograms together to customize them easily.\n\n\n# create a dendlist\ndend_list_agnes_complete_ward <- dendlist(agnes_complete_dendrogram, agnes_ward_dendrogram)\n\ntanglegram(dend1 = agnes_complete_dendrogram, dend2 = agnes_ward_dendrogram,\n           highlight_distinct_edges = TRUE, # Turn off dashed lines\n           common_subtrees_color_lines = TRUE, # turn-off line colors\n           common_subtrees_color_branches = TRUE, # color common branches\n           highlight_branches_col = TRUE,\n           main_left = \"Agglomerative Nesting (complete method)\",\n           main_right = \"Agglomerative Nesting (ward method)\",\n           sub = \"Expenditure on Petrol, Kerosene and Health -States in Nigeria\",\n           cex_main_left = 1.1,\n           cex_main = 0.8,\n            cex_sub = .6,\n           lab.cex = 1,\n            margin_inner = 5,\n           main = paste(\"Entanglement = \", round(entanglement(dend_list_agnes_complete_ward), 2)))\n\n\nagnes_complete_dendrogram\n\n'dendrogram' with 2 branches and 36 members total, at height 6.648864 \n\nWe have added entanglement co-efficient to the graph. It measures how well-align the trees are on a scale of 0 to 1. The low co-oefficient of 0.17 indicates low entanglement with good alignment.\nDetermining optimal number of clusters\nA number of methods can be used to determine the optimal number of clusters for grouping\nElbow method of determing optimal number of clusters\nTo determine optimal number of clusters with elbow method, we need to look at where the bend is located which can be gauged to be 3 from our analysis below\n\n\nfviz_nbclust(dataset_all_scaled, FUN = hcut, method = \"wss\")\n\n\n\nAverage silhouette method\nAverage silhouette method indicates how well the clusters form with within cluster difference minimized at the optimal cluster number. Thus, the highest average silhouette width corresponds to the optimal number of clusters.\n\n\nfviz_nbclust(dataset_all_scaled, FUN = hcut, method = \"silhouette\")\n\n\n\nFrom the plot above, the optimal number of clusters is 3.\nGap statistics method\n\n\ndataset_gapstat <- clusGap(dataset_all_scaled, FUN = hcut, K.max = 10)\n\nfviz_gap_stat(dataset_gapstat)\n\n\n\nThe gaps method identified 1 to be the optimal number of clusters within a range of 1 to 10 clusters. Well, as we can deduce, different methods can identify different number of optimal clusters. With 2 of the methods suggesting 3 to be the optimal number of clusters, we can go by that.\nNow that we know the optimal number of clusters, let’s use that to visualize the data but in a way different from the previous.\n\n\n# Cut the dendrogram into 4 clusters\ncolors = c(\"brown\", \"orange\", \"#1DEfbb\", \"yellow\")\n\nward_cut <- cutree(hcl_complete, k = 3)\nplot(as.phylo(hcl_complete), type = \"fan\", tip.color = colors[ward_cut], label.offset = .5, cex = 0.7, no.margin = TRUE, use.edge.length = TRUE)\n\n\n\nConclusion\nWith this simple exercise, we learnt how to undertake hierarchical clustering analysis using a project that is typical of the real world. We went through process of data wrangling and transformation, clustering and determining the optimal number of clusters.\n\n\n\n",
    "preview": "posts/2022-07-20-hierarchical-clustering-which-states-in-nigeria-have-similar-expenditure-profile/hierarchical-clustering-which-states-in-nigeria-have-similar-expenditure-profile_files/figure-html5/customize-dendro-1.png",
    "last_modified": "2022-09-04T13:24:50+02:00",
    "input_file": "hierarchical-clustering-which-states-in-nigeria-have-similar-expenditure-profile.knit.md",
    "preview_width": 1536,
    "preview_height": 1152
  },
  {
    "path": "posts/2022-07-17-data-visualization/",
    "title": "Data visualization",
    "description": "This post entails the use of ggplot for data visualization.",
    "author": [
      {
        "name": "Linus Agbleze",
        "url": "https://agbleze.github.io/Portfolio/"
      }
    ],
    "date": "2022-07-17",
    "categories": [
      "visualization"
    ],
    "contents": "\n\n\n\n\n\n\n",
    "preview": "posts/2022-07-17-data-visualization/data-visualization_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2022-07-20T03:40:04+02:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/welcome/",
    "title": "Welcome to Datasiast!",
    "description": "Datasiast exists for enthusiatic data science knowledge sharing and practice.",
    "author": [
      {
        "name": "Linus Agbleze",
        "url": "https://agbleze.github.io/Portfolio/"
      }
    ],
    "date": "2022-07-17",
    "categories": [],
    "contents": "\n\n\n\n",
    "preview": {},
    "last_modified": "2022-07-20T03:40:25+02:00",
    "input_file": {}
  }
]
