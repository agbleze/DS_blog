<?xml version="1.0" encoding="UTF-8"?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:distill="https://distill.pub/journal/" version="2.0">
  <channel>
    <title>Data science tutorials</title>
    <link>https://addurl/</link>
    <atom:link href="https://addurl/index.xml" rel="self" type="application/rss+xml"/>
    <description>This blog aims demonstrating various data analysis techniques in a 
practical way.
</description>
    <generator>Distill</generator>
    <lastBuildDate>Tue, 11 Oct 2022 00:00:00 +0000</lastBuildDate>
    <item>
      <title>Hypothesis testing and regression: Determining which marketing strategy will increase sales and conversion and how factors influence them</title>
      <dc:creator>Linus Agbleze</dc:creator>
      <link>https://addurl/posts/2022-10-11-marketing-analysis-hypothesis-testing-to-determine-best-sales-script-for-increasing-conversion</link>
      <description>


&lt;h2 id="background"&gt;Background&lt;/h2&gt;
&lt;p&gt;Businesses are well aware of the various strategies by which to reach their ends which could selling products, increasing sales, reducing cost, enlisting more clients among others. In many cases, the goal is not to know the various strategies that exist impractically execute all of them. Rather the goal is to utilize the most rewarding and optimal strategy that maximizes the business goal. This understanding directly feeds into how products are marketed, customers are reached among others.&lt;/p&gt;
&lt;p&gt;Of the many strategies that exists, how do we know if the best method is being used to achieve the best results? For instance, how do marketing teams know an ad will achieve a better sales than a different one? How do sales team determine the best demo session approach to convert a prospective client? Well, nothing is known by gut feeling, at least when money is at stake. The best evidence that can be made for a strategy is one that is testable with verifiable principles. What are a way to make a case for statistical analysis for businesses! This post aims to demonstrate such a case.&lt;/p&gt;
&lt;p&gt;In essence, this post undertakes statistical analysis for a firm that is challenged with the problem of determining the best sales script to use during a demo session with a prospective client in order to increase number of conversions. The firm provide services for which prospective clients sign-up. In order to stay in business, it is not enough to access the market by landing demo request but more essentially turn those requests into conversions and increase conversion rate. To get prospective clients sign-up for their services, the firm provides a demo session after a client has requested for one. The sales team then decide the strategy to use in convincing the prospective client to sign-up during the demo session. The strategy is one of 3 sales scripts that is used during the demo session. For such a strategy, there is the possibility that one of the scripts is optimal in attaining higher conversion rate by design and not by chance. The aim is to identify this script if existing and recommend it to the sales team in order to stop incurring the opportunity cost of not using the optimal script.&lt;/p&gt;
&lt;p&gt;A key component for data analysis in business setting is that the business goal informs the kind of analysis to be done and business understanding determines how the analysis should be undertaken. In this case, the goal of the business, sales team to be specific, is stated as follows;&lt;/p&gt;
&lt;h2 id="goal-for-analysis"&gt;Goal for analysis&lt;/h2&gt;
&lt;p&gt;To increase conversion from demo request to contract signing&lt;/p&gt;
&lt;h2 id="business-problem-statement"&gt;Business Problem statement&lt;/h2&gt;
&lt;p&gt;The sales team aims at undertaking a data-based decision making to achieve their goal. To achieve that, data has been collected to be analyzed to gain insights. The challenge is to discern insights from the data that enable them attain their goal by asking the right business questions. Data analysis is undertaken in response to business problems hence it is critical that all the problems that the analysis aims to provide solutions or rather recommendations on how to solve them, are identified in an analytical framework or statement. This is identified as problem statement as highlighted as follows;&lt;/p&gt;
&lt;h3 id="problem-statement-1-assessing-the-relationship-between-sales-script-type-and-conversion"&gt;Problem statement 1: Assessing the relationship between sales script type and conversion&lt;/h3&gt;
&lt;p&gt;Given the fact that sales script was the main tool used during the demo session, it is hypothesized to be associated with conversion. This understanding translates into the following analytical framework for further assessment.&lt;/p&gt;
&lt;p&gt;Null Hypothesis (H0): There is no relationship between type of sales script and conversion&lt;/p&gt;
&lt;p&gt;Alternative Hypothesis (H1): There is a relationship between type of sales script and conversion&lt;/p&gt;
&lt;h3 id="problem-statement-2-assessing-difference-in-total-conversions-made-by-different-sales-scripts"&gt;Problem statement 2: Assessing difference in total conversions made by different sales scripts&lt;/h3&gt;
&lt;p&gt;While the total conversion for each script can be easily assessed, it is not definitive of a script’s better performance without an element of chance. There is the need to assess whether difference in conversion is significant to suggest a preference for one of them to increase the chances of achieving higher conversion. This need not be a guesswork but data driven hence translated into the following hypothesis for analysis;&lt;/p&gt;
&lt;p&gt;Null hypothesis (H0): Difference in mean conversion among all sales scripts is equal&lt;/p&gt;
&lt;p&gt;Alternative hypothesis (H1): There is difference in mean conversion for at least one of the sales scripts&lt;/p&gt;
&lt;h3 id="problem-statement-3determining-the-sales-script-that-produces-significantly-higher-conversions"&gt;Problem statement 3:Determining the sales script that produces significantly higher conversions&lt;/h3&gt;
&lt;p&gt;The result for the hypothesis stated above will determine whether or not there is the need for further enquiry in the form of post-hoc test. If the analysis results in rejection of the null hypothesis, then there will be the need to assess how the different sales scripts compare to each other in order to identify which of the sales scripts produce significantly higher mean conversion.&lt;/p&gt;
&lt;h3 id="problem-statement-4-assessing-factors-that-influence-the-decision-of-a-client-to-convert"&gt;Problem statement 4: Assessing factors that influence the decision of a client to convert&lt;/h3&gt;
&lt;p&gt;For business success, the higher the amount of conversions, the better. The business problem at hand is to understand the drivers of conversion. The decision by a client to sign-up or not during or after a demo session is one that needs to be better understood in order to reinforce what produces conversion and eliminates factors that prevents conversion. This problem requires assessing how various factors for which data is available are influencing conversion.&lt;/p&gt;
&lt;h2 id="business-questions-to-answer"&gt;Business questions to answer&lt;/h2&gt;
&lt;p&gt;Based on the the business problem statements conceptualized, this analysis is undertaken to provide answers to the following business questions&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Is there a significant relationship between type of sales script and conversion?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Are all sales scripts achieving a similar amount of conversions?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;How do sales scripts compare in terms of conversions and which sales scripts can be used to achiever higher conversions?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;How do various factors influence the probability of a client to sign-up?&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="business-objectives"&gt;Business objectives&lt;/h2&gt;
&lt;p&gt;To define the data analysis tasks, the business questions are translated into objectives as follows&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;To test the hypothesis that there is a statistically significant relationship between type of sales script and conversion&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;To test the hypothesis that difference in mean conversion among sales script is equal&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;To assess and identify which sales script produces higher average conversions if any&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;To assess potential drivers of conversion and understand their influence&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="data-analysis-task"&gt;Data analysis task&lt;/h2&gt;
&lt;p&gt;This section details the procedure used to analyzed the data to derive insights and draw recommendations. First of all it is important to highlight some of the formula used.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Formula used&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Conversion rate = &lt;span class="math inline"&gt;\(frac{Number of contracts signed}{Number of Demo requests}\100\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Conversion rate for script = &lt;span class="math inline"&gt;\(frac{Total conversions when script was used}{Total number of demo sessions when script was used}\)&lt;/span&gt;&lt;/p&gt;
&lt;h3 id="assumptions"&gt;Assumptions&lt;/h3&gt;
&lt;p&gt;In order to make results reproducible and understandable for contextualized interpretation, much effort is made to lay bare assumptions that may influence how insights is drawn. Some of these assumptions are required for the statistical analysis undertaken to hold true. These are highlighted in this section as follows.&lt;/p&gt;
&lt;p&gt;Scripts were used independently, that is a single script was applied for a client. All scripts were used for approximately the same time period. It is however realized from the data that the first demo appointment session for which script C was used was on 2020-01-08 16:50:35 and the last date of use was 2021-03-01 19:40:01. For script A, it was first used for a demo session on 2019-12-28 03:57:38 and its last usage was on 2021-03-29 12:44:07. Script B was first used for a demo session on 2019-12-28 11:38:55 and last used was on 2021-03-06 16:57:21. Thus, some differences in date of first usage and last usage of script exist but this is assumed to be negligible. The script were indeed used independent of each other.&lt;/p&gt;
&lt;h2 id="data-exploration"&gt;Data exploration&lt;/h2&gt;
&lt;pre class="python"&gt;&lt;code&gt;## Import packages
import pandas as pd
from plotnine import ggplot, ggtitle, aes, geom_col, theme_dark, theme_light, geom_boxplot
import numpy as np
import matplotlib.pyplot as plt
import datetime
import seaborn as sns
import scipy.stats as stats 
from bioinfokit.analys import stat
import statsmodels.api as sm
import pingouin as pg
import scikit_posthocs as sp
from sklearn.linear_model import LogisticRegression, LogisticRegressionCV
from sklearn.model_selection import train_test_split&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id="importing-and-inspecting-dataset"&gt;Importing and inspecting dataset&lt;/h3&gt;
&lt;p&gt;The data analysis begins with importing the data and inspecting the variables. In this section, the data is explored.&lt;/p&gt;
&lt;pre class="python"&gt;&lt;code&gt;## import data and read top 5

df = pd.read_csv(data_path)

## View first 3 rows of the data
df.head(3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;   Unnamed: 0  ...  first_call_to_1streach_timelength_minutes_
0           1  ...                                  327.465776
1           2  ...                                  138.429704
2           3  ...                                  357.238224

[3 rows x 26 columns]&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;## Identify the categories of sales scripts
df.sales_script_variant.unique()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([&amp;#39;a170e8b5b0085420fa52f9f9e1d546f9&amp;#39;,
       &amp;#39;e908f62885515872936a2bf07e5960a0&amp;#39;,
       &amp;#39;21790c97eeb6336e5f0fdb9ef4de636f&amp;#39;], dtype=object)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;## get description of the variables in the data

df.info()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;class &amp;#39;pandas.core.frame.DataFrame&amp;#39;&amp;gt;
RangeIndex: 10000 entries, 0 to 9999
Data columns (total 26 columns):
 #   Column                                      Non-Null Count  Dtype  
---  ------                                      --------------  -----  
 0   Unnamed: 0                                  10000 non-null  int64  
 1   request_id                                  10000 non-null  int64  
 2   source                                      10000 non-null  object 
 3   request_created_at                          10000 non-null  object 
 4   company_type                                5084 non-null   float64
 5   first_call                                  10000 non-null  object 
 6   first_reach                                 9418 non-null   object 
 7   sales_script_variant                        10000 non-null  object 
 8   sales_group_name                            10000 non-null  object 
 9   demo_appointment_datetime                   10000 non-null  object 
 10  appointment_added_by_id                     10000 non-null  int64  
 11  is_signed                                   10000 non-null  int64  
 12  conversion                                  10000 non-null  object 
 13  sales_script_type                           10000 non-null  object 
 14  company_group                               10000 non-null  object 
 15  request_created_date                        10000 non-null  object 
 16  first_call_date                             10000 non-null  object 
 17  first_reach_date                            9418 non-null   object 
 18  request_created_year                        10000 non-null  int64  
 19  request_created_month                       10000 non-null  object 
 20  request_to_1stcall_interval                 10000 non-null  object 
 21  request_to_1streach_interval                10000 non-null  object 
 22  first_call_to_1streach_interval             10000 non-null  object 
 23  request_to_1stcall_timelength_minutes_      10000 non-null  float64
 24  request_to_1streach_timelength_minutes_     9418 non-null   float64
 25  first_call_to_1streach_timelength_minutes_  9418 non-null   float64
dtypes: float64(4), int64(5), object(17)
memory usage: 2.0+ MB&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;## shows the shape of the data -- number of rows and columns
df.shape&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(10000, 26)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;## Cast the data into DplyFrame in order to use dplython functions on it.

df_dataframe = df.copy()
df_dataframe.columns&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Index([&amp;#39;Unnamed: 0&amp;#39;, &amp;#39;request_id&amp;#39;, &amp;#39;source&amp;#39;, &amp;#39;request_created_at&amp;#39;,
       &amp;#39;company_type&amp;#39;, &amp;#39;first_call&amp;#39;, &amp;#39;first_reach&amp;#39;, &amp;#39;sales_script_variant&amp;#39;,
       &amp;#39;sales_group_name&amp;#39;, &amp;#39;demo_appointment_datetime&amp;#39;,
       &amp;#39;appointment_added_by_id&amp;#39;, &amp;#39;is_signed&amp;#39;, &amp;#39;conversion&amp;#39;,
       &amp;#39;sales_script_type&amp;#39;, &amp;#39;company_group&amp;#39;, &amp;#39;request_created_date&amp;#39;,
       &amp;#39;first_call_date&amp;#39;, &amp;#39;first_reach_date&amp;#39;, &amp;#39;request_created_year&amp;#39;,
       &amp;#39;request_created_month&amp;#39;, &amp;#39;request_to_1stcall_interval&amp;#39;,
       &amp;#39;request_to_1streach_interval&amp;#39;, &amp;#39;first_call_to_1streach_interval&amp;#39;,
       &amp;#39;request_to_1stcall_timelength_minutes_&amp;#39;,
       &amp;#39;request_to_1streach_timelength_minutes_&amp;#39;,
       &amp;#39;first_call_to_1streach_timelength_minutes_&amp;#39;],
      dtype=&amp;#39;object&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id="estimating-total-number-of-conversions"&gt;Estimating total number of conversions&lt;/h3&gt;
&lt;h4 id="keywords"&gt;Keywords&lt;/h4&gt;
&lt;p&gt;Non-conversion are cases where the client did not sign up during and after the demo session Given that our goal is to increase conversion, the analysis will be centered around that. First, the total number of conversion is estimated and compared to non-conversion&lt;/p&gt;
&lt;p&gt;From the analysis below, total number of conversion was 5,018 which is slightly higher than non-conversion of 4,982. The sum of both conversion and non-conversion equates to the total number of requests made.&lt;/p&gt;
&lt;pre class="python"&gt;&lt;code&gt;
## select some columns needed to estimating conversion
df_dplyselect = df_dataframe[[&amp;#39;is_signed&amp;#39;, &amp;#39;conversion&amp;#39;, &amp;#39;sales_script_type&amp;#39;, &amp;#39;request_to_1streach_timelength_minutes_&amp;#39;, &amp;#39;company_group&amp;#39;]] 

## Group data based on conversion column and count number of conversion
conversion_total = df_dplyselect[&amp;#39;conversion&amp;#39;].value_counts().reset_index()

conversion_total.rename(columns={&amp;#39;index&amp;#39;: &amp;#39;conversion&amp;#39;, &amp;#39;conversion&amp;#39;: &amp;#39;total_conversion&amp;#39;}, inplace=True)
conversion_total&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;    conversion  total_conversion
0      convert              5018
1  not_convert              4982&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;
(ggplot(conversion_total, aes(x=&amp;#39;conversion&amp;#39;, y=&amp;#39;total_conversion&amp;#39;))
 + geom_col(stat = &amp;#39;identity&amp;#39;) + ggtitle(&amp;#39;Bar chart of Total Conversion and non-conversion&amp;#39;) + theme_light()
)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;ggplot: (8765602144184)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="https://addurl//posts/2022-10-11-marketing-analysis-hypothesis-testing-to-determine-best-sales-script-for-increasing-conversion/sales-hypothesis-analysis_files/figure-html5/unnamed-chunk-2-1.png" width="614" style="display: block; margin: auto;" /&gt;&lt;/p&gt;
&lt;h3 id="estimating-conversion-rate"&gt;Estimating conversion rate&lt;/h3&gt;
&lt;p&gt;After estimating conversion and non-conversion, conversion needs to be estimated. This can be considered as one of the major KPI of interest to the sales team.&lt;/p&gt;
&lt;p&gt;Сonversion rate = &lt;span class="math inline"&gt;\(frac{Number of contracts signed}{Number of Demo requests}\)&lt;/span&gt;&lt;/p&gt;
&lt;pre class="python"&gt;&lt;code&gt;## The number of Demo requests] is estimated below

request_total = conversion_total[&amp;#39;total_conversion&amp;#39;].sum()
request_total&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;10000&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;## Conversion rate 
conversion_rate = conversion_total.iloc[[0],[1]] / request_total

conversion_rate.rename(index=str, columns={&amp;quot;total_conversion&amp;quot;: &amp;quot;C1&amp;quot;})&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;       C1
0  0.5018&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Thus, conversion rate is estimated to be 0.5018. It is concluded that conversion rate in percentage terms is 50.2%&lt;/p&gt;
&lt;h3 id="disaagregating-conversion-for-indepth-analysis"&gt;Disaagregating conversion for indepth analysis&lt;/h3&gt;
&lt;p&gt;In order to better understand conversion, there is the need to explore the data based on certain dimensions.&lt;/p&gt;
&lt;h4 id="conversion-based-on-company-type"&gt;Conversion based on company type&lt;/h4&gt;
&lt;pre class="python"&gt;&lt;code&gt;
company_conversion = df_dplyselect.groupby([&amp;#39;company_group&amp;#39;, &amp;#39;conversion&amp;#39;])[&amp;#39;conversion&amp;#39;].count().to_frame().rename(columns={&amp;#39;conversion&amp;#39;: &amp;#39;total_count&amp;#39;}).reset_index()

company_conversion&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;  company_group   conversion  total_count
0        Others      convert         2446
1        Others  not_convert         2470
2     company_A      convert         1027
3     company_A  not_convert         1039
4     company_B      convert         1545
5     company_B  not_convert         1473&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;
(ggplot(company_conversion, aes(x=&amp;#39;company_group&amp;#39;, y=&amp;#39;total_count&amp;#39;, fill=&amp;#39;conversion&amp;#39;))
 + geom_col(stat=&amp;#39;identity&amp;#39;, position=&amp;#39;dodge&amp;#39;)) + theme_dark() + ggtitle(&amp;#39;Conversion based on type of company&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;ggplot: (8765602180569)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="https://addurl//posts/2022-10-11-marketing-analysis-hypothesis-testing-to-determine-best-sales-script-for-increasing-conversion/sales-hypothesis-analysis_files/figure-html5/unnamed-chunk-2-3.png" width="614" style="display: block; margin: auto;" /&gt;&lt;/p&gt;
&lt;h4 id="conversion-based-on-sales-group"&gt;Conversion based on sales group&lt;/h4&gt;
&lt;p&gt;From our research questions and hypothesis, conversion based on sales script type is will offer valuable insight hence the data is grouped and conversion is estimated and visualized for the different sales group before testing their hypothesis further.&lt;/p&gt;
&lt;pre class="python"&gt;&lt;code&gt;
## group data based on type of sales script and count total conversion

script_conversion = df_dplyselect.groupby([&amp;#39;sales_script_type&amp;#39;, &amp;#39;conversion&amp;#39;])[&amp;#39;conversion&amp;#39;].count().to_frame().rename(columns={&amp;#39;conversion&amp;#39;: &amp;#39;total_conversion&amp;#39;}).reset_index()

script_conversion&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;  sales_script_type   conversion  total_conversion
0          script_A      convert              3320
1          script_A  not_convert              3237
2          script_B      convert              1638
3          script_B  not_convert              1692
4          script_C      convert                60
5          script_C  not_convert                53&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id="the-result-of-the-table-can-be-visualize-below"&gt;The result of the table can be visualize below&lt;/h4&gt;
&lt;pre class="python"&gt;&lt;code&gt;
### Plot the total conversion based on script type

(ggplot(script_conversion, aes(x=&amp;#39;sales_script_type&amp;#39;, y=&amp;#39;total_conversion&amp;#39;, fill=&amp;#39;conversion&amp;#39;))
 + geom_col(stat=&amp;#39;identity&amp;#39;, position=&amp;#39;dodge&amp;#39;)) + theme_dark() + ggtitle(&amp;#39;Conversion based on type of sales script&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;ggplot: (8765603085493)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="https://addurl//posts/2022-10-11-marketing-analysis-hypothesis-testing-to-determine-best-sales-script-for-increasing-conversion/sales-hypothesis-analysis_files/figure-html5/unnamed-chunk-2-5.png" width="614" data-distill-preview=1 style="display: block; margin: auto;" /&gt;&lt;/p&gt;
&lt;h3 id="insights-from-the-visuals"&gt;Insights from the visuals&lt;/h3&gt;
&lt;p&gt;Estimates of conversion based on script type shows that both script A and script C made a higher conversion compared to non-conversion. Script A made 83 more conversions than non-conversion while script C made 7 more conversion than non-conversion. On the contrary, script B made 54 less conversions compared to non-conversion.&lt;/p&gt;
&lt;p&gt;While this difference gives a clue about performance of the various script, it does not enables us to make decisive conclusion but guesses of what the difference could result in. Forinstance, the total conversion is a summation and hence the difference could be the result of number of demo sessions that a script has been used for. Clarity needs to be brought to such guessestimates by computing their mean.&lt;/p&gt;
&lt;p&gt;In order to make a data driven decision, hypothesis need to be tested to derive better understanding base on statistical significance.&lt;/p&gt;
&lt;h2 id="objective-1"&gt;Objective 1&lt;/h2&gt;
&lt;p&gt;To test the hypothesis that there is statistically significant relationship between type of sales script and conversion.&lt;/p&gt;
&lt;p&gt;Proceeding from the insights gained, this section tests the hypothesis for the first objective&lt;/p&gt;
&lt;p&gt;H0: Conversion is independent of type of sales script used&lt;/p&gt;
&lt;p&gt;H1: Conversion is dependent on type of sales script used&lt;/p&gt;
&lt;h3 id="method-of-analysis"&gt;Method of analysis&lt;/h3&gt;
&lt;p&gt;Chi squared test of independence is appropriate for assessing whether there is a relationship between two categorical variables hence used. The procedure involves computing a contingency table and using that for the analysis. This is demonstrated below.&lt;/p&gt;
&lt;p&gt;Contingency table&lt;/p&gt;
&lt;pre class="python"&gt;&lt;code&gt;## contingency table between sales script type and conversion

conversion_script_type_contengency = pd.crosstab(df_dplyselect[&amp;#39;sales_script_type&amp;#39;], df_dplyselect[&amp;#39;conversion&amp;#39;])

print(conversion_script_type_contengency)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;conversion         convert  not_convert
sales_script_type                      
script_A              3320         3237
script_B              1638         1692
script_C                60           53&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;### chi-square test of independence
chi_square_result = stat()

chi_square_result.chisq(df=conversion_script_type_contengency)

print(chi_square_result.summary)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Chi-squared test for independence

Test              Df    Chi-square    P-value
--------------  ----  ------------  ---------
Pearson            2       2.23037   0.327855
Log-likelihood     2       2.23068   0.327804&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id="results-for-objective-one"&gt;Results for objective one&lt;/h3&gt;
&lt;p&gt;With a p-value of 0.3279 being greater than 0.05 (5% significance level), it is suggested that there is no statistically significant relationship between type of sales script and conversion. Thus, we fail to reject the null hypothesis of independence based on available evidence.&lt;/p&gt;
&lt;p&gt;This result has an added twist to it, which is that, there is the possibility of a change in result when more and better data is acquired.&lt;/p&gt;
&lt;p&gt;A major research gap that remains is whether the differences in conversion between scripts as deduced from the bar plots is significant. This requires the need to test another hypothesis hence research objective 2.&lt;/p&gt;
&lt;h3 id="research-objective-2"&gt;Research objective 2&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;To test the hypothesis that difference in mean conversion among sales script are equal&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In order to test this hypothesis, continuous variable is needed. For this, the total number of conversions on daily basis can analyzed and used as a continuous variable. The rational for estimating daily total conversions instead of monthly or yearly is to ensure that there are enough data points to make statistical inference.&lt;/p&gt;
&lt;p&gt;The available data covers only a year and a couple of days, hence daily conversions is a logical time frame for estimating total conversion. By this, the dataset needs to be grouped based on days and the sum of conversions estimated for each sales script type. The question that arise is how are conversions to be counted? For this, ‘is_signed’ variable is used; where its boolean data type (true or false) are treated as integers with 1 counted as a single conversion and 0 as no conversion for each demo session. This allows for the total daily conversion to be estimated.&lt;/p&gt;
&lt;p&gt;Another question is, which of the dates should be used as a basis for counting the total daily conversion? For this, the demo_appointment_datetime variable is a better option based on the context that attempt to convert the potential client is done during and after a demo session hence the assumption that the impact of the script used on conversion came into effect on the demo appointment day.&lt;/p&gt;
&lt;p&gt;It is assume that the date used will be of less relevance for testing the hypothesis despite there could be changes in the total number of daily conversion for scripts when the date is change. But the impact is assumed to be negligible.&lt;/p&gt;
&lt;h4 id="key-assumption"&gt;Key assumption&lt;/h4&gt;
&lt;ol style="list-style-type: decimal"&gt;
&lt;li&gt;&lt;p&gt;An assumption was made that all sales scripts had equal chance of being used during a demo session.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;It is assumed that there were no cases of repeated sessions where scripts were used again as repetition of campaign could be a contributor to conversion and not the script used.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class="python"&gt;&lt;code&gt;
### select various columns and split the demo appointment column into date and time

df_grp = df[[&amp;#39;demo_appointment_datetime&amp;#39;, &amp;#39;request_to_1streach_timelength_minutes_&amp;#39;, 
                  &amp;#39;conversion&amp;#39;, &amp;#39;company_group&amp;#39;, &amp;#39;sales_script_type&amp;#39;, &amp;#39;is_signed&amp;#39;]]
                  
                  
df_grp[[&amp;#39;demo_appointment_yyyymmdd&amp;#39;, &amp;#39;demo_appointment_hhmmss&amp;#39;]] = df_grp.demo_appointment_datetime.str.split(&amp;#39; &amp;#39;, expand=True)
    
## sum the total conversion for each script on each day    &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;string&amp;gt;:1: SettingWithCopyWarning:


A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy

&amp;lt;string&amp;gt;:1: SettingWithCopyWarning:


A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;df_grp_sum = (df_grp.groupby([&amp;#39;demo_appointment_yyyymmdd&amp;#39;,&amp;#39;sales_script_type&amp;#39;])[&amp;#39;is_signed&amp;#39;]
                            .sum().to_frame()
                            .rename(columns={&amp;#39;is_signed&amp;#39;: &amp;#39;total_conversion&amp;#39;}).reset_index()  
              )

df_grp_sum.head()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;  demo_appointment_yyyymmdd sales_script_type  total_conversion
0                2019-12-28          script_A                 0
1                2019-12-28          script_B                 2
2                2019-12-29          script_A                 0
3                2019-12-29          script_B                 1
4                2019-12-30          script_B                 1&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id="visualizing-the-total-conversion-on-demo-day-for-the-various-scripts"&gt;Visualizing the total conversion on demo day for the various scripts&lt;/h3&gt;
&lt;p&gt;First, the distribution of total conversion based on sales script is visualized using boxplot which shows the mean conversion for sales scripts as well as the minimum, second quartile, third quartile and maximum.&lt;/p&gt;
&lt;pre class="python"&gt;&lt;code&gt;
(ggplot(data = df_grp_sum, mapping = aes(x = &amp;#39;sales_script_type&amp;#39;, y = &amp;#39;total_conversion&amp;#39;))
+ geom_boxplot() + 
 ggtitle(&amp;#39;Total conversion by script type (based on conversion counts on demo day)&amp;#39;)) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;ggplot: (8765602171814)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="https://addurl//posts/2022-10-11-marketing-analysis-hypothesis-testing-to-determine-best-sales-script-for-increasing-conversion/sales-hypothesis-analysis_files/figure-html5/unnamed-chunk-2-7.png" width="614" style="display: block; margin: auto;" /&gt;&lt;/p&gt;
&lt;h3 id="insights-from-boxplot-of-total-conversions-based-on-demo-date"&gt;Insights from boxplot of total conversions based on demo date&lt;/h3&gt;
&lt;p&gt;From the boxplot, some points which are outside the normal range of the plot can be regarded as outliers. However, they are not remove for now. Moreover the difference in conversion between scripts is clearer with the boxplot. It is deduced that script C has the lowest average conversion while script A has the highest.&lt;/p&gt;
&lt;h3 id="visual-inspection-testing-statistical-assumptions-for-the-analysis-of-hypothesis"&gt;Visual inspection: Testing statistical assumptions for the analysis of hypothesis&lt;/h3&gt;
&lt;p&gt;Before deciding on the statistical method to use to answer the business questions, there is the need to verify that statistical assumptions hold true for the data. For valid statistical inference to be made about our target market based on our current clients data, it is very important that appropriate methods are used. The appropriateness of the method is not a guesswork but one informed by both visual and statistical test.&lt;/p&gt;
&lt;p&gt;Generally, there are two family of statistical techniques that can be employed in analyzing the data. Namely, Parametric and non-parametric methods. The parametric methods usually have a greater statistical power but this comes at the cost of requiring the data to meet a number of assumptions for proper conclusions to be reach. Given that, our aim is to present the possible best solution, the assumptions for using a parametric method are tested first before considering their use or otherwise.&lt;/p&gt;
&lt;h3 id="visual-inspection-of-normality"&gt;Visual inspection of normality&lt;/h3&gt;
&lt;p&gt;To use a parametric method, normality of data distribution is assumed. A simple approach to verifying this is with the aid of histogram as shown below.&lt;/p&gt;
&lt;pre class="python"&gt;&lt;code&gt;
sns.histplot(df_grp_sum[&amp;#39;total_conversion&amp;#39;], bins=20, kde=True, color=&amp;#39;g&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="https://addurl//posts/2022-10-11-marketing-analysis-hypothesis-testing-to-determine-best-sales-script-for-increasing-conversion/sales-hypothesis-analysis_files/figure-html5/unnamed-chunk-2-9.png" width="614" style="display: block; margin: auto;" /&gt;&lt;/p&gt;
&lt;h3 id="results-of-the-histogram"&gt;Results of the histogram&lt;/h3&gt;
&lt;p&gt;The histogram plotted above does not necessarily answer the question on normality assumption. Nonetheless, it is one of several visualization techniques that gives clues.&lt;/p&gt;
&lt;p&gt;From visual inspection, the distribution is right-tailed hence a right-skewed or positive-skewed distribution is noted for the data.&lt;/p&gt;
&lt;h3 id="q-q-plot-for-visualizing-normality"&gt;Q-Q plot for visualizing normality&lt;/h3&gt;
&lt;p&gt;In order to gain a better view on the normality of the distribution, a Q-Q plot is use to compare actual total conversion to an expected value in a normal distribution. The yardstick for detecting normality will be to verify that the actual data distribution are linearly arranged along a straight 45 degrees diagonal line. The result is illustrated below.&lt;/p&gt;
&lt;pre class="python"&gt;&lt;code&gt;
### QQ-PLOT

sm.qqplot(df_grp_sum[&amp;#39;total_conversion&amp;#39;], line = &amp;#39;45&amp;#39;)&lt;/code&gt;&lt;/pre&gt;

&lt;pre class="python"&gt;&lt;code&gt;plt.title(&amp;#39;Q-Q plot for total conversion&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.show()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="https://addurl//posts/2022-10-11-marketing-analysis-hypothesis-testing-to-determine-best-sales-script-for-increasing-conversion/sales-hypothesis-analysis_files/figure-html5/unnamed-chunk-2-11.png" width="672" style="display: block; margin: auto;" /&gt;&lt;/p&gt;
&lt;h3 id="insights-from-q-q-plot"&gt;Insights from Q-Q plot&lt;/h3&gt;
&lt;p&gt;From the Q-Q plot, it is deduced that the dataset deviates from the line of expected normal distribution hence heavily skewed. While the visualization so far points to the direction of a non-normal distribution, there is still the opportunity to cross-check these suggestions with some statistical test for normality.&lt;/p&gt;
&lt;h3 id="statistical-test-for-normality-shapiro-wilk-test-of-normality"&gt;Statistical test for normality: Shapiro-Wilk test of normality&lt;/h3&gt;
&lt;p&gt;The visual inspections are supported with Shapiro-Wilk test to test the hypothesis that the distribution of data is not different from an expected normal distribution.&lt;/p&gt;
&lt;pre class="python"&gt;&lt;code&gt;
#### Shapiro-Wilk test

w, pvalue = stats.shapiro(df_grp_sum[&amp;#39;total_conversion&amp;#39;])
print(w, pvalue)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0.9258283376693726 2.6865092770727672e-20&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id="calculating-normality"&gt;Calculating normality&lt;/h3&gt;
&lt;p&gt;This is another approach is to assessing normality using the pingouin module.&lt;/p&gt;
&lt;pre class="python"&gt;&lt;code&gt;
pg.normality(df_grp_sum, group=&amp;#39;sales_script_type&amp;#39;, dv=&amp;#39;total_conversion&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;                 W          pval  normal
script_A  0.977833  8.850418e-06   False
script_B  0.960671  1.574002e-08   False
script_C  0.745857  1.854477e-11   False&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id="insight-from-shapiro-wilk-test-of-normality"&gt;Insight from Shapiro-Wilk test of normality&lt;/h3&gt;
&lt;p&gt;The p-value of the test is less than 0.05 (5% significance level) which suggests a statistically significant difference from a normal distribution. Thus, the null hypothesis is rejected. This is not one of the relevant hypothesis to be tested for the business goal but its provides an important clue as to the right statistical method to adopt for assessing business relevant hypothesis.&lt;/p&gt;
&lt;p&gt;With the data failing to meet the assumption of normal distribution required for the adoption of a parametric method, the compass of the analysis is gearing towards a non-parametric method. But before ascertaining that, there is the need to test for other assumptions required such as homogeneity which stipulates that variance between categories to be analyzed should be equal across the data distribution for methods such ANOVA to be properly use.&lt;/p&gt;
&lt;h3 id="statistical-test-for-homogeneity"&gt;Statistical test for homogeneity&lt;/h3&gt;
&lt;h3 id="bartletts-test"&gt;Bartlett’s test&lt;/h3&gt;
&lt;p&gt;The first approach being used to test for equal variance in the distribution is the Bartlett’s test. Given that the distribution is non-normal, Barlett’s test is supported with Levene’s test which is a much more robust test when the data is not a normal distribution.&lt;/p&gt;
&lt;pre class="python"&gt;&lt;code&gt;
### bartlett&amp;#39;s test for homogeneity

script_bartlett = stat()
script_bartlett.bartlett(df = df_grp_sum, res_var = &amp;#39;total_conversion&amp;#39;, xfac_var = &amp;#39;sales_script_type&amp;#39;)
script_bartlett.bartlett_summary&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;                 Parameter     Value
0      Test statistics (T)  425.8573
1  Degrees of freedom (Df)    2.0000
2                  p value    0.0000&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id="insight-from-the-bartletts-test"&gt;Insight from the Bartlett’s test&lt;/h3&gt;
&lt;p&gt;The bartlett test result shows a p-value that is statistically significant hence the hypothesis that variation in the distribution is equal is rejected. This points in the direction of a non-parametric approach for the analysis but before then the result needs to be verified with a Levene’s test.&lt;/p&gt;
&lt;h3 id="levenes-statistical-test-for-homogeneity"&gt;Levene’s statistical test for homogeneity&lt;/h3&gt;
&lt;p&gt;Levene’s test is employed as a final approach in this context to verify whether not the variance in the data is equal.&lt;/p&gt;
&lt;pre class="python"&gt;&lt;code&gt;
## levene test

script_lev = stat()
script_lev.levene(df = df_grp_sum, res_var=&amp;#39;total_conversion&amp;#39;, xfac_var = &amp;#39;sales_script_type&amp;#39;)
script_lev.levene_summary&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;                 Parameter     Value
0      Test statistics (W)  119.2498
1  Degrees of freedom (Df)    2.0000
2                  p value    0.0000&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;### Usingthe pingouin module for the analysis

pg.homoscedasticity(df_grp_sum, group=&amp;#39;sales_script_type&amp;#39;, dv=&amp;#39;total_conversion&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;                 W          pval  equal_var
levene  119.249752  1.772745e-46      False&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id="insight-from-levenes-test"&gt;Insight from Levene’s test&lt;/h3&gt;
&lt;p&gt;The Levene’s test depict an unequal variance with a statistically significant p-value &amp;lt; 0.05 (at 5% significant level) hence confirming Bartlett’s test.&lt;/p&gt;
&lt;h3 id="conclusion-drawn-from-testing-of-statistical-assumptions"&gt;Conclusion drawn from testing of statistical assumptions&lt;/h3&gt;
&lt;p&gt;With visual inspections and all statistical tests indicating the data has a non-normal distribution and variance of values are unequal, the statistical approach adopted for testing the various business relevant hypothesis devised is a non-parametric one.&lt;/p&gt;
&lt;h3 id="kruskal-wallis-test"&gt;Kruskal-Wallis test&lt;/h3&gt;
&lt;p&gt;After finally deciding on the type of statistical approach to adopt, a selection is made among several non-parametric methods. For this, consideration is given to the number of categories or groupings in the independent variable. Three types of sales scripts were identified in the datset hence the decision to use Kruskal-Wallis test for the analysis of the second objective.&lt;/p&gt;
&lt;pre class="python"&gt;&lt;code&gt;pg.kruskal(data=df_grp_sum, dv=&amp;#39;total_conversion&amp;#39;, between=&amp;#39;sales_script_type&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;                    Source  ddof1           H         p-unc
Kruskal  sales_script_type      2  338.713092  2.814406e-74&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id="results"&gt;Results&lt;/h3&gt;
&lt;p&gt;The p-value from Kruskal-Wallis test is statistically significant at 5% significant level which suggests the null hypothesis that conversion is equal among the different scripts is rejected.&lt;/p&gt;
&lt;h3 id="conclusion-on-objective-2"&gt;Conclusion on objective 2&lt;/h3&gt;
&lt;p&gt;The objective 2 seeks to test the hypothesis that all sales scripts produce equal conversions without statistically significant difference. The conclusion drawn is to reject this hypothesis. This can be seen as a breakthrough from the exploratory data analysis yet this result equally raise another inquiry. Though, it is now clear that at least one sales script seems to do better than another for conversion, the analysis does not indicate which sales scripts has a higher conversion and how they differ. This calls for further analysis in the form of post-hoc test.&lt;/p&gt;
&lt;h3 id="recommendation-on-objective-2"&gt;Recommendation on objective 2&lt;/h3&gt;
&lt;p&gt;With the suggestion that different sales script do not produce equal conversions, it is recommended that, a deliberate effort is made to choose the script with higher rate of conversion. While this recommendation indicates there is opportunity to prioritize sales scripts for higher conversion, it has not specifically suggested which sales script in particular should be used. To provide such a data driven recommendation, further analysis is required to determine which script should be used to maximize conversion.&lt;/p&gt;
&lt;h3 id="objective-3-to-identify-which-sales-script-produces-higher-total-conversions-on-the-average"&gt;Objective 3: To identify which sales script produces higher total conversions on the average&lt;/h3&gt;
&lt;p&gt;Post-hoc test is used to satisfy this objective. It provides a better insight through a pairwise comparison of the significant level of differences in conversion between scripts and further identify which scripts are associated with higher conversion.&lt;/p&gt;
&lt;pre class="python"&gt;&lt;code&gt;
### Post-hoc test with Dunn

dunnposthoc_result = sp.posthoc_dunn(a = df_grp_sum, val_col = &amp;#39;total_conversion&amp;#39;, 
                                      group_col =&amp;#39;sales_script_type&amp;#39;, p_adjust=&amp;#39;bonferroni&amp;#39;
                                      )
                                      
dunnposthoc_result&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;              script_A      script_B      script_C
script_A  1.000000e+00  9.195689e-30  5.152029e-66
script_B  9.195689e-30  1.000000e+00  3.801421e-23
script_C  5.152029e-66  3.801421e-23  1.000000e+00&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id="result-of-post-hoc-test-of-difference-in-conversion"&gt;Result of Post-hoc test of difference in conversion&lt;/h3&gt;
&lt;p&gt;The Post-hoc test based on Dunn test indicates that there are significant differences in conversion between all the sales scripts and not just at least one. The p-value for the pairwise difference between all the scripts are statistically significant. The result shown above only captures the p-values and in order to discern the difference better, another module is used for the estimation.&lt;/p&gt;
&lt;pre class="python"&gt;&lt;code&gt;
## Post-hoc test with means of script among other feature output
posthoc_result = pg.pairwise_ttests(dv=&amp;#39;total_conversion&amp;#39;, between=&amp;#39;sales_script_type&amp;#39;, 
                                    data=df_grp_sum, parametric = &amp;#39;false&amp;#39;, alpha = 0.05,
                                    padjust = &amp;#39;bonf&amp;#39;, return_desc = &amp;#39;true&amp;#39;).round(3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;/Users/lin/Documents/python_venvs/pytorch_deepL/lib/python3.9/site-packages/pingouin/pairwise.py:27: UserWarning:

pairwise_ttests is deprecated, use pairwise_tests instead.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;posthoc_result&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;            Contrast         A         B  ...  p-adjust        BF10  hedges
0  sales_script_type  script_A  script_B  ...      bonf   1.127e+41   1.055
1  sales_script_type  script_A  script_C  ...      bonf  9.194e+113   1.784
2  sales_script_type  script_B  script_C  ...      bonf   6.315e+94   1.745

[3 rows x 17 columns]&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id="interpretation-of-post-hoc-rest-results"&gt;Interpretation of post-hoc rest results&lt;/h3&gt;
&lt;p&gt;From the Post-hoc test results, the mean of script B is 4.333, mean of script A is 8.342 and the mean of script c is 0.638. The results suggests sales script A has a significantly higher conversion than sales script B and C. In a similar manner, sales script B has a significantly higher conversion than script C in statistical terms.&lt;/p&gt;
&lt;h3 id="conclusion-drawn-for-objective-3"&gt;Conclusion drawn for objective 3&lt;/h3&gt;
&lt;p&gt;The third objective seeks to assess whether difference in conversion is statistically significant and identify which sales script has a significantly higher conversion. The results based on available data suggests that on a given demo day, the number of conversion that sales script A will produce is on the average, significantly higher than the other script types hence should be prioritize to increase conversion.&lt;/p&gt;
&lt;h3 id="recommendation-for-objective-3"&gt;Recommendation for objective 3&lt;/h3&gt;
&lt;p&gt;The sales team should consider prioritizing script A as the best bet to attain a higher conversion rate.&lt;/p&gt;
&lt;h2 id="objective-4-assessing-factors-that-influence-the-likelihood-of-a-client-to-convert"&gt;Objective 4: Assessing factors that influence the likelihood of a client to convert&lt;/h2&gt;
&lt;p&gt;The fourth objective of the analysis is to move a step further with focus on predicting how various factors influence probability of a client to sign up for the product. In order to achieve the goal of higher amount of conversion, as assessment is undertaken to model whether or not a client will sign-up. Knowing how these factors influence a client’s decision to purchase our service will enable us better target clients by managing such predictors well. For this, a review of the various variables available has been undertaken to identify potential predictors.&lt;/p&gt;
&lt;p&gt;The business problem at hand can be translated into an analytical one that requires a classification solution. In this case, the aim is to classify clients into two groups of convert and non-convert. This is the outcome we seek to predict hence the response or dependent variable. The response variable has two categories or classes as indicated, conversion and non-conversion, hence a binary classification method is chosen.&lt;/p&gt;
&lt;h2 id="method-logistic-regression"&gt;Method: Logistic regression&lt;/h2&gt;
&lt;p&gt;Logistic regression will be used for assessing how various factors influence the probability of a client to convert.&lt;/p&gt;
&lt;p&gt;Based on the available limited data and understanding of the business operations, the potential predictors identified in the data for the analysis are the following&lt;/p&gt;
&lt;ol style="list-style-type: decimal"&gt;
&lt;li&gt;&lt;p&gt;Type of sales script&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Company type&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id="description-of-predictors-and-rational-for-their-selection"&gt;Description of predictors and rational for their selection&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The relationship between the type of sales script and conversion has been our main focus for earlier analysis and yet it is still important to understand how they influence the actual decision to convert and not the total number of conversion. It can be presume, as it is the case to hypothesize, that there is a logical understanding of type of sales script being a potential influential determinant.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Company type as a potential predictor is based on the assumption that the characteristics of the company will determine its decision to finally purchase our product. Given that, these features are not disclosed, the difference in labeling of company type is taken to be directly related to difference in company characteristics.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="data-treatment-preprocessing"&gt;Data treatment / preprocessing&lt;/h3&gt;
&lt;p&gt;Modeling requires data preprocessing and the type of preprocessing undertaken is dependent on the data hence highlighted for transparency and reproducibility of analysis.&lt;/p&gt;
&lt;p&gt;First of all, the company type variable has two categories being 0 and 1, and several missing data. It is possible the missing data is an instance of data that was not provided due to some privacy concerns. Moreover, as much as 4,916 missing data is identified which constitutes a sizable amount to eliminate from the analysis and very likely to reduce the possibility of capturing as much variations as the reality is on the ground. Supported by this, is the fact that, such a large amount of missing data is enough to constitute a category in its own right for analysis and insight. Based on these available information and context, the decision was made to replace all instances of missing data for company type with ‘Others’ as another company type. This led to reclassification of company type into company group A, B and Others corresponding to 0, 1 and missing data respectively. The act of reclassifying all missing data into one group could in itself undermine the performance of the model given that companies without similar characteristics could end up in same group and not providing the needed discrimination in predicting the outcome classes. Given that the focus is not on model performance but interpretability and insights, taking extra steps for model performance is of least importance&lt;/p&gt;
&lt;pre class="python"&gt;&lt;code&gt;### make dummy variables for categorical variables with more than 2 classes

logdata_dummy = pd.get_dummies(df_dplyselect, prefix_sep=&amp;#39;_&amp;#39;, drop_first=True)

logdata_dummy&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;      is_signed  ...  company_group_company_B
0             0  ...                        1
1             0  ...                        0
2             0  ...                        0
3             0  ...                        0
4             0  ...                        0
...         ...  ...                      ...
9995          0  ...                        1
9996          1  ...                        1
9997          1  ...                        0
9998          0  ...                        1
9999          1  ...                        0

[10000 rows x 7 columns]&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;## Select columns for dependent and independent varible
y = logdata_dummy[&amp;#39;is_signed&amp;#39;]

X = logdata_dummy.drop(columns=[&amp;#39;conversion_not_convert&amp;#39;, &amp;#39;is_signed&amp;#39;, &amp;#39;request_to_1streach_timelength_minutes_&amp;#39;])
## Split data into training and validation dataset
train_X, valid_X, train_y, valid_y = train_test_split(X, y, test_size=0.4, random_state=1)
## Fit logistic regression on training dataset

reg_result = pg.logistic_regression(train_X, train_y)

reg_result&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;                        names      coef  ...  CI[2.5%]  CI[97.5%]
0                   Intercept -0.026989  ... -0.107403   0.053425
1  sales_script_type_script_B -0.049030  ... -0.157495   0.059436
2  sales_script_type_script_C  0.243650  ... -0.223056   0.710357
3     company_group_company_A  0.045234  ... -0.087648   0.178115
4     company_group_company_B  0.103170  ... -0.013828   0.220169

[5 rows x 7 columns]&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id="insights-from-multiple-logistic-regression"&gt;Insights from multiple logistic regression&lt;/h3&gt;
&lt;p&gt;The logistic regression for predicting the probability that a client will sign-up produced a non-significant p-value for all cases. Script A was used as reference point for script B and C. Thus, with a co-efficient of -0.049 for script B, it is suggested that using script B results in a decrease in the log odds of signing a client by 0.049 in comparison to script A. This is however statistically insignificant. Script C has a co-efficient of 0.24 which suggests that when a sales manager uses script C, the log odds of signing a client increases by 0.24 compared to script A which is statistically insignificant.&lt;/p&gt;
&lt;p&gt;With regards to prioritizing companies to reach out to in order to increase conversion, when sales managers take a session with company A or company B, the log odds of landing a conversion increases compared to when they target companies categorized as ‘Others’ in this analysis. The result is however statistically insignificant.&lt;/p&gt;
&lt;h3 id="recommendation-for-objective-4"&gt;Recommendation for objective 4&lt;/h3&gt;
&lt;p&gt;From the analysis undertaken, it is recommended that the sales team consider not prioritizing demo sessions with company type ‘Others’. This should however not be treated as a de facto discrimination favouring company A and B to increase conversion given that the results are statistically insignificant.&lt;/p&gt;
&lt;h4 id="recommendation-on-improving-data-collection-for-better-analytics-of-greater-statistical-power"&gt;Recommendation on improving data collection for better analytics of greater statistical power&lt;/h4&gt;
&lt;p&gt;Some potential predictors that could be included in the model are available in the dataset but the data format or how it is organized is not suitable for further transformation and inclusion in the model. This is highlighted with the suggestion of how to organize such data to lay foundation for a cross-functional and collaborative teamwork with other departments that generate data to be analyze.&lt;/p&gt;
&lt;p&gt;source* - marketing source of the lead. This variable is very important in order to gain insight into the effectiveness of marketing strategy in producing conversion. Currently, the variable has 28 unique categories which requires that they are classified into logical groups of similarity, for instance organic source, referral, direct or indirect source, in order to be used as a categorical predictor. Given that actual names of the source have been hashed, such exercise is currently not possible hence the exclusion of this potential predictor. Thus, it is recommended that the actual names of the marketing source be provided to consider the possibility of using them as potential predictors.&lt;/p&gt;
&lt;p&gt;sales_group_name* The type of sales team is likely to be a potential predictor influencing conversion. This variable has 22 categories which needs to be grouped in order to be used a categorical predictor. To do this, there is the need to provide further description about the various groups in order to classify them into logical groups for predicting conversion.&lt;/p&gt;
&lt;h2 id="summary"&gt;Summary&lt;/h2&gt;
&lt;p&gt;This analysis aims to demonstrate how to provide data-driven evidence to support decision making by the product team with the aim of increasing conversion.&lt;/p&gt;
&lt;p&gt;A major tool that the sales team use to aid conversion is sales script of which there are different variant. Thus, the main focus of the analysis is to assess the role of sales script as a driver of conversion among others. To achieve this, the business problem was conceptualized and translated into several hypothesis.&lt;/p&gt;
&lt;p&gt;Among others, the analysis sought to test the hypothesis that there is statistically significant relationship between type of sales script and conversion and that there is statistically significant difference in the total conversion made using different script. Further, the analysis sought to assess how sales script and company type influences the likelihood of a company signing up during and after a demo session.&lt;/p&gt;
&lt;h3 id="call-to-action"&gt;Call to action&lt;/h3&gt;
&lt;ol style="list-style-type: decimal"&gt;
&lt;li&gt;&lt;p&gt;It is recommended that sales script A be prioritized when deciding on the type of sales script to use for a demo. This is informed by the fact that sales script A had higher average statistically significant conversion compared to the other scripts. It should be noted that even though the multiple logistic regression suggested that the odds of signing up slightly increases in favour of script C, this result was not statistically significant. Moreover, other variables such as company type may have contributed as a confounding factor&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The analysis has underscored that while it is possible to derive insights from the available data, there is the need to include other potential predictors in the data collected. This is informed by the fact that, the model developed does not adequately explain variation in conversion.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;</description>
      <distill:md5>e078fd2cef6c75db6050f49372ddaf96</distill:md5>
      <guid>https://addurl/posts/2022-10-11-marketing-analysis-hypothesis-testing-to-determine-best-sales-script-for-increasing-conversion</guid>
      <pubDate>Tue, 11 Oct 2022 00:00:00 +0000</pubDate>
      <media:content url="https://addurl/posts/2022-10-11-marketing-analysis-hypothesis-testing-to-determine-best-sales-script-for-increasing-conversion/marketing-analysis-hypothesis-testing-to-determine-best-sales-script-for-increasing-conversion_files/figure-html5/unnamed-chunk-2-5.png" medium="image" type="image/png" width="1228" height="921"/>
    </item>
    <item>
      <title>Predicting review sentiments using NLP and Deep Learning with Pytorch</title>
      <dc:creator>Linus Agbleze</dc:creator>
      <link>https://addurl/posts/2022-10-02-classifying-review-statements-using-deep-learning-with-pytorch</link>
      <description>This post describes an end to end project on predicting review sentiments by customers using Natural Language Processing and Deep Learning with Pytorch.</description>
      <category>NLP</category>
      <category>Python</category>
      <category>Deep Learning</category>
      <guid>https://addurl/posts/2022-10-02-classifying-review-statements-using-deep-learning-with-pytorch</guid>
      <pubDate>Sun, 02 Oct 2022 00:00:00 +0000</pubDate>
      <media:content url="https://addurl/posts/2022-10-02-classifying-review-statements-using-deep-learning-with-pytorch/classifying-review-statements-using-deep-learning-with-pytorch_files/figure-html5/unnamed-chunk-3-1.png" medium="image" type="image/png" width="1640" height="1068"/>
    </item>
    <item>
      <title>One-Hot representation: Encoding text for Natural Language Processing (NLP) project -Part I</title>
      <dc:creator>Linus Agbleze</dc:creator>
      <link>https://addurl/posts/2022-09-27-encoding-text-for-natural-language-processing-nlp-project</link>
      <description>One Hot encoding is one of techniques employed in representing texts to natural language processing task. This post discusses one-hot encoding as part I of discussions on text representation in NLP</description>
      <category>NLP</category>
      <category>Python</category>
      <guid>https://addurl/posts/2022-09-27-encoding-text-for-natural-language-processing-nlp-project</guid>
      <pubDate>Tue, 27 Sep 2022 00:00:00 +0000</pubDate>
      <media:content url="https://addurl/posts/2022-09-27-encoding-text-for-natural-language-processing-nlp-project/encoding-text-for-natural-language-processing-nlp-project_files/figure-html5/unnamed-chunk-2-1.png" medium="image" type="image/png" width="1344" height="1344"/>
    </item>
    <item>
      <title>Term-Frequency Inverse-Documnent-Frequency (TF IDF): Encoding text for Natural Language Processing (NLP) project -Part II</title>
      <dc:creator>Linus Agbleze</dc:creator>
      <link>https://addurl/posts/2022-09-27-term-frequency-inverse-documnent-frequency-tf-idf-encoding</link>
      <description>A short description of the post.</description>
      <category>NLP</category>
      <category>Python</category>
      <guid>https://addurl/posts/2022-09-27-term-frequency-inverse-documnent-frequency-tf-idf-encoding</guid>
      <pubDate>Tue, 27 Sep 2022 00:00:00 +0000</pubDate>
      <media:content url="https://addurl/posts/2022-09-27-term-frequency-inverse-documnent-frequency-tf-idf-encoding/term-frequency-inverse-documnent-frequency-tf-idf-encoding_files/figure-html5/unnamed-chunk-2-1.png" medium="image" type="image/png" width="1344" height="1344"/>
    </item>
    <item>
      <title>Online Marketing Ads click prediction: End-to-End workflow for machine learning solution</title>
      <dc:creator>Linus Agbleze</dc:creator>
      <link>https://addurl/posts/2022-09-10-sales-hypothesis-analysis</link>
      <description>


&lt;h2 id="introduction"&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Online space is now a ubiquitous market place for almost every company; not just eCommerce. Just like office spaces and sales centers, there is high competition for who gets offered the place to display and sell goods and services. Ads campaign and online marketing in general is often seen as the silver bullet by most marketing teams so much so that budget for traditional word of mouth marketing or sign-post are increasingly being redistributed among several online marketing channels.This highlight the demand for online marketing by advertisers. Taking note of this trend, a number of online businesses have sprung up as online space providers for marketers with the promise of linking marketers to end-consumers to increase sales and providing end-users with the best offer. This promise in itself has become a business problem where online marketing platforms introduces bidding systems that not only identifies which ads get displayed on the platform but also selects Ads that is relevant and offer best value to users to enable the platform to deliver on their promise.&lt;/p&gt;
&lt;p&gt;While this problem could be tackled from both ends, that is either the marketing team designing ads that increase conversion or platform providers selecting ads that are likely to drive engagement hence satisfy both the advertiser and platform visitor, the latter is addressed in this post.&lt;/p&gt;
&lt;p&gt;Thus, the analysis is undertaken for a business that provides it platform for ads advertisement with the aim of selecting ads that are likely to have higher clicks in order to fulfill the promise of optimizing online marketing for clients and offering the best deal to platform visitors. Platform visitors aim to click ads and patronize services that offer value for money.&lt;/p&gt;
&lt;h2 id="problem-statement"&gt;Problem Statement&lt;/h2&gt;
&lt;p&gt;For an anonymous online advertisement platform provider with business clients advertising their accommodation services, the business problem of deciding which ads gets to be advertise on the platform to online visitors requires a strategic decision making which seeks to balance the satisfaction between end-users at opposite divide; mainly advertisers who run campaigns and users (consumers) who consume (click) those campaign ads. Identified among others is the the number of clicks as a Key Performance Indicator (KPI) capturing satisfaction by both users and advertisers. From this, the higher the number of clicks, the higher the perceived campaign success by advertisers and the higher the user utility (to some extent). With this understanding, a higher number of click is likely to result from displaying ads with accommodation characteristics that satisfy users’ interests. This suggests that predicting number of clicks can be done by identifying patterns in accommodation Ads’ characteristics that compel users to click more often.&lt;/p&gt;
&lt;p&gt;Thus, the number of clicks is the target variable and variables that capture various properties of the accommodation ads are the predictor variable.&lt;/p&gt;
&lt;p&gt;In order to translate this understanding into a technical solution that can be communicated to non-technical audience later, there is the need to capture the prevailing challenge as a snapshot with a problem design framework.&lt;/p&gt;
&lt;p&gt;The Situation Complication Question (SCQ) framework will serve as a great tool for that purpose. A key element is identifying the stakeholder and end-user of the solution to be provided. Hypothetically, the bidding team is tasked with the problem of Ads bidding and number of clicks is one of the key components of their bidding system hence identified as a stakeholder and end-user of the analysis.&lt;/p&gt;
&lt;p&gt;The SCQ is depicted below;&lt;/p&gt;
&lt;p&gt;&lt;img src="fileb9457bbcc12d_files/figure-html/unnamed-chunk-4-1.png" width="937" style="display: block; margin: auto;" /&gt;&lt;/p&gt;
&lt;h3 id="identifying-variables-in-the-data"&gt;Identifying variables in the data&lt;/h3&gt;
&lt;p&gt;In developing an algorithm for prediction, identifying the variables to use and how to categorize them is important. The following is deduced;&lt;/p&gt;
&lt;h4 id="target-response-outcome-variable"&gt;Target / Response / Outcome variable&lt;/h4&gt;
&lt;p&gt;n clicks: Continuous variable&lt;/p&gt;
&lt;h4 id="predictor-feature-variables"&gt;Predictor / feature variables&lt;/h4&gt;
&lt;p&gt;• city id: Categorical&lt;/p&gt;
&lt;p&gt;• content score: ordinal&lt;/p&gt;
&lt;p&gt;• n images: discrete&lt;/p&gt;
&lt;p&gt;• distance to center: continuous&lt;/p&gt;
&lt;p&gt;• avg rating: discrete&lt;/p&gt;
&lt;p&gt;• n reviews: continuous&lt;/p&gt;
&lt;p&gt;• avg rank: ordinal&lt;/p&gt;
&lt;p&gt;• avg price: Continuous&lt;/p&gt;
&lt;p&gt;• avg saving percent: discrete&lt;/p&gt;
&lt;p&gt;By identifying the type of variable, appropriate visualization can be undertaken for different variables during exploratory data analysis.&lt;/p&gt;
&lt;h2 id="exploratory-data-analysis"&gt;Exploratory data analysis&lt;/h2&gt;
&lt;p&gt;Given that target variable is present, a supervised machine learning algorithm will be used for the prediction exercise. Generally, algorithms can be seen to belong to two categories, namely parametric and non-parametric. Deciding on the category from which an algorithm is chosen for modeling is dependent on whether the dataset exhibits characteristics that satisfy certain assumptions. This need to be verified by undertaking exploratory analysis and visualization hence the focus of this section. The insights gained from the exploratory analysis serves as basis on which to narrow down the group of algorithms that will produce good results&lt;/p&gt;
&lt;p&gt;The implementation of the exploratory analysis is highlighted below beginning with importing packages and the dataset&lt;/p&gt;
&lt;pre class="python"&gt;&lt;code&gt;# import packages
import pandas as pd
import seaborn as sns
from sklearn.model_selection import GridSearchCV, train_test_split
import numpy as np
import matplotlib.pyplot as plt
import plotly.express as px
from plotnine import ggplot, geom_point, geom_smooth, ggtitle, aes
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import  HistGradientBoostingRegressor, BaggingRegressor
from xgboost import XGBRFRegressor
from dmba import regressionSummary
from sklearn.metrics import make_scorer&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;# %% load data
data = pd.read_csv(r&amp;quot;Data/train_set.csv&amp;quot;)

print(f&amp;quot;Data types \n {data.dtypes}&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Data types 
 hotel_id              float64
city_id               float64
content_score         float64
n_images              float64
distance_to_center    float64
avg_rating            float64
stars                 float64
n_reviews             float64
avg_rank              float64
avg_price             float64
avg_saving_percent    float64
n_clicks                int64
dtype: object&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;print(data.head())&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;       hotel_id   city_id  ...  avg_saving_percent  n_clicks
0  9.767406e+10  134520.0  ...                18.0         0
1  9.768889e+10  133876.0  ...                28.0         4
2  9.811544e+10  133732.0  ...                27.0        44
3  9.824279e+10   43772.0  ...                 2.0         4
4  9.833438e+10   50532.0  ...                 0.0        10

[5 rows x 12 columns]&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id="basic-descriptive-statistics"&gt;Basic descriptive statistics&lt;/h3&gt;
&lt;p&gt;One of the first checks to do during exploratory analysis is to obtain a description of the data by identifying the mean, median, quantiles among others to enable understanding of the data distribution among others. This is done as follows;&lt;/p&gt;
&lt;pre class="python"&gt;&lt;code&gt;data.describe()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;           hotel_id        city_id  ...  avg_saving_percent       n_clicks
count  3.964870e+05  395979.000000  ...       396317.000000  396487.000000
mean   1.326304e+11  149193.465376  ...            7.179601      13.781980
std    1.033722e+11  219189.285044  ...           13.081529     123.572896
min    1.557962e+08       2.000000  ...            0.000000       0.000000
25%    4.062255e+10   32014.000000  ...            0.000000       0.000000
50%    1.087280e+11   55122.000000  ...            0.000000       0.000000
75%    2.281935e+11  137464.000000  ...           10.000000       2.000000
max    3.237114e+11  878736.000000  ...           99.000000   13742.000000

[8 rows x 12 columns]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From the above, the maximum avg_price is 8000 while the 75th percentile is 120 which suggests there could be outliers. This high difference between the maximum and the 75th percentile is also noted for n_views, n_clicks among others. Outliers are better visualize with Boxplot and are a key consideration in determining which model to choose as some models are more robust to outliers than others.&lt;/p&gt;
&lt;h4 id="missing-values"&gt;Missing values&lt;/h4&gt;
&lt;p&gt;An important consideration for analysis and modeling is the proportion of missing values. This usually defines the type of preprocessing tasks which could include imputing missing values or choosing a model that handles missing values well. Hence, the number of missing values are analyzed below to make such decision.&lt;/p&gt;
&lt;pre class="python"&gt;&lt;code&gt;# number of missing values per variable
data.isnull().sum()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;hotel_id                   0
city_id                  508
content_score            508
n_images                 509
distance_to_center       529
avg_rating            110398
stars                    562
n_reviews                529
avg_rank                   0
avg_price                170
avg_saving_percent       170
n_clicks                   0
dtype: int64&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;
# number of data points (observations)
n_obs = data.shape[0]

# number of observation after removing all missing values
n_obs_drop = data.dropna().shape[0]


narm_percent = (n_obs_drop/n_obs) * 100

print(f&amp;quot;Percentage of data left if all missing values are removed: {narm_percent: .2f}%&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Percentage of data left if all missing values are removed:  72.14%&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;72% (286,026) of data will remain after removing all missing values. While a sizeable amount of data will still be left for modeling, a lost of 28% of data could be quite impactful on the model to be developed particularly when attempt is made to impute missing values. Dropping all missing values will lead to loss of information while an imputation could lead to introducing a sizable amount of noise or “fake” data that may deviate from reality. In this regard, a decision is made in favour of choosing an algorithm that natively handles and is less impacted by missing values.&lt;/p&gt;
&lt;h4 id="data-visualization-to-ascertain-certain-assumptions-required-by-some-models"&gt;Data visualization to ascertain certain assumptions required by some models&lt;/h4&gt;
&lt;p&gt;Generally, parametric models like linear regression requires data to be normally distributed and a linear relationship to exist between the predictor and target variable (s). Among others, data visualization is one of the techniques used to verify such assumptions. Various plots are used to visualize certain characteristics of the data to ascertain if certain assumptions are met for certain models to be used.&lt;/p&gt;
&lt;p&gt;To investigate this, histogram is used to visualize the distribution of data for continuous variables. This is implemented below.&lt;/p&gt;
&lt;pre class="python"&gt;&lt;code&gt;
variables=[&amp;#39;content_score&amp;#39;, &amp;#39;n_images&amp;#39;,
            &amp;#39;distance_to_center&amp;#39;, &amp;#39;avg_rating&amp;#39;, 
            &amp;#39;stars&amp;#39;, &amp;#39;n_reviews&amp;#39;, &amp;#39;avg_rank&amp;#39;,
            &amp;#39;avg_price&amp;#39;, &amp;#39;avg_saving_percent&amp;#39;,
            &amp;#39;n_clicks&amp;#39;
            ]
            &lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;# function to create histogram 
def plot_histogram(data: pd.DataFrame, colname: str):
    &amp;quot;&amp;quot;&amp;quot;Plot the distribution of variable using a histogram

    Args:
        data (pd.DataFrame): Data to use for plotting
        
        colname (str): column name or name of variable to plot 
    &amp;quot;&amp;quot;&amp;quot;
    img = data[colname].plot(kind=&amp;#39;hist&amp;#39;, 
                       title=f&amp;#39;Distribution of {colname}&amp;#39;
                       )
    plt.show()
    &lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;# plot histogram of all variables  
for colname in variables:
        plot_histogram(data=data, colname=colname)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="https://addurl//posts/2022-09-10-sales-hypothesis-analysis/sales-hypothesis-analysis_files/figure-html5/unnamed-chunk-2-1.png" width="672" style="display: block; margin: auto;" /&gt;&lt;img src="https://addurl//posts/2022-09-10-sales-hypothesis-analysis/sales-hypothesis-analysis_files/figure-html5/unnamed-chunk-2-2.png" width="672" style="display: block; margin: auto;" /&gt;&lt;img src="https://addurl//posts/2022-09-10-sales-hypothesis-analysis/sales-hypothesis-analysis_files/figure-html5/unnamed-chunk-2-3.png" width="672" style="display: block; margin: auto;" /&gt;&lt;img src="https://addurl//posts/2022-09-10-sales-hypothesis-analysis/sales-hypothesis-analysis_files/figure-html5/unnamed-chunk-2-4.png" width="672" style="display: block; margin: auto;" /&gt;&lt;img src="https://addurl//posts/2022-09-10-sales-hypothesis-analysis/sales-hypothesis-analysis_files/figure-html5/unnamed-chunk-2-5.png" width="672" style="display: block; margin: auto;" /&gt;&lt;img src="https://addurl//posts/2022-09-10-sales-hypothesis-analysis/sales-hypothesis-analysis_files/figure-html5/unnamed-chunk-2-6.png" width="672" style="display: block; margin: auto;" /&gt;&lt;img src="https://addurl//posts/2022-09-10-sales-hypothesis-analysis/sales-hypothesis-analysis_files/figure-html5/unnamed-chunk-2-7.png" width="672" style="display: block; margin: auto;" /&gt;&lt;img src="https://addurl//posts/2022-09-10-sales-hypothesis-analysis/sales-hypothesis-analysis_files/figure-html5/unnamed-chunk-2-8.png" width="672" style="display: block; margin: auto;" /&gt;&lt;img src="https://addurl//posts/2022-09-10-sales-hypothesis-analysis/sales-hypothesis-analysis_files/figure-html5/unnamed-chunk-2-9.png" width="672" style="display: block; margin: auto;" /&gt;&lt;img src="https://addurl//posts/2022-09-10-sales-hypothesis-analysis/sales-hypothesis-analysis_files/figure-html5/unnamed-chunk-2-10.png" width="672" style="display: block; margin: auto;" /&gt;&lt;/p&gt;
&lt;p&gt;The result shows that most of the variables are right skewed. The variable content_score is left skewed (not too bad). The variables n_images, distance_to_center, avg_rating, n_reviews, avg_rank, avg_price, avg_saving_percent and n_clicks are right skewed&lt;/p&gt;
&lt;p&gt;city_id is a categorical variable and a number of ways exist to treat such variables including OneHot encoding method. This method has the disadvantage of significantly increasing the dimensionality of the dataset when several categorical variables are present or even when a single categorical variable with numerous classes exist. This could lead to significant increase in computation time of model. Hence, the decision on how to handle city_id variable is made by first understanding number of classes it has below.&lt;/p&gt;
&lt;pre class="python"&gt;&lt;code&gt;data[&amp;#39;city_id&amp;#39;].nunique()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;33213&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;city_id is a high cardinality categorical variable with 33213 unique values. This will make OneHot encoding an expensive exercise for modeling. Worthy of notice is that the city_id has been hashed as numeric values which makes it acceptable for the Scikit-learn API.&lt;/p&gt;
&lt;h4 id="boxplot-to-visualize-outliers"&gt;Boxplot to visualize outliers&lt;/h4&gt;
&lt;p&gt;As indicated earlier, a key factor in deciding which algorithm to use is the distribution of the target and predictor variables. Some algorithms are influence by the presence of outliers hence analyzed to make an inform decision on which class of algorithm to choose from.&lt;/p&gt;
&lt;pre class="python"&gt;&lt;code&gt;# function to create boxplot
def make_boxplot(data: pd.DataFrame, variable_name: str):
    &amp;quot;&amp;quot;&amp;quot;This function accepts a data and variable name and returns a boxplot

    Args:
        data (pd.DataFrame): Data to visualize
        variable_name (str): variable to visualize with boxplot
    &amp;quot;&amp;quot;&amp;quot;
    fig = px.box(data_frame=data, y = variable_name,
                 template=&amp;#39;plotly_dark&amp;#39;, 
                 title = f&amp;#39;Boxplot to visualize outliers in {variable_name}&amp;#39;
                 )
    fig.show()&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;for var in variables:
    make_boxplot(data=data, variable_name=var)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From the boxplot of content_score, the least score of 7 is quite distant from the lower fence and could be seen as an outlier. The boxplot of other variables such as n_images, distance_to_center, avg_rating, n_reviews, avg_price,n_clicks suggest outliers are present upon visual inspection. This information informs our decision making process about which model to choose. From the visualization, a decision is made in favour of choosing a model that is fairly robust against outliers instead of removing or imputing the outliers altogether.&lt;/p&gt;
&lt;h3 id="scatterplot-to-visualize-outliers"&gt;Scatterplot to visualize outliers&lt;/h3&gt;
&lt;p&gt;In order to investigate whether or not a linear relationship between the target variable and predictor variables exists, a scatterplot is used to visualize them. Thus, all the predictor variables are plotted against n_clicks on the y-axis This is implemented below;&lt;/p&gt;
&lt;pre class="python"&gt;&lt;code&gt;def plot_scatterplot(data: pd.DataFrame,
                 x_colname: str,
                 y_colname: str = &amp;#39;n_clicks&amp;#39;):
    &amp;quot;&amp;quot;&amp;quot; Scatterplot to visualize relationship between two variables. 
    Args:
        data (pd.DataFrame): Data which contains variables to plot
        
        y_colname (str): column name (variable) to plot on y-axis
        x_colname (str): column name (variable) to plot on x-axis
    &amp;quot;&amp;quot;&amp;quot;
    print(
        (ggplot(data=data, 
                mapping=aes(y=y_colname, x=x_colname)
                ) 
                + geom_point() + geom_smooth(method=&amp;#39;lm&amp;#39;)
                + ggtitle(f&amp;#39;Scatter plot to visualize relationship between {y_colname} and {x_colname}&amp;#39;
                    )
        )
    )&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;#
for colname in variables:
    plot_scatterplot(data=data, x_colname=colname)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="https://addurl//posts/2022-09-10-sales-hypothesis-analysis/sales-hypothesis-analysis_files/figure-html5/unnamed-chunk-2-21.png" width="614" style="display: block; margin: auto;" /&gt;&lt;img src="https://addurl//posts/2022-09-10-sales-hypothesis-analysis/sales-hypothesis-analysis_files/figure-html5/unnamed-chunk-2-22.png" width="614" style="display: block; margin: auto;" /&gt;&lt;img src="https://addurl//posts/2022-09-10-sales-hypothesis-analysis/sales-hypothesis-analysis_files/figure-html5/unnamed-chunk-2-23.png" width="614" style="display: block; margin: auto;" /&gt;&lt;img src="https://addurl//posts/2022-09-10-sales-hypothesis-analysis/sales-hypothesis-analysis_files/figure-html5/unnamed-chunk-2-24.png" width="614" style="display: block; margin: auto;" /&gt;&lt;img src="https://addurl//posts/2022-09-10-sales-hypothesis-analysis/sales-hypothesis-analysis_files/figure-html5/unnamed-chunk-2-25.png" width="614" style="display: block; margin: auto;" /&gt;&lt;img src="https://addurl//posts/2022-09-10-sales-hypothesis-analysis/sales-hypothesis-analysis_files/figure-html5/unnamed-chunk-2-26.png" width="614" style="display: block; margin: auto;" /&gt;&lt;img src="https://addurl//posts/2022-09-10-sales-hypothesis-analysis/sales-hypothesis-analysis_files/figure-html5/unnamed-chunk-2-27.png" width="614" style="display: block; margin: auto;" /&gt;&lt;img src="https://addurl//posts/2022-09-10-sales-hypothesis-analysis/sales-hypothesis-analysis_files/figure-html5/unnamed-chunk-2-28.png" width="614" style="display: block; margin: auto;" /&gt;&lt;img src="https://addurl//posts/2022-09-10-sales-hypothesis-analysis/sales-hypothesis-analysis_files/figure-html5/unnamed-chunk-2-29.png" width="614" style="display: block; margin: auto;" /&gt;&lt;img src="https://addurl//posts/2022-09-10-sales-hypothesis-analysis/sales-hypothesis-analysis_files/figure-html5/unnamed-chunk-2-30.png" width="614" style="display: block; margin: auto;" /&gt;&lt;/p&gt;
&lt;p&gt;The scatterplots show that none of the predictors has a linear relationship with the response variable. Hence an insight is being gain that parametric algorithms such as linear regression with the assumption of linear relationship are less likely to produce a convincing prediction with low error.&lt;/p&gt;
&lt;h3 id="correlation-analysis-to-check-for-multicollinearity"&gt;Correlation analysis to check for multicollinearity&lt;/h3&gt;
&lt;p&gt;Another exploration to do for the data is checking for multicollinearity among predictor variables given that some algorithms assume that the variables are not strongly correlated to each other. Strong correlation between variables implies the variables are supplying similar information to the algorithm hence dimension reduction technique could be used to reduce or select only variables that enable the algorithm to gain new insights from the data and improve predictive power.&lt;/p&gt;
&lt;p&gt;Correlation analysis is undertaken on the predictor variables to check for multicollinearity as follows;.&lt;/p&gt;
&lt;pre class="python"&gt;&lt;code&gt;# &amp;#39;n_clicks&amp;#39; is the target variable
## all others with exception of &amp;#39;hotel_id&amp;#39; are predictors
y = data[&amp;#39;n_clicks&amp;#39;]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;hotel_id is not included as predictor because it is just an identifier and offers no real insight about a hotel Ad characteristics for modeling.&lt;/p&gt;
&lt;pre class="python"&gt;&lt;code&gt;
## create predictor variables data
X = data.drop(columns=[&amp;#39;hotel_id&amp;#39;, &amp;#39;n_clicks&amp;#39;])
X.head()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;    city_id  content_score  n_images  ...  avg_rank  avg_price  avg_saving_percent
0  134520.0           70.0       2.0  ...    17.550      81.64                18.0
1  133876.0           67.0       3.0  ...    17.383     189.38                28.0
2  133732.0           39.0       3.0  ...    16.438      57.63                27.0
3   43772.0           59.0       8.0  ...     7.000      72.16                 2.0
4   50532.0           66.0       1.0  ...    12.564     173.25                 0.0

[5 rows x 10 columns]&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;#%% create correlation
corr = X.corr()
corr&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;                     city_id  content_score  ...  avg_price  avg_saving_percent
city_id             1.000000      -0.038639  ...  -0.052988           -0.057525
content_score      -0.038639       1.000000  ...  -0.074976            0.388301
n_images           -0.003005       0.012113  ...  -0.001429            0.004675
distance_to_center  0.015587      -0.113261  ...   0.007168           -0.014881
avg_rating          0.043550      -0.135439  ...   0.211038           -0.153566
stars              -0.043517       0.535937  ...  -0.010134            0.468344
n_reviews          -0.091224       0.289233  ...  -0.002831            0.372850
avg_rank           -0.029956      -0.142255  ...   0.016033           -0.121451
avg_price          -0.052988      -0.074976  ...   1.000000           -0.028133
avg_saving_percent -0.057525       0.388301  ...  -0.028133            1.000000

[10 rows x 10 columns]&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;# Create a mask to hide the upper triangle
mask = np.zeros_like(corr, dtype=np.bool)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;string&amp;gt;:1: DeprecationWarning:

`np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;mask[np.triu_indices_from(mask)] = True

# visualize correlation matrix
sns.heatmap(corr, mask=mask, cmap=sns.color_palette(&amp;quot;GnBu_d&amp;quot;), vmax=.3, center=0, 
            square=True, linewidths=.5, cbar_kws={&amp;quot;shrink&amp;quot;: .5})&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="https://addurl//posts/2022-09-10-sales-hypothesis-analysis/sales-hypothesis-analysis_files/figure-html5/unnamed-chunk-2-41.png" width="614" style="display: block; margin: auto;" /&gt;&lt;/p&gt;
&lt;p&gt;The correlation analysis shows a weak correlation between the predictor variables hence multicollinearity is absent.&lt;/p&gt;
&lt;h3 id="using-insights-gained-from-exploratory-analysis-to-inform-modeling-approach"&gt;Using insights gained from exploratory analysis to inform modeling approach&lt;/h3&gt;
&lt;p&gt;The findings of non-linear relationship between the predictors and the outcome variable(s), presence of outliers, and sizable missing values suggest that a non-parametric model that handles non-linear relationship, outliers and missing values well needs to be used. Moreover, the business goal is to achieve a good precision rather than interpretability of the model.&lt;/p&gt;
&lt;p&gt;On the basis of the findings from the exploratory analysis, decision tree model will be used. Before that, a critical aspect of the findings was that missing data is present and has to handled. For this, a model that natively handles missing data as part of the modeling process is explored. Hence, HistGradientBoostingRegressor from the sklearn library is implemented because it is a decison tree based model with an inbuilt handling of missing data.&lt;/p&gt;
&lt;p&gt;The implementation is as follows;&lt;/p&gt;
&lt;h3 id="splitting-into-training-and-validation-dataset"&gt;Splitting into Training and validation dataset&lt;/h3&gt;
&lt;p&gt;The decision on how the data is proportioned for learning and evaluation is one that is sometimes subjective. For this exercise, 70% of the data is used for training and 30% for validation. Given that, there is relatively enough data, 70% dataset will likely provide enough data points to learn and derived as much insight from and 30% will probably be enough to test the model on data points capturing varying characteristics that are unknown to the model. The implementation is as follows;&lt;/p&gt;
&lt;pre class="python"&gt;&lt;code&gt;# split the data into train and validation set
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=.3, random_state=2022)&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id="define-baseline-model"&gt;Define baseline model&lt;/h3&gt;
&lt;p&gt;Modeling can be an iterative process in attempt to optimize and arrive at the best generalizable model.&lt;/p&gt;
&lt;p&gt;A key question that needs to be answered before beginning the modeling process is at what level of performance do we accept the model as doing a better job than random guesses hence adding value to the business. There is the need to ascertain that efforts and resources put into developing a model is worthwhile than doing nothing and making random guesses (in which case those resources are also saved)&lt;/p&gt;
&lt;p&gt;To provide an objective answer to this question requires defining a baseline model that the model to be developed needs to perform better than in order to be accepted for deployment. This implies that, all algorithms used to develop the model including those for which hyperparameters have been tuned need to do better than the baseline model. Once this is satisfied, a comparison between the models are made to select the best model using the evaluation metric define. In this case, the evaluation metric is defined to be Root Mean Squared Error (RMSE).&lt;/p&gt;
&lt;p&gt;In defining the baseline model, an assumption is made that, for each observation, the average of all clicks in the training data is predicted as the number of clicks for the validation data. With this, the error is estimated for the baseline model for which the actual model needs to do better. Thus, the baseline model is estimated as follows,&lt;/p&gt;
&lt;pre class="python"&gt;&lt;code&gt;#%% mean clicks in the training data
mean_clicks = y_train.mean()
print(f&amp;quot;Average click is {mean_clicks: .3f}&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Average click is  14.078&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With the assumption that prediction for all observations in the validation dataset is the average clicks in training data, the the RMSE estimated as follows;&lt;/p&gt;
&lt;pre class="python"&gt;&lt;code&gt;# Baseline model RMSE, Mean Absolute Error (MAE)

# function to calculate RMSE
def rmse(y_true, y_pred) -&amp;gt; float:
    return np.sqrt(np.mean((y_true - y_pred)**2))
    
baseline_rmse = rmse(y_val, mean_clicks)
print(f&amp;quot;Validation data RMSE for baseline model: {baseline_rmse: .3f}&amp;quot;)
# MAE for baseline model&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Validation data RMSE for baseline model:  114.057&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;print(f&amp;quot;Validation data MAE for baseline model: {abs(y_val - mean_clicks).mean(): .3f}&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Validation data MAE for baseline model:  22.300&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From the above analysis, the baseline model metric performance is 114.057 for RMSE and 22.3 for MAE. Thus for the actual model to be accepted as worthy of adding value to business, it has to achieve an error better (lower) than RMSE estimated for baseline model.&lt;/p&gt;
&lt;h3 id="developing-model-for-click-prediction-histgradientboostingregressor"&gt;Developing model for click prediction – HistGradientBoostingRegressor&lt;/h3&gt;
&lt;p&gt;After defining the baseline model performance, the actual model development is done as follows;&lt;/p&gt;
&lt;pre class="python"&gt;&lt;code&gt;#%% Histogram-based Gradient Boosting Regression Tree.
## chosen because it has an inbuilt handling on missing data

# define the HistGradientBoostingRegressor model instance
# random state set to achieve reproducible results  
histgb = HistGradientBoostingRegressor(random_state=2022)

# fit the model on the training dataset
histgb.fit(X_train, y_train)&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id="determining-extent-to-which-model-captures-pattern-in-the-data"&gt;Determining extent to which model captures pattern in the data&lt;/h3&gt;
&lt;p&gt;A common metric used to understand how well the model explains the data and capture variance present is the co-efficient of determination. For regression models, the score function is used to estimate that and is implemented below;&lt;/p&gt;
&lt;pre class="python"&gt;&lt;code&gt;# Co-efficient of determination
histgb_r2score = histgb.score(X_train, y_train)

print(f&amp;quot;HistGradientBoosting explains {(histgb_r2score * 100): .2f}% of variance in the training dataset&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;HistGradientBoosting explains  57.68% of variance in the training dataset&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A co-efficient of 0.577 indicates that only about 57.7% of variance in the training data is captured by the model leaving about 42.3% unaccounted for. Hence a lot of insights and patterns in the data has not been captured by the model.&lt;/p&gt;
&lt;h3 id="evaluation-of-model"&gt;Evaluation of Model&lt;/h3&gt;
&lt;p&gt;RMSE and MAE (Mean Absolute Error) is used to evaluate the model as follows&lt;/p&gt;
&lt;pre class="python"&gt;&lt;code&gt;# model evaluation on training data
print(&amp;quot;Metrics for HistGradientBoosting on Training dataset&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Metrics for HistGradientBoosting on Training dataset&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;regressionSummary(y_train, histgb.predict(X_train))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Regression statistics

               Mean Error (ME) : 0.0206
Root Mean Squared Error (RMSE) : 82.9041
     Mean Absolute Error (MAE) : 13.3683&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;# model evaluation on validation data
print(f&amp;quot;Metrics for HistGradientBoosting on Validation dataset&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Metrics for HistGradientBoosting on Validation dataset&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;regressionSummary(y_val, histgb.predict(X_val))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Regression statistics

               Mean Error (ME) : -0.8717
Root Mean Squared Error (RMSE) : 93.0712
     Mean Absolute Error (MAE) : 14.2191&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The results show a RMSE of 82.9 and 93.07 on training and validation dataset respectively. Thus, it is suggested that HistGradientBoosting model is quite overfitting to the training data. The 82.9 RMSE on the training data implies that on the average, the model predicts 83 more or less clicks than the actual number of clicks made. The evaluation indicates that the model is performing better than the baseline model. Nonetheless, there is the need to improve the model by tuning hyperparameters and using alternative models.&lt;/p&gt;
&lt;h3 id="hyperparameter-tuning"&gt;Hyperparameter tuning&lt;/h3&gt;
&lt;p&gt;While model with the default parameters is doing a better job than random guess (suggested by the baseline model), the aim is to reduce error as much as possible to increase accuracy in prediction. While it is possible to achieve this by using a different model, a common approach is to tune hyperparameters of the model already developed to verify if improvement is attainable. Hyperparameter tuning can be undertaken using the grid search where a set of parameters are specified and a search is made using various combinations or permutation to test which combination best reduces error. Another approach is the Radomized grid search where a range is specified for numeric parameters and the algorithmn randomly select the values of the hyperparameter within the range or condition specified to optimize the model. Bayesian optimization is also another approach.&lt;/p&gt;
&lt;p&gt;For grid search, all possible combinations are used while random search randomly choose some of the combination a number of times equal to the value of n_iter argument specified.&lt;/p&gt;
&lt;p&gt;GridSearch is demonstrated below. Given the time and computational constraints, limited combinations of parameters can be specified to tune HistGradientBoosting Regressor.&lt;/p&gt;
&lt;pre class="python"&gt;&lt;code&gt;# specify values for hyperparameters for the grid search
param_grid = {&amp;#39;max_depth&amp;#39;: [10,11,12,],
              &amp;#39;learning_rate&amp;#39;: [0.1, 0.8, 0.9],
              &amp;#39;min_samples_leaf&amp;#39;: [10,12,14],
              &amp;#39;l2_regularization&amp;#39;: [0.1, 0.8, 1]
            }

# create an instance of the grid search
gridSearch = GridSearchCV(HistGradientBoostingRegressor(random_state=2022,
                                                        verbose=0
                                                        ),
                          param_grid, cv=5
                          )
# fit the model for grid search
gridSearch.fit(X_train, y_train)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;
# show parameters that produce the least error from the possible combinations
print(&amp;#39;Best parameters: &amp;#39;, gridSearch.best_params_)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Best parameters:  {&amp;#39;l2_regularization&amp;#39;: 0.8, &amp;#39;learning_rate&amp;#39;: 0.1, &amp;#39;max_depth&amp;#39;: 10, &amp;#39;min_samples_leaf&amp;#39;: 10}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The hyperparameter tuning was done as a quick search for demonstration purpose only. A well designed parameter set will take a longer computational time to achieve good result. That noted, the parameters for the best model from the tuning is evaluated as follows;&lt;/p&gt;
&lt;pre class="python"&gt;&lt;code&gt;improved_histboostgb = gridSearch.best_estimator_&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;print(f&amp;quot;Metrics for best tuned HistGradientBoosting on Training dataset&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Metrics for best tuned HistGradientBoosting on Training dataset&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;regressionSummary(y_train, improved_histboostgb.predict(X_train))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Regression statistics

               Mean Error (ME) : 0.0253
Root Mean Squared Error (RMSE) : 79.0735
     Mean Absolute Error (MAE) : 13.1996&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;
print(&amp;quot;Metrics for best tuned HistGradientBoosting on Validation dataset&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Metrics for best tuned HistGradientBoosting on Validation dataset&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;regressionSummary(y_val, improved_histboostgb.predict(X_val))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Regression statistics

               Mean Error (ME) : -0.8557
Root Mean Squared Error (RMSE) : 92.1132
     Mean Absolute Error (MAE) : 14.0488&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Using a grid search for various combinations of the hyperparameters specified, RMSE reduced from 82.9 to 79.07 on training data and that of validation data reduced from 93.09 to 92.11. Generally, some reduction in error is attainable with hyperparameter tuning particularly when and time and computational resource is dedicated to it. Overfitting is suggested in the result and could be handled by tuning the regularization parameter.&lt;/p&gt;
&lt;h3 id="bagging-as-an-approach-to-improving-model-performance-and-overfitting"&gt;Bagging as an approach to improving model performance and overfitting&lt;/h3&gt;
&lt;p&gt;In order to further reduce the error, the tuned model is bagged. This approach undertakes multiple random sampling with replacement and for each sample fits HistGradientBoostingRegressor to produce scores which are aggregated. As expected, this helps reduce overfitting as more samples are fitted hence making the model more stable to unseen data.&lt;/p&gt;
&lt;p&gt;Bagging is implemented for the HistGradientBoostingRegressor as follows&lt;/p&gt;
&lt;pre class="python"&gt;&lt;code&gt;# create an instance for model
bagging = BaggingRegressor(HistGradientBoostingRegressor(random_state=2022,
                                                        l2_regularization=0.8, 
                                                        learning_rate=0.1, 
                                                        max_depth=10, 
                                                        min_samples_leaf=10
                                                        ),
                           n_estimators=100,
                           random_state=2022
                           )
                           &lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;# fit the model 
bagging.fit(X_train, y_train)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;print(&amp;quot;Metrics for bagged HistGradientBoosting on Validation dataset&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Metrics for bagged HistGradientBoosting on Validation dataset&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;regressionSummary(y_val, bagging.predict(X_val))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Regression statistics

               Mean Error (ME) : -1.0130
Root Mean Squared Error (RMSE) : 90.2835
     Mean Absolute Error (MAE) : 14.0052&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;print(&amp;quot;Metrics for bagged HistGradientBoosting on Training dataset&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Metrics for bagged HistGradientBoosting on Training dataset&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;regressionSummary(y_train, bagging.predict(X_train))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Regression statistics

               Mean Error (ME) : -0.0685
Root Mean Squared Error (RMSE) : 80.8758
     Mean Absolute Error (MAE) : 13.3553&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The result for bagging of HistGradientBoosting shows further improvement with RMSE on the validation data reducing to 90.28 compared to 92.11 by the tuned HistGradientBoosting without bagging.&lt;/p&gt;
&lt;p&gt;In addition, an insight provided is that bagging equally reduced the amount of overfitting on the training dataset given that it decreased the difference in error between validation RMSE and training RMSE (90.28 vs. 80.75) compared to tuned HistGradientBoosting which produced 92.1132 and 79.0735 for validation and training RMSE. Generally, the lesser the difference in error between the training and validation dataset, the better and the lower the overfitting or underfitting. The goal is to produce a model that scores lower error with minimal difference on the validation and training dataset.&lt;/p&gt;
&lt;h3 id="alternative-models-xgbrfregressor"&gt;Alternative models – XGBRFRegressor&lt;/h3&gt;
&lt;p&gt;XGBRFRegressor is Extreme Gradient Boosting Random Forest regression model and be seen as an advance variation of decision tree model where multiple trees are grown with random sampling of features and data hence Random Forest. This API also inherently handles missing data hence another option to using decision tree models that cater for missing data as undertaken with HistGradientBoostingRegressor.&lt;/p&gt;
&lt;p&gt;Its implementation as below;&lt;/p&gt;
&lt;pre class="python"&gt;&lt;code&gt;## create an instance of XGBRFRegressor with default parameters 
## and set random state for reproducible results
xgb = XGBRFRegressor(random_state=2022)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;# fit model on training data
xgb.fit(X_train, y_train)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;# XGBRFRegressor model evaluation on training data 

print(f&amp;quot;Metrics for XGBRFRegressor on Training dataset&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Metrics for XGBRFRegressor on Training dataset&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;regressionSummary(y_train, xgb.predict(X_train))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Regression statistics

               Mean Error (ME) : -0.0010
Root Mean Squared Error (RMSE) : 95.5276
     Mean Absolute Error (MAE) : 15.1109&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;# XGBRFRegressor model evaluation on validation data
print(f&amp;quot;Metrics for XGBRFRegressor on Validation dataset&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Metrics for XGBRFRegressor on Validation dataset&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;regressionSummary(y_val, xgb.predict(X_val))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Regression statistics

               Mean Error (ME) : -0.9871
Root Mean Squared Error (RMSE) : 99.1090
     Mean Absolute Error (MAE) : 15.2338&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The RMSE of 99.1 on validation dataset for XGBRFRegressor indicates better performance of XGBRFRegressor compared to the baseline model but with a higher error compared to HistGradientBoosting. Thus, HistGradientBoosting will be chosen over XGBRFRegressor on the basis of a lower RMSE on the validation dataset.&lt;/p&gt;
&lt;p&gt;The importance of various features in contributing to the model, otherwise known as predictive power of the variables can be estimated for the model below.&lt;/p&gt;
&lt;pre class="python"&gt;&lt;code&gt;#%% estimate feature importance in the model
important_features = pd.Series(data=xgb.feature_importances_)
important_features.sort_values(ascending=True,inplace=True)

## create a DataFrame with features and their importance score in the model
feature_importance_df = pd.DataFrame({&amp;#39;Predictors&amp;#39;: X_train.columns,
                                    &amp;#39;importance&amp;#39;: important_features,
                                    }                                    
                                    )
feature_importance_df&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;           Predictors  importance
5             city_id    0.054296
9       content_score    0.055762
4            n_images    0.065593
7  distance_to_center    0.071565
1          avg_rating    0.080671
2               stars    0.090017
8           n_reviews    0.092771
3            avg_rank    0.138946
6           avg_price    0.152653
0  avg_saving_percent    0.197725&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id="visualize-feature-importance"&gt;Visualize feature importance&lt;/h3&gt;
&lt;pre class="python"&gt;&lt;code&gt;fig = px.bar(data_frame=feature_importance_df, y=&amp;quot;Predictors&amp;quot;, x=&amp;quot;importance&amp;quot;, orientation=&amp;quot;h&amp;quot;,
             color=&amp;quot;Predictors&amp;quot;, title=&amp;quot;Feature Importance in XGBRFRegressor&amp;quot;,template=&amp;quot;plotly_dark&amp;quot;
             )
fig.show()&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;                        &lt;script type="text/javascript"&gt;window.PlotlyConfig = {MathJaxConfig: 'local'};&lt;/script&gt;
        &lt;script src="https://cdn.plot.ly/plotly-2.14.0.min.js"&gt;&lt;/script&gt;                &lt;div id="0ad89d0a-4868-4ece-b397-7274c85562ee" class="plotly-graph-div" style="height:100%; width:100%;"&gt;&lt;/div&gt;            &lt;script type="text/javascript"&gt;                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById("0ad89d0a-4868-4ece-b397-7274c85562ee")) {                    Plotly.newPlot(                        "0ad89d0a-4868-4ece-b397-7274c85562ee",                        [{"alignmentgroup":"True","hovertemplate":"Predictors=%{y}&lt;br&gt;importance=%{x}&lt;extra&gt;&lt;/extra&gt;","legendgroup":"city_id","marker":{"color":"#636efa","pattern":{"shape":""}},"name":"city_id","offsetgroup":"city_id","orientation":"h","showlegend":true,"textposition":"auto","x":[0.05429615452885628],"xaxis":"x","y":["city_id"],"yaxis":"y","type":"bar"},{"alignmentgroup":"True","hovertemplate":"Predictors=%{y}&lt;br&gt;importance=%{x}&lt;extra&gt;&lt;/extra&gt;","legendgroup":"content_score","marker":{"color":"#EF553B","pattern":{"shape":""}},"name":"content_score","offsetgroup":"content_score","orientation":"h","showlegend":true,"textposition":"auto","x":[0.055762000381946564],"xaxis":"x","y":["content_score"],"yaxis":"y","type":"bar"},{"alignmentgroup":"True","hovertemplate":"Predictors=%{y}&lt;br&gt;importance=%{x}&lt;extra&gt;&lt;/extra&gt;","legendgroup":"n_images","marker":{"color":"#00cc96","pattern":{"shape":""}},"name":"n_images","offsetgroup":"n_images","orientation":"h","showlegend":true,"textposition":"auto","x":[0.06559327989816666],"xaxis":"x","y":["n_images"],"yaxis":"y","type":"bar"},{"alignmentgroup":"True","hovertemplate":"Predictors=%{y}&lt;br&gt;importance=%{x}&lt;extra&gt;&lt;/extra&gt;","legendgroup":"distance_to_center","marker":{"color":"#ab63fa","pattern":{"shape":""}},"name":"distance_to_center","offsetgroup":"distance_to_center","orientation":"h","showlegend":true,"textposition":"auto","x":[0.07156529277563095],"xaxis":"x","y":["distance_to_center"],"yaxis":"y","type":"bar"},{"alignmentgroup":"True","hovertemplate":"Predictors=%{y}&lt;br&gt;importance=%{x}&lt;extra&gt;&lt;/extra&gt;","legendgroup":"avg_rating","marker":{"color":"#FFA15A","pattern":{"shape":""}},"name":"avg_rating","offsetgroup":"avg_rating","orientation":"h","showlegend":true,"textposition":"auto","x":[0.08067073673009872],"xaxis":"x","y":["avg_rating"],"yaxis":"y","type":"bar"},{"alignmentgroup":"True","hovertemplate":"Predictors=%{y}&lt;br&gt;importance=%{x}&lt;extra&gt;&lt;/extra&gt;","legendgroup":"stars","marker":{"color":"#19d3f3","pattern":{"shape":""}},"name":"stars","offsetgroup":"stars","orientation":"h","showlegend":true,"textposition":"auto","x":[0.09001738578081131],"xaxis":"x","y":["stars"],"yaxis":"y","type":"bar"},{"alignmentgroup":"True","hovertemplate":"Predictors=%{y}&lt;br&gt;importance=%{x}&lt;extra&gt;&lt;/extra&gt;","legendgroup":"n_reviews","marker":{"color":"#FF6692","pattern":{"shape":""}},"name":"n_reviews","offsetgroup":"n_reviews","orientation":"h","showlegend":true,"textposition":"auto","x":[0.09277134388685226],"xaxis":"x","y":["n_reviews"],"yaxis":"y","type":"bar"},{"alignmentgroup":"True","hovertemplate":"Predictors=%{y}&lt;br&gt;importance=%{x}&lt;extra&gt;&lt;/extra&gt;","legendgroup":"avg_rank","marker":{"color":"#B6E880","pattern":{"shape":""}},"name":"avg_rank","offsetgroup":"avg_rank","orientation":"h","showlegend":true,"textposition":"auto","x":[0.13894571363925934],"xaxis":"x","y":["avg_rank"],"yaxis":"y","type":"bar"},{"alignmentgroup":"True","hovertemplate":"Predictors=%{y}&lt;br&gt;importance=%{x}&lt;extra&gt;&lt;/extra&gt;","legendgroup":"avg_price","marker":{"color":"#FF97FF","pattern":{"shape":""}},"name":"avg_price","offsetgroup":"avg_price","orientation":"h","showlegend":true,"textposition":"auto","x":[0.1526525914669037],"xaxis":"x","y":["avg_price"],"yaxis":"y","type":"bar"},{"alignmentgroup":"True","hovertemplate":"Predictors=%{y}&lt;br&gt;importance=%{x}&lt;extra&gt;&lt;/extra&gt;","legendgroup":"avg_saving_percent","marker":{"color":"#FECB52","pattern":{"shape":""}},"name":"avg_saving_percent","offsetgroup":"avg_saving_percent","orientation":"h","showlegend":true,"textposition":"auto","x":[0.19772541522979736],"xaxis":"x","y":["avg_saving_percent"],"yaxis":"y","type":"bar"}],                        {"template":{"data":{"barpolar":[{"marker":{"line":{"color":"rgb(17,17,17)","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"bar":[{"error_x":{"color":"#f2f5fa"},"error_y":{"color":"#f2f5fa"},"marker":{"line":{"color":"rgb(17,17,17)","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"carpet":[{"aaxis":{"endlinecolor":"#A2B1C6","gridcolor":"#506784","linecolor":"#506784","minorgridcolor":"#506784","startlinecolor":"#A2B1C6"},"baxis":{"endlinecolor":"#A2B1C6","gridcolor":"#506784","linecolor":"#506784","minorgridcolor":"#506784","startlinecolor":"#A2B1C6"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"contour"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"heatmapgl"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"heatmap"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"histogram2dcontour"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"histogram2d"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"line":{"color":"#283442"}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatter":[{"marker":{"line":{"color":"#283442"}},"type":"scatter"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#506784"},"line":{"color":"rgb(17,17,17)"}},"header":{"fill":{"color":"#2a3f5f"},"line":{"color":"rgb(17,17,17)"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#f2f5fa","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#f2f5fa"},"geo":{"bgcolor":"rgb(17,17,17)","lakecolor":"rgb(17,17,17)","landcolor":"rgb(17,17,17)","showlakes":true,"showland":true,"subunitcolor":"#506784"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"dark"},"paper_bgcolor":"rgb(17,17,17)","plot_bgcolor":"rgb(17,17,17)","polar":{"angularaxis":{"gridcolor":"#506784","linecolor":"#506784","ticks":""},"bgcolor":"rgb(17,17,17)","radialaxis":{"gridcolor":"#506784","linecolor":"#506784","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"rgb(17,17,17)","gridcolor":"#506784","gridwidth":2,"linecolor":"#506784","showbackground":true,"ticks":"","zerolinecolor":"#C8D4E3"},"yaxis":{"backgroundcolor":"rgb(17,17,17)","gridcolor":"#506784","gridwidth":2,"linecolor":"#506784","showbackground":true,"ticks":"","zerolinecolor":"#C8D4E3"},"zaxis":{"backgroundcolor":"rgb(17,17,17)","gridcolor":"#506784","gridwidth":2,"linecolor":"#506784","showbackground":true,"ticks":"","zerolinecolor":"#C8D4E3"}},"shapedefaults":{"line":{"color":"#f2f5fa"}},"sliderdefaults":{"bgcolor":"#C8D4E3","bordercolor":"rgb(17,17,17)","borderwidth":1,"tickwidth":0},"ternary":{"aaxis":{"gridcolor":"#506784","linecolor":"#506784","ticks":""},"baxis":{"gridcolor":"#506784","linecolor":"#506784","ticks":""},"bgcolor":"rgb(17,17,17)","caxis":{"gridcolor":"#506784","linecolor":"#506784","ticks":""}},"title":{"x":0.05},"updatemenudefaults":{"bgcolor":"#506784","borderwidth":0},"xaxis":{"automargin":true,"gridcolor":"#283442","linecolor":"#506784","ticks":"","title":{"standoff":15},"zerolinecolor":"#283442","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"#283442","linecolor":"#506784","ticks":"","title":{"standoff":15},"zerolinecolor":"#283442","zerolinewidth":2}}},"xaxis":{"anchor":"y","domain":[0.0,1.0],"title":{"text":"importance"}},"yaxis":{"anchor":"x","domain":[0.0,1.0],"title":{"text":"Predictors"},"categoryorder":"array","categoryarray":["avg_saving_percent","avg_price","avg_rank","n_reviews","stars","avg_rating","distance_to_center","n_images","content_score","city_id"]},"legend":{"title":{"text":"Predictors"},"tracegroupgap":0},"title":{"text":"Feature Importance in XGBRFRegressor"},"barmode":"relative"},                        {"responsive": true}                    )                };                            &lt;/script&gt;        &lt;/div&gt;
&lt;p&gt;From the visualization above, avg_saving_percent has the most predictive power in XGBRFRegressor model fitted on the data&lt;/p&gt;
&lt;h3 id="potential-measures-for-to-improve-the-model"&gt;Potential measures for to improve the model&lt;/h3&gt;
&lt;p&gt;Modeling can be a very iterative exercise with time and resource such as computational power constraints and this is duly recognized here. It is therefore concluded that, there is a good chance the model developed here can be further improved with those available resources using appropriate measures. Instead of aiming to exhaust all implementation to produce the lowest error possible for a model; some of the measures to improve the model are highlighted here as further course of action.&lt;/p&gt;
&lt;ol style="list-style-type: decimal"&gt;
&lt;li&gt;Feature selection and engineering:&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Feature selection could be explored to select only the most influential features and probably reduce overfitting. Some categorical features could also be transformed by encoding if computation power allows for the increase in data dimensionality to explore the potential of improving the model. One-Hot encoding was ruled-out for city_id due to its high cardinality. While only normalization or scaling is less likely to significantly improve performance of decision tree based models explored here, the technique should be explore for other type of models. Closely related is the option of augmenting the data with new variables. Potential variables and data to use or collect include availability of special services at hotels like excursions and guided tours, cultural performance among others. This could be an influential variable that appeal to users to click and book. This could be a categorical variable or all special services use to rate a hotel as an ordinal variable.&lt;/p&gt;
&lt;ol start="2" style="list-style-type: decimal"&gt;
&lt;li&gt;&lt;p&gt;Experiment with different techniques of handling missing data to discover which approach reliably produce a stable and optimized model. This approach could be iterative but a very interesting domain dedicate resources to establish domain relevant method of handling missing data since it appears to be a challenge for the data.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Tuning hyperparameters:&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The models developed can further be improved by experimenting with hyperparameters to select combinations that best reduces error.&lt;/p&gt;
&lt;h2 id="making-prediction-on-test-data"&gt;Making prediction on test data&lt;/h2&gt;
&lt;p&gt;The test data is not used in the training the model but for prediction. This is one of the approaches used in gaining an objective view of the model’s generalizable performance. Something that was done on the validation dataset. To make predictions on the test data, the bagged HistGradienBoostingRegressor model is used.&lt;/p&gt;
&lt;p&gt;Generally, the prediction made were float values some of which do not reconcile with reality in terms of the unit of measuring the target variable which is count of clicks. For instance, a prediction of 8.387 in reality deviates from logic as there can only be clicks numbering whole numbers and not decimals. Thus, the prediction were round-off to nearest whole numbers which for this example will be 8. It should be noted that, this action in itself could introduce some margin of error. More importantly, number of clicks are counts hence a poisson regression could be more appropriate for the modeling exercise and should be explored.&lt;/p&gt;
&lt;pre class="python"&gt;&lt;code&gt;## read test dataset
test_data = pd.read_csv(r&amp;quot;Data/test_set.csv&amp;quot;)

test_data.head()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;       hotel_id   city_id  ...  avg_price  avg_saving_percent
0   14942256073  122750.0  ...      90.19                32.0
1   16036037903   28134.0  ...      98.27                19.0
2  288585940112   30578.0  ...      48.77                 0.0
3  129041645070   54398.0  ...      72.32                 0.0
4   12460296563   63890.0  ...      24.54                19.0

[5 rows x 11 columns]&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;# making prediction on test dataset -- select features and use model to make prediction
test_data[&amp;quot;predictions_made&amp;quot;] = bagging.predict(test_data.iloc[:, 1:])
test_data&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;            hotel_id   city_id  ...  avg_saving_percent  predictions_made
0        14942256073  122750.0  ...                32.0          8.387677
1        16036037903   28134.0  ...                19.0         25.187326
2       288585940112   30578.0  ...                 0.0          0.142417
3       129041645070   54398.0  ...                 0.0          1.738487
4        12460296563   63890.0  ...                19.0          7.663731
...              ...       ...  ...                 ...               ...
132157  146542092044   22896.0  ...                 0.0         10.893020
132158  191459858176   28030.0  ...                 4.0          6.136969
132159   71241988826  137926.0  ...                19.0          2.265503
132160  204878950620   54574.0  ...                 0.0          0.580113
132161   99788791152  774630.0  ...                 0.0          2.434706

[132162 rows x 12 columns]&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;# round off predictions to nearest whole to reflect actual counts of clicks


test_data[&amp;#39;n_clicks&amp;#39;] = round(test_data[&amp;#39;predictions_made&amp;#39;])
test_data&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;            hotel_id   city_id  ...  predictions_made  n_clicks
0        14942256073  122750.0  ...          8.387677       8.0
1        16036037903   28134.0  ...         25.187326      25.0
2       288585940112   30578.0  ...          0.142417       0.0
3       129041645070   54398.0  ...          1.738487       2.0
4        12460296563   63890.0  ...          7.663731       8.0
...              ...       ...  ...               ...       ...
132157  146542092044   22896.0  ...         10.893020      11.0
132158  191459858176   28030.0  ...          6.136969       6.0
132159   71241988826  137926.0  ...          2.265503       2.0
132160  204878950620   54574.0  ...          0.580113       1.0
132161   99788791152  774630.0  ...          2.434706       2.0

[132162 rows x 13 columns]&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="summary"&gt;Summary&lt;/h2&gt;
&lt;p&gt;This project demonstrates a real world approach to using machine learning to solve business problem. The problem was that the bidding team lacks a mechanism that supports them to achieve their goal of enabling advertisers run successful campaign ads. The solution offered is a prediction tool powered by decision tree model that enables the bidding team predict ads clicks better than random guesses. The project demonstrates how exploratory analysis is undertaken as a sign-post showing the way for which machine learning algorithm to use. The model was trained and evaluated and hyperpramater tuning undertaken. Bagging was also undertaken to reduce overfitting. Further, suggestions are provided to be explored in improving the model.&lt;/p&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;</description>
      <distill:md5>47ab80b0b355ac3618ebe016422b5211</distill:md5>
      <category>Machine learning</category>
      <category>Python</category>
      <guid>https://addurl/posts/2022-09-10-sales-hypothesis-analysis</guid>
      <pubDate>Fri, 09 Sep 2022 00:00:00 +0000</pubDate>
      <media:content url="https://addurl/posts/2022-09-10-sales-hypothesis-analysis/sales-hypothesis-analysis_files/figure-html5/unnamed-chunk-4-1.png" medium="image" type="image/png" width="1874" height="1208"/>
    </item>
    <item>
      <title>Interactive time series data visualization – Dygraphs in R</title>
      <dc:creator>Linus Agbleze</dc:creator>
      <link>https://addurl/posts/2022-09-05-dygraphs</link>
      <description>This post demonstrates how to undertake customized interactive visualization using time series data.</description>
      <category>visualization</category>
      <category>time series</category>
      <guid>https://addurl/posts/2022-09-05-dygraphs</guid>
      <pubDate>Mon, 05 Sep 2022 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Analyzing the impact of various factors on apartment booking sales</title>
      <dc:creator>Linus Agbleze</dc:creator>
      <link>https://addurl/posts/2022-09-05-linear-regression</link>
      <description>This post uses linear regression to analyze how various factors influence booking sales.</description>
      <category>Linear regression</category>
      <category>revenue impact assessment</category>
      <guid>https://addurl/posts/2022-09-05-linear-regression</guid>
      <pubDate>Mon, 05 Sep 2022 00:00:00 +0000</pubDate>
      <media:content url="https://addurl/posts/2022-09-05-linear-regression/linear-regression_files/figure-html5/unnamed-chunk-4-1.png" medium="image" type="image/png" width="1248" height="768"/>
    </item>
    <item>
      <title>Product analysis: A/B testing and KPI analysis of products for a start-up</title>
      <dc:creator>Linus Agbleze</dc:creator>
      <link>https://addurl/posts/2022-09-04-homelikeassignmentcode</link>
      <description>This post discusses identifying and analyzing KPIs for products as well as undertaking A/B testing to make recommendations to the products team. The post is an end-to-end project that demonstrates how to support product development with statistical analysis.</description>
      <category>product analysis</category>
      <category>R</category>
      <category>A/B testing</category>
      <category>Hypothesis testing</category>
      <category>Product KPI analysis</category>
      <guid>https://addurl/posts/2022-09-04-homelikeassignmentcode</guid>
      <pubDate>Sun, 04 Sep 2022 00:00:00 +0000</pubDate>
      <media:content url="https://addurl/posts/2022-09-04-homelikeassignmentcode/homelikeassignmentcode_files/figure-html5/unnamed-chunk-2-1.png" medium="image" type="image/png" width="888" height="384"/>
    </item>
    <item>
      <title>Logistics regression: Assessing factors that inlfuence client conversion</title>
      <dc:creator>Linus Agbleze</dc:creator>
      <link>https://addurl/posts/2022-09-04-logistics-regression</link>
      <description>This post uses logistic regression to support sales team to decipher the impact of various factors on their conversion rate.</description>
      <category>logistic regression</category>
      <guid>https://addurl/posts/2022-09-04-logistics-regression</guid>
      <pubDate>Sun, 04 Sep 2022 00:00:00 +0000</pubDate>
      <media:content url="https://addurl/posts/2022-09-04-logistics-regression/logistics-regression_files/figure-html5/unnamed-chunk-6-1.png" medium="image" type="image/png" width="1248" height="768"/>
    </item>
    <item>
      <title>Hierarchical clustering -- Which states in Nigeria have similar expenditure profile?</title>
      <dc:creator>Linus Agbleze</dc:creator>
      <link>https://addurl/posts/2022-07-20-hierarchical-clustering-which-states-in-nigeria-have-similar-expenditure-profile</link>
      <description>Clustering analysis is popular unsupervised machine 
learning techniques that categorize data into groups of similarity.</description>
      <category>Unsupervised learning</category>
      <guid>https://addurl/posts/2022-07-20-hierarchical-clustering-which-states-in-nigeria-have-similar-expenditure-profile</guid>
      <pubDate>Mon, 18 Jul 2022 00:00:00 +0000</pubDate>
      <media:content url="https://addurl/posts/2022-07-20-hierarchical-clustering-which-states-in-nigeria-have-similar-expenditure-profile/hierarchical-clustering-which-states-in-nigeria-have-similar-expenditure-profile_files/figure-html5/customize-dendro-1.png" medium="image" type="image/png" width="1536" height="1152"/>
    </item>
    <item>
      <title>Data visualization</title>
      <dc:creator>Linus Agbleze</dc:creator>
      <link>https://addurl/posts/2022-07-17-data-visualization</link>
      <description>This post entails the use of ggplot for data visualization.</description>
      <category>visualization</category>
      <guid>https://addurl/posts/2022-07-17-data-visualization</guid>
      <pubDate>Sun, 17 Jul 2022 00:00:00 +0000</pubDate>
      <media:content url="https://addurl/posts/2022-07-17-data-visualization/data-visualization_files/figure-html5/unnamed-chunk-1-1.png" medium="image" type="image/png" width="1248" height="768"/>
    </item>
    <item>
      <title>Welcome to Datasiast!</title>
      <dc:creator>Linus Agbleze</dc:creator>
      <link>https://addurl/posts/welcome</link>
      <description>Datasiast exists for enthusiatic data science knowledge sharing and practice.</description>
      <guid>https://addurl/posts/welcome</guid>
      <pubDate>Sun, 17 Jul 2022 00:00:00 +0000</pubDate>
    </item>
  </channel>
</rss>
